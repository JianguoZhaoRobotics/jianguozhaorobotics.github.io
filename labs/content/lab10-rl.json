{"version":3,"kind":"Notebook","sha256":"38e14c940da3563cf012a6f3b0d8164d5d79c990db4107cd617d6724735189d6","slug":"lab10-rl","location":"/Lab10 - RL.ipynb","dependencies":[],"frontmatter":{"title":"Lab 10: RL: Policy Gradient Methods","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"Python 3"},"authors":[{"nameParsed":{"literal":"Jianguo Zhao","given":"Jianguo","family":"Zhao"},"name":"Jianguo Zhao","affiliations":["Department of Mechanical Engineering, Colorado State University"],"id":"contributors-Users\\jguoz\\OneDrive - Colostate\\Teaching\\MECH 529 Advanced Mechanical Systems\\2025 Fall\\Lab\\All Labs\\myst-generated-uid-0"}],"github":"https://github.com/YOUR_USERNAME/YOUR_REPO","copyright":"2025","affiliations":[{"id":"Department of Mechanical Engineering, Colorado State University","name":"Department of Mechanical Engineering, Colorado State University"}],"thumbnail":"/c01983909a444910204ca64e43cd5bd5.png","exports":[{"format":"ipynb","filename":"Lab10 - RL.ipynb","url":"/Lab10 - RL-3320cd386a272e7b08733fa04502ab90.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","data":{"id":"d826f7f8"},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"This lab explores ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ClH14nZddC"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"reinforcement learning (RL)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PugNnxZdgl"}],"key":"OJBOKScbxe"},{"type":"text","value":" techniques for learning control policies. We progress from simple policy gradient methods to advanced algorithms, applying them to increasingly complex control tasks.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lutAqjHucz"}],"key":"VZg0fCSvH4"},{"type":"heading","depth":2,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Overview of Tasks","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"CFR3YXs6w8"}],"identifier":"overview-of-tasks","label":"Overview of Tasks","html_id":"overview-of-tasks","implicit":true,"key":"GRQezUNsdp"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"strong","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Task 1: REINFORCE on GridWorld","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"A76fBIMHkJ"}],"key":"UtTr4FJVnD"},{"type":"text","value":": Implement the REINFORCE algorithm, a Monte-Carlo policy gradient method. We train a neural network policy using one-hot state encoding to navigate a 5×5 grid while avoiding obstacles and reaching a goal. This task will directly use ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"fDlfw88HkH"},{"type":"inlineCode","value":"Pytorch","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"TrSbGO3peS"},{"type":"text","value":" libabary to create neural networks.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"u8FEHO7sbX"}],"key":"ZTSAAV1deS"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"strong","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Task 2: PPO on Continuous Control (Pendulum)","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"LAVyWUFgoC"}],"key":"Kb0AP9VSef"},{"type":"text","value":": Apply Proximal Policy Optimization (PPO) from Stable Baselines3 to the pendulum swing-up problem. PPO offers more stable learning than REINFORCE through variance reduction.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"Pz8JA6cHjy"}],"key":"wsj5Aopsbw"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"strong","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Task 3: CartPole Stabilization","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"AwCKZsEZjr"}],"key":"AOT5QGcUKG"},{"type":"text","value":": Use PPO with custom wrappers to convert the discrete CartPole environment to continuous control. We learn a stabilization policy similar to LQR control, then extend it to handle larger initial conditions.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"o7D1Qrcjjk"}],"key":"P1vJo7xHfZ"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"strong","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Task 4: CartPole Swing-Up","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"Hb3C79ETJG"}],"key":"hWCiIco96J"},{"type":"text","value":": Train a policy to swing the pole from the downward position to upright. This demonstrates how RL can solve complex non-linear control problems without explicit trajectory optimization.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"Hpnrrzmeu6"}],"key":"yVUAgFhpqD"},{"type":"paragraph","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"strong","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Task 5: Hybrid Policy (Swing-Up + Stabilization)","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"CidPqW2Itv"}],"key":"nHPncQKoQu"},{"type":"text","value":": Combine the swing-up and stabilization policies using a mode-switching strategy. The agent swings up using one policy, then switches to a stabilizer policy once near the goal—demonstrating hybrid control.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"z3CTKBiRxJ"}],"key":"M3n0DngMM9"}],"identifier":"d826f7f8","label":"d826f7f8","html_id":"d826f7f8","key":"e6MBel3r6O"},{"type":"block","kind":"notebook-content","data":{"id":"knpdanig8Q81"},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Task1: REINFORCE on a Grid Environment","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"f7NrX8QjWI"}],"identifier":"task1-reinforce-on-a-grid-environment","label":"Task1: REINFORCE on a Grid Environment","html_id":"task1-reinforce-on-a-grid-environment","implicit":true,"key":"TEwzHBpX5d"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This task introduces the policy gradient method: REINFORCE algorithm. We will use the same environment file (the GridWorld class) as defined in Lab 9. For policy learning, it is better that we randomize the starting position, instead of always starting at (0,0) as in Lab 9. This ensures the agent experiences all parts of the grid during training. If we always start from (0,0), the agent only learns a good policy for states along the path from (0,0) to the goal — it may never visit other cells and thus never learn what to do there. By randomizing the starting position, every cell gets visited, and the agent learns a complete policy for the entire grid. This becomes essential for larger and more complex environments, where a fixed starting position would leave most states unexplored.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"DFoNY2TuCf"}],"key":"ff4ti6dHIK"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Check the ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"Uv3xO6e9Nx"},{"type":"inlineCode","value":"reset","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"NLjwqVhTkk"},{"type":"text","value":" method in the GridWorld class to see how we randomize the starting position.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"Bn19tLoNaJ"}],"key":"ma8N8EbrbB"}],"identifier":"knpdanig8q81","label":"knpdanig8Q81","html_id":"knpdanig8q81","key":"KgRjq1kahl"},{"type":"block","kind":"notebook-code","data":{"cellView":"form","executionInfo":{"elapsed":90,"status":"ok","timestamp":1765561958942,"user":{"displayName":"Jianguo Zhao","userId":"09996222056778326224"},"user_tz":420},"id":"zAGCrRIlpofN"},"children":[{"type":"code","lang":"python","executable":true,"value":"# @title\nimport gymnasium as gym\nfrom gymnasium import spaces # spaces are used to define action and observation spaces\nimport numpy as np\n\n# Define the set of possible actions and their visual representations\nACTIONS = [0, 1, 2, 3, 4]  # up, right, down, left, stay\nACTION_ARROWS = {0:\"↑\", 1:\"→\", 2:\"↓\", 3:\"←\", 4:\"*\"}\n\n\nclass GridWorld(gym.Env):\n\n    \"\"\"Custom GridWorld environment compatible with Gymnasium API.\n    Note that Gymnasium environments are typically implemented as classes that inherit from gym.Env.\n    This allows them to integrate seamlessly with the Gymnasium framework,\n    which provides tools for reinforcement learning research and development.\n    \"\"\"\n\n    def __init__(self,\n                 grid_shape=(5, 5),\n                 forbidden=None,\n                 goal=None,\n                 goal_reward=10.0,\n                 step_reward=-0.01,\n                 forbidden_reward=-5.0,\n                 boundary_reward=-1.0,\n                 render_mode=None):\n        \"\"\"\n        Initializes the GridWorld environment.\n\n        Args:\n            grid_shape (tuple): The shape of the grid (rows, columns).\n            forbidden (list of tuples): A list of coordinates for forbidden states.\n            goal (tuple): The coordinates of the goal state.\n            goal_reward (float): The reward for reaching the goal.\n            step_reward (float): The reward for a regular step.\n            forbidden_reward (float): The reward for entering a forbidden state.\n            boundary_reward (float): The reward for hitting a boundary.\n            render_mode (str, optional): The mode for rendering the environment.\n        \"\"\"\n        super().__init__()\n\n        self.grid_shape = grid_shape\n        # Default locations for forbidden cells and the goal if not provided\n        self.forbidden = forbidden if forbidden is not None else [(1,1), (1,2),(3,1), (3,3) ,(4,1)]\n        # Example forbidden positions\n        self.goal = goal if goal is not None else (3,2)\n        # Example goal position\n\n        # Reward structure\n        self.goal_reward = goal_reward\n        self.step_reward = step_reward\n        self.forbidden_reward = forbidden_reward\n        self.boundary_reward = boundary_reward\n\n        # Discount factor\n        self.render_mode = render_mode\n\n        # Agent's current position, initialized in reset()\n        self.agent_pos = None\n\n        # Define Gymnasium spaces\n        # The action space is discrete with a size equal to the number of actions.\n        self.action_space = spaces.Discrete(len(ACTIONS))\n        # The observation space represents the agent's position on the grid.\n        self.observation_space = spaces.MultiDiscrete(list(grid_shape))\n\n\n    def in_bounds(self, s):\n        \"\"\"Check if a state 's' is within the grid boundaries.\"\"\"\n        r, c = s\n        R, C = self.grid_shape\n        return 0 <= r < R and 0 <= c < C\n\n    def is_forbidden(self, s):\n        \"\"\"Check if a state 's' is a forbidden state.\"\"\"\n        return s in self.forbidden\n\n    def states(self):\n        \"\"\"Return a list of all possible states (coordinates) in the grid.\"\"\"\n        valid_states = []\n        R, C = self.grid_shape\n        for r in range(R):\n            for c in range(C):\n                valid_states.append((r,c))\n        return valid_states\n\n    def reset(self, seed=None, options=None):\n        \"\"\"\n        Reset the environment to a RANDOM valid position.\n        \"\"\"\n        super().reset(seed=seed)\n\n        # Loop until we find a valid starting position\n        while True:\n            # Randomly pick a row and column\n            r = np.random.randint(self.grid_shape[0])\n            c = np.random.randint(self.grid_shape[1])\n            s = (r, c)\n\n            # Ensure we don't start in a forbidden cell or on the goal\n            # if not self.is_forbidden(s) and s != self.goal:\n            #     self.agent_pos = s\n            #     break\n            if s != self.goal:\n                self.agent_pos = s\n                break\n\n        obs = np.array(self.agent_pos, dtype=np.int32)\n        return obs, {}\n\n    def step(self, action):\n        \"\"\"\n        Execute one timestep in the environment.\n        The agent takes an action, and the environment transitions to a new state.\n\n        Args:\n            action (int): The action to be taken by the agent.\n\n        Returns:\n            tuple: A tuple containing the new observation, the reward,\n                   a boolean indicating if the episode is terminated,\n                   a boolean indicating if the episode is truncated,\n                   and an info dictionary.\n        \"\"\"\n        assert self.action_space.contains(action), f\"Invalid action {action}\"\n\n        r, c = self.agent_pos\n\n        # Map the action to a new position (nr, nc for new row, new col)\n        if action == 0:    nr, nc = r - 1, c     # up\n        elif action == 1:  nr, nc = r, c + 1     # right\n        elif action == 2:  nr, nc = r + 1, c     # down\n        elif action == 3:  nr, nc = r, c - 1     # left\n        else:              nr, nc = r, c         # stay\n\n        # Check for environment boundaries\n        terminated, truncated, info = False, False, {}\n        next_pos = (nr, nc)\n        if not self.in_bounds(next_pos):\n            next_pos = self.agent_pos  # Agent stays in the same position\n            reward = self.boundary_reward\n        elif self.is_forbidden(next_pos):\n            # next_pos = self.agent_pos  # Agent stays in the same position\n            reward = self.forbidden_reward\n        elif next_pos == self.goal:\n            reward = self.goal_reward\n            terminated = True\n        elif action == 4:\n            reward = -1\n        else:\n            reward = 0\n\n        # Update the agent's position\n        self.agent_pos = next_pos\n\n        # Prepare the return values\n        obs = np.array(self.agent_pos, dtype=np.int32)\n\n        return obs, reward, terminated, truncated, info\n\n    def render(self, P, V=None, ax=None, show=True):\n        \"\"\"\n        Renders the greedy policy as arrows and overlays the value for each state in a matplotlib plot.\n        This function will be used as a class method for the env class.\n\n        Args:\n            P (dict): The policy mapping states (r, c) to actions (int).\n            V (dict): Optional. A dictionary mapping states to their values.\n            ax: Optional matplotlib axes object. If provided, plots on this axes instead of creating a new figure.\n            show (bool): Whether to call plt.show() at the end. Default True. Set False when creating subplots.\n\n        Returns:\n            tuple: (fig, ax) - The figure and axes objects.\n        \"\"\"\n        import matplotlib.pyplot as plt\n        import numpy as np\n\n        R, C = self.grid_shape\n        if ax is None:\n            fig, ax = plt.subplots(figsize=(7, 6))\n        else:\n            fig = ax.figure\n        ax.set_xlim(-0.5, C - 0.5)\n        ax.set_ylim(-0.5, R - 0.5)\n        ax.set_xticks(np.arange(-0.5, C, 1))\n        ax.set_yticks(np.arange(-0.5, R, 1))\n        ax.grid(True, color='gray', linewidth=1, linestyle='-')\n        ax.set_aspect('equal')\n        ax.invert_yaxis()  # Match array indexing\n\n        # Draw cell highlights for forbidden/goal/start\n        for r in range(R):\n            for c in range(C):\n                s = (r, c)\n                if self.is_forbidden(s):\n                    rect = plt.Rectangle((c-0.5, r-0.5), 1, 1, color='gray', alpha=0.5)\n                    ax.add_patch(rect)\n                    ax.text(c, r, \"X\", ha='center', va='center', color='black', fontsize=14)\n                elif s == self.goal:\n                    rect = plt.Rectangle((c-0.5, r-0.5), 1, 1, color='yellow', alpha=0.45)\n                    ax.add_patch(rect)\n                    ax.text(c, r, \"G\", ha='center', va='center', color='black', fontsize=14)\n\n        # Print arrows and values in every cell (except forbidden)\n        for r in range(R):\n            for c in range(C):\n                s = (r, c)\n                # Print policy arrow\n                if (r, c) in P:\n                    arrow = ACTION_ARROWS[P[(r, c)]]\n                else:\n                    arrow = \"\"\n                if (V is not None) and ((r, c) in V):\n                    valstr = f\"{V[(r, c)]:.2f}\"\n                else:\n                    valstr = \"\"\n\n                # Place arrow in cell\n                ax.text(\n                    c, r-0.18, arrow,\n                    ha='center', va='center', color='darkblue', fontsize=22, fontweight='bold', zorder=10\n                )\n\n                ax.text(\n                    c, r+0.22, valstr,\n                    ha='center', va='center', color='crimson', fontsize=11, alpha=0.99,\n                    zorder=10\n                )\n\n        # Label grid\n        for x in range(C):\n            ax.text(x, -0.75, str(x), ha='center', va='center', color='black')\n        for y in range(R):\n            ax.text(-0.75, y, str(y), ha='center', va='center', color='black')\n\n        ax.set_title(\"Policy and Values\")\n        ax.tick_params(bottom=False, left=False, right=False, top=False,\n                      labelbottom=False, labelleft=False, labeltop=False, labelright=False)\n        if show:\n            plt.tight_layout()\n            plt.show()\n\n        return fig, ax","identifier":"zagcrrilpofn-code","enumerator":"1","html_id":"zagcrrilpofn-code","key":"jF5wGQFR0S"},{"type":"outputs","id":"YzZF7RSBH27AaNukbQ0oB","children":[],"identifier":"zagcrrilpofn-outputs","html_id":"zagcrrilpofn-outputs","key":"QwaHsuehy9"}],"identifier":"zagcrrilpofn","label":"zAGCrRIlpofN","html_id":"zagcrrilpofn","key":"tq7OlmyxAN"},{"type":"block","kind":"notebook-content","data":{"id":"eOLo3_Y5gWFS"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"REINFORCE (Monte-Carlo Policy Gradient)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"y16t1qseuE"}],"identifier":"reinforce-monte-carlo-policy-gradient","label":"REINFORCE (Monte-Carlo Policy Gradient)","html_id":"reinforce-monte-carlo-policy-gradient","implicit":true,"key":"zl7bA2jra4"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"The REINFORCE algorithm directly learns a stochastic policy ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"p3TBbYGcOr"},{"type":"inlineMath","value":"\\pi(a \\mid s)","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>π</mi><mo stretchy=\"false\">(</mo><mi>a</mi><mo>∣</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\pi(a \\mid s)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">a</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∣</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mclose\">)</span></span></span></span>","key":"gA9D7DEGb6"},{"type":"text","value":". It updates the policy so that actions leading to high returns become more likely, and actions leading to low returns become less likely.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Heia2FRUgo"}],"key":"hnV9NvD7Ey"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"strong","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Parameterizing the Policy","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"qfIWvV3mLZ"}],"key":"utnAnx00Rv"}],"key":"mpLxqYx9zn"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"We use a neural network with parameters ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"wbzfOWaZKb"},{"type":"inlineMath","value":"\\theta","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>","key":"TlqIfPGhxe"},{"type":"text","value":" to represent the policy. The network outputs ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"InLpeGFPaJ"},{"type":"strong","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"logits","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"vB9gHdSMJL"}],"key":"yfda1fEWi9"},{"type":"text","value":" — raw, unnormalized scores for each action. These logits are then converted to a probability distribution using softmax:","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"YufU5rLd3I"}],"key":"PTNcDQzP8F"},{"type":"math","value":"\\pi_\\theta(a \\mid s) = \\frac{\\exp(f_\\theta(s, a))}{\\sum_{a'} \\exp(f_\\theta(s, a'))}","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"tight":true,"html":"<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy=\"false\">(</mo><mi>a</mi><mo>∣</mo><mi>s</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>θ</mi></msub><mo stretchy=\"false\">(</mo><mi>s</mi><mo separator=\"true\">,</mo><mi>a</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><mrow><munder><mo>∑</mo><msup><mi>a</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup></munder><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>θ</mi></msub><mo stretchy=\"false\">(</mo><mi>s</mi><mo separator=\"true\">,</mo><msup><mi>a</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\pi_\\theta(a \\mid s) = \\frac{\\exp(f_\\theta(s, a))}{\\sum_{a&#x27;} \\exp(f_\\theta(s, a&#x27;))}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">a</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∣</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.4127em;vertical-align:-0.9857em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mop\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:0em;\">∑</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1783em;\"><span style=\"top:-2.4003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">a</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6828em;\"><span style=\"top:-2.786em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2997em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop\">exp</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6779em;\"><span style=\"top:-2.989em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mclose\">))</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mop\">exp</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">a</span><span class=\"mclose\">))</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9857em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>","enumerator":"1","key":"YH94XWtVyf"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"where ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"ofAL8fc5c4"},{"type":"inlineMath","value":"f_\\theta(s, a)","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>f</mi><mi>θ</mi></msub><mo stretchy=\"false\">(</mo><mi>s</mi><mo separator=\"true\">,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f_\\theta(s, a)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">s</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">a</span><span class=\"mclose\">)</span></span></span></span>","key":"aQTM0Sn2mT"},{"type":"text","value":" is the network’s output (logit) for action ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"vE67X8VOoQ"},{"type":"inlineMath","value":"a","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>a</mi></mrow><annotation encoding=\"application/x-tex\">a</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">a</span></span></span></span>","key":"JOSo3lgOkF"},{"type":"text","value":" in state ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"UHVMGpI4nQ"},{"type":"inlineMath","value":"s","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">s</span></span></span></span>","key":"E0hEHaHNYu"},{"type":"text","value":".","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"YEAuq44TZp"}],"key":"mNQjVH17FL"},{"type":"paragraph","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"strong","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Why logits instead of direct action probabilities or actions?","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"nuQG5qRl7z"}],"key":"gQTxoewpKE"},{"type":"text","value":" Logits provide numerical stability, allow flexible real-valued outputs that softmax normalizes into valid probabilities, enable efficient gradient computation through ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"r6DysZmHNB"},{"type":"inlineCode","value":"log_prob()","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"Gwcgkb5IlU"},{"type":"text","value":", and naturally support exploration by sampling from a distribution rather than greedily selecting a single action.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"LSIpGDchKc"}],"key":"u8U3Ir6QwW"},{"type":"paragraph","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"In our GridWorld:","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"xMCkd4m3cp"}],"key":"KuYxGqPo82"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":16,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"each state is a grid cell (r, c), represented using one-hot encoding","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"sFYGPZgkJr"}],"key":"Xkw2tWZF0z"}],"key":"QVN4bmZiPX"},{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"available actions: up, right, down, left, stay","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"brX5JByjY9"}],"key":"mIQ3KNJO9u"}],"key":"bQId0FOr0g"},{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"the policy samples actions using these probabilities","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"jlx7koEyzX"}],"key":"E1eKuJ5hsU"}],"key":"UOhq6yionz"}],"key":"clNW7KGvPq"},{"type":"paragraph","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"text","value":"The algorithm is like the following:","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"xT1OXRIO1V"}],"key":"BLpktoHFmE"},{"type":"image","url":"/c01983909a444910204ca64e43cd5bd5.png","alt":"image.png","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"oTEvECGalX","urlSource":"data:image/png;base64,iVBORw0KGg...5ErkJggg=="},{"type":"paragraph","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"strong","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"text","value":"Generating an Episode","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"SWezdOJFAg"}],"key":"IjA1oYvJEt"}],"key":"f3Kv6wwEij"},{"type":"paragraph","position":{"start":{"line":26,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"text","value":"Starting from a start state ","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"aM0bhjBPIq"},{"type":"inlineMath","value":"s_0","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>s</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">s_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>","key":"tXwnIm0Tb5"},{"type":"text","value":", the agent repeatedly:samples an action ","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"xTyc5ttYGQ"},{"type":"inlineMath","value":"a_t \\sim \\pi_\\theta(a \\mid s_t)","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>∼</mo><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy=\"false\">(</mo><mi>a</mi><mo>∣</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">a_t \\sim \\pi_\\theta(a \\mid s_t)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">a</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∣</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>","key":"FpVbr0Fxlx"},{"type":"text","value":"; transitions to the next state ","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"dLtWaor4PQ"},{"type":"inlineMath","value":"s_{t+1}","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">s_{t+1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span></span></span></span>","key":"XfAUb8mEV2"},{"type":"text","value":" via the environment; receives reward ","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"JFjtuE2JJl"},{"type":"inlineMath","value":"r_{t+1}","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>r</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">r_{t+1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span></span></span></span>","key":"mXwXt5rlIa"},{"type":"text","value":". This creates a trajectory:\n","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"uIwmj9hPA4"},{"type":"inlineMath","value":"\\{s_0, a_0, r_1, s_1, a_1, r_2, \\dots, s_{T-1}, a_{T-1}, r_T\\}","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mn>0</mn></msub><mo separator=\"true\">,</mo><msub><mi>a</mi><mn>0</mn></msub><mo separator=\"true\">,</mo><msub><mi>r</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>s</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>a</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>r</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mo>…</mo><mo separator=\"true\">,</mo><msub><mi>s</mi><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator=\"true\">,</mo><msub><mi>a</mi><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator=\"true\">,</mo><msub><mi>r</mi><mi>T</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{s_0, a_0, r_1, s_1, a_1, r_2, \\dots, s_{T-1}, a_{T-1}, r_T\\}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">{</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\">…</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">}</span></span></span></span>","key":"D7trlLorYi"},{"type":"text","value":". The episode ends when the agent reaches the goal or when a step limit is reached.","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"pXE4baQbPl"}],"key":"ESBGCxDKKr"},{"type":"paragraph","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"strong","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"text","value":"Action value update using Monte-Carlo Return","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"TjzrMDEKAI"}],"key":"d9AenvU7H0"}],"key":"WUG6gIhghn"},{"type":"paragraph","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"children":[{"type":"text","value":"For each timestep ","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"key":"VcMmqKNNfr"},{"type":"inlineMath","value":"t","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6151em;\"></span><span class=\"mord mathnormal\">t</span></span></span></span>","key":"C1k0168RAi"},{"type":"text","value":", the action value is the cumulative discounted reward from that point onward:","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"key":"BmThZPb2Zn"}],"key":"E97TBSDLvY"},{"type":"math","value":"q_t(s_t,a_t)=G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots + \\gamma^{T-t-1} r_{T}= \\sum_{k=t+1}^{T} \\gamma^{k-t-1} r_{k}","position":{"start":{"line":34,"column":1},"end":{"line":36,"column":1}},"html":"<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>q</mi><mi>t</mi></msub><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo separator=\"true\">,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo><mo>=</mo><msub><mi>G</mi><mi>t</mi></msub><mo>=</mo><msub><mi>r</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><msub><mi>r</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>+</mo><msup><mi>γ</mi><mn>2</mn></msup><msub><mi>r</mi><mrow><mi>t</mi><mo>+</mo><mn>3</mn></mrow></msub><mo>+</mo><mo>⋯</mo><mo>+</mo><msup><mi>γ</mi><mrow><mi>T</mi><mo>−</mo><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><msub><mi>r</mi><mi>T</mi></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>T</mi></munderover><msup><mi>γ</mi><mrow><mi>k</mi><mo>−</mo><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><msub><mi>r</mi><mi>k</mi></msub></mrow><annotation encoding=\"application/x-tex\">q_t(s_t,a_t)=G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots + \\gamma^{T-t-1} r_{T}= \\sum_{k=t+1}^{T} \\gamma^{k-t-1} r_{k}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">G</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7917em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7917em;vertical-align:-0.2083em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">2</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0724em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8641em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">3</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"minner\">⋯</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0858em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span><span class=\"mbin mtight\">−</span><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.1888em;vertical-align:-1.3604em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.8283em;\"><span style=\"top:-1.8479em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span><span class=\"mrel mtight\">=</span><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.05em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3604em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8991em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span><span class=\"mbin mtight\">−</span><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span>","enumerator":"2","key":"B4vN1MwHJP"},{"type":"paragraph","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"children":[{"type":"text","value":"In our GridWorld, we can set the following (you can change this to other values)","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"X659oJQPoy"}],"key":"sRQvLCR0Za"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":39,"column":1},"end":{"line":43,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"reward +10 for reaching the goal","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"key":"fsQwr9FbPH"}],"key":"YyClBVWg4t"}],"key":"GJDWGF5zrv"},{"type":"listItem","spread":true,"position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"reward -10 for entering forbidden cells","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"key":"CYwrLtoZ3Z"}],"key":"ZarEKzN1YO"}],"key":"hslRSRbCM4"},{"type":"listItem","spread":true,"position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"reward -1 for hitting walls","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"C18nELD7JK"}],"key":"Mvc95Osl7F"}],"key":"i9KO829QFH"},{"type":"listItem","spread":true,"position":{"start":{"line":42,"column":1},"end":{"line":43,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"reward -0.01 for each step","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"key":"MXvnPo75tO"}],"key":"CjNhuUnphp"}],"key":"V8mncLDlw6"}],"key":"WWfgT38JMJ"},{"type":"paragraph","position":{"start":{"line":44,"column":1},"end":{"line":44,"column":1}},"children":[{"type":"strong","position":{"start":{"line":44,"column":1},"end":{"line":44,"column":1}},"children":[{"type":"text","value":"Policy Gradient Update","position":{"start":{"line":44,"column":1},"end":{"line":44,"column":1}},"key":"A9aDthHw4l"}],"key":"mNF6c8YpbJ"}],"key":"Doj1tRjO2Z"},{"type":"paragraph","position":{"start":{"line":46,"column":1},"end":{"line":49,"column":1}},"children":[{"type":"text","value":"REINFORCE increases the probability of actions that produced high returns. The policy gradient theorem gives us:","position":{"start":{"line":46,"column":1},"end":{"line":46,"column":1}},"key":"wEJ6n9UTcD"}],"key":"I3St4NMYrq"},{"type":"math","value":"\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\ln \\pi(a_t \\mid s_t, \\theta)\\, q_t(s_t, a_t)","position":{"start":{"line":46,"column":1},"end":{"line":46,"column":1}},"tight":"before","html":"<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>θ</mi><mo>←</mo><mi>θ</mi><mo>+</mo><mi>α</mi><msub><mi mathvariant=\"normal\">∇</mi><mi>θ</mi></msub><mi>ln</mi><mo>⁡</mo><mi>π</mi><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>∣</mo><msub><mi>s</mi><mi>t</mi></msub><mo separator=\"true\">,</mo><mi>θ</mi><mo stretchy=\"false\">)</mo><mtext> </mtext><msub><mi>q</mi><mi>t</mi></msub><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo separator=\"true\">,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\ln \\pi(a_t \\mid s_t, \\theta)\\, q_t(s_t, a_t)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">←</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span><span class=\"mord\"><span class=\"mord\">∇</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop\">ln</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∣</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span>","enumerator":"3","key":"C7VkvLtLI0"},{"type":"paragraph","position":{"start":{"line":51,"column":1},"end":{"line":52,"column":1}},"children":[{"type":"text","value":"where ","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"key":"XjRdZqKmbD"},{"type":"inlineMath","value":"\\alpha","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span></span></span></span>","key":"INew11oMMq"},{"type":"text","value":" is the learning rate. Note that the gradient ","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"key":"KEyvFq0eue"},{"type":"inlineMath","value":"\\nabla_\\theta \\ln \\pi(a_t \\mid s_t, \\theta)","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi mathvariant=\"normal\">∇</mi><mi>θ</mi></msub><mi>ln</mi><mo>⁡</mo><mi>π</mi><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>∣</mo><msub><mi>s</mi><mi>t</mi></msub><mo separator=\"true\">,</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\nabla_\\theta \\ln \\pi(a_t \\mid s_t, \\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord\">∇</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop\">ln</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∣</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span>","key":"alzA6cNyia"},{"type":"text","value":" can be directly computed is a neural network is used for the policy. In this case, the log-probability ","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"key":"D2JQ16KaX9"},{"type":"inlineMath","value":"\\ln \\pi(a_t \\mid s_t, \\theta)","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>ln</mi><mo>⁡</mo><mi>π</mi><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>∣</mo><msub><mi>s</mi><mi>t</mi></msub><mo separator=\"true\">,</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\ln \\pi(a_t \\mid s_t, \\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">ln</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∣</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span>","key":"MtYcIsNIb7"},{"type":"text","value":"\nis differentiable with respect to the network parameters ","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"key":"SrbDkktTj3"},{"type":"inlineMath","value":"\\theta","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>","key":"DrNTIOsLzk"},{"type":"text","value":", so automatic differentiation (backpropagation) provides the gradient without requiring any manual derivation.","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"key":"obdSmY6OK2"}],"key":"pF56fCfw9A"}],"identifier":"eolo3_y5gwfs","label":"eOLo3_Y5gWFS","html_id":"eolo3-y5gwfs","key":"qfNcFxouaA"},{"type":"block","kind":"notebook-content","data":{"id":"J6av5OTi8tni"},"children":[{"type":"thematicBreak","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Rdl0KZhd5p"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"We use a neural network to represent the policy. The policy takes the current state as input and produces the probabilities of each action as the output. Intuitively, we might use the raw state coordinates, e.g., (0,1) or (0,3), as the input to the policy network. However, we use one-hot encoding on the state instead — so (0,1) becomes [0,1,0,0,0,...,0] and (0,3) becomes [0,0,0,1,0,...,0] — since raw coordinates make learning difficult for REINFORCE. The following section explains why.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"fYxJ9cs0B4"}],"key":"F6mBp3bmya"}],"identifier":"j6av5oti8tni","label":"J6av5OTi8tni","html_id":"j6av5oti8tni","key":"mys1a4RiSW"},{"type":"block","kind":"notebook-content","data":{"id":"4G58AVv64j7V"},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"What is One-Hot Encoding?","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XnZZlZSrRG"}],"key":"KfgBBIWVeB"}],"key":"FELRbsjngx"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"One-hot encoding represents a categorical value as a binary vector where only one element is 1 and all others are 0.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"YprMCf6cBr"}],"key":"QkEoViPzcC"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"For our 5×5 GridWorld, each cell/state becomes a vector of length 25:","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"T7nfPkoZIM"}],"key":"KI3PHbCh14"},{"type":"table","position":{"start":{"line":7,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"tableRow","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"tableCell","header":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"State","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"PZG01YKEHg"}],"key":"gIQ3FU6Wou"},{"type":"tableCell","header":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Index","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"FaB10XbtUL"}],"key":"bluOI3Vype"},{"type":"tableCell","header":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"One-hot vector","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"J7mdZXpTVW"}],"key":"NKuMXtgRAU"}],"key":"MmBE0rKDes"},{"type":"tableRow","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"(0,0)","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"nDmSEzjVeY"}],"key":"hSO2c5OSee"},{"type":"tableCell","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"0","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"ylt47xRPBn"}],"key":"WeFSzSdfVD"},{"type":"tableCell","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"mn7L4lbgWk"}],"key":"ohPvVNWECA"}],"key":"QymNZf2uBe"},{"type":"tableRow","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"(0,1)","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"gAm9Hyx1w4"}],"key":"PPIEPZC3fB"},{"type":"tableCell","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"1","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"rRJr2g7OH0"}],"key":"dxas505L17"},{"type":"tableCell","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"[0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"zNWIHBu4B3"}],"key":"KJeB8KGuT4"}],"key":"fVEOFmPjuu"},{"type":"tableRow","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"(2,3)","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"cqJUwTkABa"}],"key":"xo1uHrGnoD"},{"type":"tableCell","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"13","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"YdLzQTjDbO"}],"key":"wujtd6FLZx"},{"type":"tableCell","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"[0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0]","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"x4jgFnmnXf"}],"key":"Osc8f3Tc4c"}],"key":"VCUOvVncHm"}],"key":"jKTTJa1N5x"},{"type":"paragraph","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"strong","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Why One-Hot Encoding Helps","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"GB23iVOave"}],"key":"qvd4ZG5wPM"}],"key":"TKc4BFJY5Z"},{"type":"paragraph","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"With raw coordinates ","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"LjndKAdeAO"},{"type":"inlineCode","value":"[2,3]","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"OzFCFjDEVM"},{"type":"text","value":" vs ","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"AJ1E1dldpM"},{"type":"inlineCode","value":"[3,2]","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"pR1GjAMzAl"},{"type":"text","value":", the network sees these as similar inputs — both contain a 2 and a 3. The network has to figure out that swapping row and column completely changes which cell we’re in. This is hard to learn.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"OzmGNWYL2x"}],"key":"z7G8N8IpfW"},{"type":"paragraph","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"With one-hot encoding, every cell has its own unique “slot” in the input vector. Cell (2,3) lights up position 13, while cell (3,2) lights up position 17. There’s no confusion — the network sees completely different inputs for different cells.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"Eg1XFq89D1"}],"key":"EbdZTPnStL"},{"type":"paragraph","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"This is especially useful — and often necessary — for basic algorithms like REINFORCE. Because REINFORCE has high variance and no value function to stabilize learning, it struggles when the input representation is ambiguous. One-hot encoding removes this ambiguity, allowing the simple policy network to learn effectively. For this 5×5 GridWorld environment, REINFORCE struggles to converge without one-hot encoding.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"mon4xeEEXV"}],"key":"TIPyqQW0ET"},{"type":"paragraph","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"strong","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"Tradeoff","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"UMoGr4uw65"}],"key":"Syc6EaiGwW"}],"key":"nwLDXBELS6"},{"type":"paragraph","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"One-hot encoding works well for small discrete spaces like our 5×5 grid. For larger spaces (e.g., 100×100 grid or continuous states), it becomes impractical — that’s when you need raw coordinates with more sophisticated learning algorithms.","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"jDK1LsAlYV"}],"key":"wa1kAy8eNU"}],"identifier":"4g58avv64j7v","label":"4G58AVv64j7V","html_id":"id-4g58avv64j7v","key":"tMn30Dmqe9"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":176221,"status":"ok","timestamp":1765562139443,"user":{"displayName":"Jianguo Zhao","userId":"09996222056778326224"},"user_tz":420},"id":"0d98455d","outputId":"4fa5e844-0b41-47e4-f198-07bd80b80f00"},"children":[{"type":"code","lang":"python","executable":true,"value":"import torch # PyTorch library for building and training neural networks\nimport torch.nn as nn # Neural network module from PyTorch\nimport torch.optim as optim # Optimization algorithms from PyTorch\n\n# Create a function to convert state to one-hot:\ndef state_to_onehot(state, grid_shape=(5, 5)):\n    \"\"\"Convert (row, col) to one-hot vector\"\"\"\n    r, c = state\n    index = r * grid_shape[1] + c  # Flatten to single index\n    onehot = torch.zeros(grid_shape[0] * grid_shape[1])\n    onehot[index] = 1.0\n    return onehot.unsqueeze(0)\n\nLR = 0.0003 # Learning rate\nGAMMA = 0.99 # Discount factor\nEPISODES = 20000 # Number of training episodes\nMAX_STEPS = 50 # Max steps per episode\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n#----------------------------------------------\n# Initialize environment, policy network, and optimizer\n#-----------------------\nenv = GridWorld()\n\nstate_dim = env.grid_shape[0] * env.grid_shape[1]  # state dimensions: 25 for 5x5 grid\naction_dim = env.action_space.n # action dimensions: 5 actions\n\n# Define policy network using nn.Sequential\n# Input: 25-dimensional one-hot vector\n# Output: 5 logits (one per action)\n# Architecture: Linear(25 → 128) → ReLU → Linear(128 → 5)\npolicy_net = nn.Sequential(\n    nn.Linear(state_dim, 128),\n    nn.ReLU(),\n    nn.Linear(128, action_dim)\n).to(device)\n\n# Define optimizer which is used to update the policy network parameters\noptimizer = optim.Adam(policy_net.parameters(), lr=LR)\nreturn_list = []\n\n#----------------------------------------------\n# Training loop\n#----------------------------------------------\nprint(\"Training started...\")\nfor episode in range(EPISODES):\n    log_probs = []\n    rewards = []\n\n    state, info = env.reset()\n    episode_return = 0\n\n    # Generate an episode\n    for t in range(MAX_STEPS):\n        state_tensor = state_to_onehot(state, env.grid_shape).to(device)\n\n        # Get action logits from policy network\n        # logits = [f_θ(s,0), f_θ(s,1), ..., f_θ(s,4)] are raw scores for each action\n        logits = policy_net(state_tensor)\n\n        # Categorical(logits=logits) applies softmax internally:\n        # π_θ(a|s) = exp(f_θ(s,a)) / Σ_a' exp(f_θ(s,a'))\n        # This converts logits into a valid probability distribution over actions\n        action_dist = torch.distributions.Categorical(logits=logits)\n\n        # Sample an action from the distribution π_θ(a|s)\n        action = action_dist.sample()\n\n        # Compute log π_θ(a_t|s_t) for the sampled action (needed for policy gradient)\n        log_prob = action_dist.log_prob(action)\n\n        # Take the action in the environment\n        next_state, reward, terminated, truncated, _ = env.step(action.item())\n\n        # Store log probability and reward\n        log_probs.append(log_prob)\n        rewards.append(reward)\n\n        # Move to the next state\n        state = next_state\n        # Accumulate episode return\n        episode_return += reward\n\n        if terminated or truncated:\n            break\n\n    # Compute action values (returns) G_t for each time step t\n    returns = []\n    G = 0\n    for r in reversed(rewards):\n        G = r + GAMMA * G\n        returns.insert(0, G)\n    # Convert returns to tensor\n    returns = torch.tensor(returns, dtype=torch.float32).to(device)\n\n    # Compute loss -Σ log π(a_t|s_t) * G_t\n    loss = 0\n    for log_prob, G_t in zip(log_probs, returns):\n        loss += -log_prob * G_t # loss is a tensor in PyTorch since log_prob is a tensor\n\n    # Update policy (i.e., update the parameters of the policy network)\n    optimizer.zero_grad() # Clear previous gradients\n    loss.backward() # Backpropagate to compute gradients (since loss is a tensor, it has a .backward() method)\n    optimizer.step() # Update policy network parameters\n\n    return_list.append(episode_return)\n\n    if (episode+1) % 1000 == 0:\n        print(f\"Episode {episode+1}/{EPISODES}, Return: {episode_return:.2f}\")","identifier":"0d98455d-code","enumerator":"2","html_id":"id-0d98455d-code","key":"BAgEmmmRJI"},{"type":"outputs","id":"ww3aQPeUUOE0ugiO41E-b","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"Training started...\n"},"children":[],"identifier":"0d98455d-outputs-0","html_id":"id-0d98455d-outputs-0","key":"qrSci8HUsy"},{"type":"output","jupyter_data":{"name":"stderr","output_type":"stream","text":"/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  return datetime.utcnow().replace(tzinfo=utc)\n"},"children":[],"identifier":"0d98455d-outputs-1","html_id":"id-0d98455d-outputs-1","key":"a20OAyTFob"},{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"Episode 1000/20000, Return: -19.00\nEpisode 2000/20000, Return: -4.00\nEpisode 3000/20000, Return: 0.00\nEpisode 4000/20000, Return: 8.00\nEpisode 5000/20000, Return: 10.00\nEpisode 6000/20000, Return: 10.00\nEpisode 7000/20000, Return: 3.00\nEpisode 8000/20000, Return: 10.00\nEpisode 9000/20000, Return: 10.00\nEpisode 10000/20000, Return: 10.00\nEpisode 11000/20000, Return: 10.00\nEpisode 12000/20000, Return: 10.00\nEpisode 13000/20000, Return: 10.00\nEpisode 14000/20000, Return: 10.00\nEpisode 15000/20000, Return: 10.00\nEpisode 16000/20000, Return: 10.00\nEpisode 17000/20000, Return: 5.00\nEpisode 18000/20000, Return: 3.00\nEpisode 19000/20000, Return: 10.00\nEpisode 20000/20000, Return: 10.00\n"},"children":[],"identifier":"0d98455d-outputs-2","html_id":"id-0d98455d-outputs-2","key":"JBTvUMkgf8"}],"identifier":"0d98455d-outputs","html_id":"id-0d98455d-outputs","key":"PRp9W4xjCj"}],"identifier":"0d98455d","label":"0d98455d","html_id":"id-0d98455d","key":"Pyur5hJl9l"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/","height":645},"executionInfo":{"elapsed":263,"status":"ok","timestamp":1765561302107,"user":{"displayName":"Jianguo Zhao","userId":"09996222056778326224"},"user_tz":420},"id":"2b2z6afM09NQ","outputId":"02ee04c2-82ed-4f96-c9f1-07705e748a6f"},"children":[{"type":"code","lang":"python","executable":true,"value":"# Visualization\npolicy_dict = {}\nwith torch.no_grad():\n    for r in range(env.grid_shape[0]):\n        for c in range(env.grid_shape[1]):\n            s = (r, c)\n            if s == env.goal:\n                continue\n\n            s_tensor = state_to_onehot(s, env.grid_shape).to(device)\n\n            logits = policy_net(s_tensor)\n            best_action = torch.argmax(logits).item()\n            policy_dict[s] = best_action\n\nenv.render(policy_dict)","identifier":"2b2z6afm09nq-code","enumerator":"3","html_id":"id-2b2z6afm09nq-code","key":"aVWmukYDU8"},{"type":"outputs","id":"E78fbwW0dE9uF5tpG-mwG","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"97e7cd430e14705b566ec4cdaa00a3b7","path":"/97e7cd430e14705b566ec4cdaa00a3b7.png"},"text/plain":{"content":"<Figure size 700x600 with 1 Axes>","content_type":"text/plain"}}},"children":[],"identifier":"2b2z6afm09nq-outputs-0","html_id":"id-2b2z6afm09nq-outputs-0","key":"J5FkQYzKIZ"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":3,"metadata":{},"data":{"text/plain":{"content":"(<Figure size 700x600 with 1 Axes>,\n <Axes: title={'center': 'Policy and Values'}>)","content_type":"text/plain"}}},"children":[],"identifier":"2b2z6afm09nq-outputs-1","html_id":"id-2b2z6afm09nq-outputs-1","key":"xAq2aH9Vwm"}],"identifier":"2b2z6afm09nq-outputs","html_id":"id-2b2z6afm09nq-outputs","key":"gb8Ks9xFqh"}],"identifier":"2b2z6afm09nq","label":"2b2z6afM09NQ","html_id":"id-2b2z6afm09nq","key":"i6HkAmknbc"},{"type":"block","kind":"notebook-content","data":{"id":"RibQczcExKSb"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"RL using PPO (Proximal Policy Optimization)","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"B8rxshgwAE"}],"identifier":"rl-using-ppo-proximal-policy-optimization","label":"RL using PPO (Proximal Policy Optimization)","html_id":"rl-using-ppo-proximal-policy-optimization","implicit":true,"key":"ysNi5jwBCJ"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"The REINFORCE is simple and intuitive, which can be used to illustrate the basic concepts. We use it to illustrate how to use ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"JbmP2t2J8l"},{"type":"inlineCode","value":"Pytorch","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"Y2b6WCim6X"},{"type":"text","value":" to create neural networks. But REINFORCE generally suffers from high variance — the returns can vary significantly between episodes, making learning unstable and slow. Nowadays people normally used more advanced agorithms such as PPO (Proximal Policy Optimization) that addresses this issue. It uses a value function to reduce variance and includes a clipping mechanism to prevent overly large policy updates, resulting in more stable and efficient learning.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"u2riZethJ2"}],"key":"smVNwBoHK5"},{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Instead of implementing PPO from scratch, we will use ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"lcJFO10E50"},{"type":"link","url":"https://stable-baselines3.readthedocs.io/","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Stable Baselines3 (SB3)","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"U0AHnx7B8l"}],"urlSource":"https://stable-baselines3.readthedocs.io/","key":"wImCDChudP"},{"type":"text","value":", a popular library that provides reliable implementations of many reinforcement learning algorithms, including PPO, A2C, DQN, and more. With SB3, we can train an agent in just a few lines of code.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"mvv1sf131x"}],"key":"gWM5kZvh6P"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Colab comes with many commonly used libraries preinstalled—such as NumPy, PyTorch, pandas, and FFmpeg—but Stable Baselines3 is not included by default. So we need to install it manually using the command ","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"cB4oIOK1yM"},{"type":"inlineCode","value":"!pip install stable_baselines3","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"IqCb3sz9PH"},{"type":"text","value":".","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"CbQkgVprKp"}],"key":"svzpXo8wks"}],"identifier":"ribqczcexksb","label":"RibQczcExKSb","html_id":"ribqczcexksb","key":"SLbAtTBGez"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":106330,"status":"ok","timestamp":1765562300179,"user":{"displayName":"Jianguo Zhao","userId":"09996222056778326224"},"user_tz":420},"id":"KMnuK2hrs5np","outputId":"d3af861a-27f8-4cfa-9f97-46332a88b9a2"},"children":[{"type":"code","lang":"python","executable":true,"value":"try:\n    import stable_baselines3\nexcept ImportError:\n    !pip install stable_baselines3\n    import stable_baselines3\n\n# With the SB3 library and a Gymnasium-compatible environment, training can be done in just a few lines of code.\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\n\n# 1. create the environment\nenv = GridWorld()\n\n\n# 2. Initialize PPO model\npolicy_kwargs = dict(\n    net_arch=[16]  # Single hidden layer with 16 neurons\n)\nmodel = PPO(\"MlpPolicy\",  # Multi-layer perceptron policy\n            env,  # the environment\n            verbose=1,  # print out info during training\n            gamma=0.99,  # discount factor, if not provided, default is 0.99\n            learning_rate=1e-2,  # learning rate, if not provided, default is 3e-4\n            policy_kwargs=policy_kwargs # policy network architecture: if this is not provided, default is [64, 64]\n            )\n\n\n# 3. Train model\nmodel.learn(total_timesteps=50_000)\nprint(\"training finished\")\nmodel.save(\"ppo_gridworld\")","identifier":"kmnuk2hrs5np-code","enumerator":"4","html_id":"kmnuk2hrs5np-code","key":"ol3UBoAEN5"},{"type":"outputs","id":"HI8k3w7GQn-Wt_2BPcMAO","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"Using cuda device\nWrapping the env with a `Monitor` wrapper\nWrapping the env in a DummyVecEnv.\n"},"children":[],"identifier":"kmnuk2hrs5np-outputs-0","html_id":"kmnuk2hrs5np-outputs-0","key":"KtiNKQaSb4"},{"type":"output","jupyter_data":{"name":"stderr","output_type":"stream","text":"/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n  warnings.warn(\n"},"children":[],"identifier":"kmnuk2hrs5np-outputs-1","html_id":"kmnuk2hrs5np-outputs-1","key":"UpRv0VQKf3"},{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 51.2     |\n|    ep_rew_mean     | -54.1    |\n| time/              |          |\n|    fps             | 700      |\n|    iterations      | 1        |\n|    time_elapsed    | 2        |\n|    total_timesteps | 2048     |\n---------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 39.5        |\n|    ep_rew_mean          | -35.8       |\n| time/                   |             |\n|    fps                  | 538         |\n|    iterations           | 2           |\n|    time_elapsed         | 7           |\n|    total_timesteps      | 4096        |\n| train/                  |             |\n|    approx_kl            | 0.017991526 |\n|    clip_fraction        | 0.29        |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.59       |\n|    explained_variance   | -0.00408    |\n|    learning_rate        | 0.01        |\n|    loss                 | 44.1        |\n|    n_updates            | 10          |\n|    policy_gradient_loss | -0.0225     |\n|    value_loss           | 111         |\n-----------------------------------------\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 22.5       |\n|    ep_rew_mean          | -12.8      |\n| time/                   |            |\n|    fps                  | 519        |\n|    iterations           | 3          |\n|    time_elapsed         | 11         |\n|    total_timesteps      | 6144       |\n| train/                  |            |\n|    approx_kl            | 0.01518676 |\n|    clip_fraction        | 0.265      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -1.56      |\n|    explained_variance   | 0.122      |\n|    learning_rate        | 0.01       |\n|    loss                 | 63.8       |\n|    n_updates            | 20         |\n|    policy_gradient_loss | -0.029     |\n|    value_loss           | 124        |\n----------------------------------------\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 13.2       |\n|    ep_rew_mean          | -3.11      |\n| time/                   |            |\n|    fps                  | 510        |\n|    iterations           | 4          |\n|    time_elapsed         | 16         |\n|    total_timesteps      | 8192       |\n| train/                  |            |\n|    approx_kl            | 0.02179869 |\n|    clip_fraction        | 0.321      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -1.51      |\n|    explained_variance   | 0.113      |\n|    learning_rate        | 0.01       |\n|    loss                 | 51.5       |\n|    n_updates            | 30         |\n|    policy_gradient_loss | -0.0346    |\n|    value_loss           | 94.9       |\n----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 7.48        |\n|    ep_rew_mean          | 2.51        |\n| time/                   |             |\n|    fps                  | 505         |\n|    iterations           | 5           |\n|    time_elapsed         | 20          |\n|    total_timesteps      | 10240       |\n| train/                  |             |\n|    approx_kl            | 0.028445829 |\n|    clip_fraction        | 0.418       |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.44       |\n|    explained_variance   | 0.212       |\n|    learning_rate        | 0.01        |\n|    loss                 | 52.7        |\n|    n_updates            | 40          |\n|    policy_gradient_loss | -0.0473     |\n|    value_loss           | 78.5        |\n-----------------------------------------\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 5.32       |\n|    ep_rew_mean          | 4.69       |\n| time/                   |            |\n|    fps                  | 501        |\n|    iterations           | 6          |\n|    time_elapsed         | 24         |\n|    total_timesteps      | 12288      |\n| train/                  |            |\n|    approx_kl            | 0.03659422 |\n|    clip_fraction        | 0.527      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -1.24      |\n|    explained_variance   | 0.19       |\n|    learning_rate        | 0.01       |\n|    loss                 | 20.8       |\n|    n_updates            | 50         |\n|    policy_gradient_loss | -0.0554    |\n|    value_loss           | 33.2       |\n----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 4.8         |\n|    ep_rew_mean          | 6.49        |\n| time/                   |             |\n|    fps                  | 499         |\n|    iterations           | 7           |\n|    time_elapsed         | 28          |\n|    total_timesteps      | 14336       |\n| train/                  |             |\n|    approx_kl            | 0.049726743 |\n|    clip_fraction        | 0.551       |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1          |\n|    explained_variance   | 0.273       |\n|    learning_rate        | 0.01        |\n|    loss                 | 3.13        |\n|    n_updates            | 60          |\n|    policy_gradient_loss | -0.0582     |\n|    value_loss           | 15          |\n-----------------------------------------\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 3.73       |\n|    ep_rew_mean          | 7.83       |\n| time/                   |            |\n|    fps                  | 496        |\n|    iterations           | 8          |\n|    time_elapsed         | 32         |\n|    total_timesteps      | 16384      |\n| train/                  |            |\n|    approx_kl            | 0.05858105 |\n|    clip_fraction        | 0.451      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -0.84      |\n|    explained_variance   | 0.385      |\n|    learning_rate        | 0.01       |\n|    loss                 | 3.69       |\n|    n_updates            | 70         |\n|    policy_gradient_loss | -0.0684    |\n|    value_loss           | 6.19       |\n----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 3.66        |\n|    ep_rew_mean          | 8.94        |\n| time/                   |             |\n|    fps                  | 495         |\n|    iterations           | 9           |\n|    time_elapsed         | 37          |\n|    total_timesteps      | 18432       |\n| train/                  |             |\n|    approx_kl            | 0.053590894 |\n|    clip_fraction        | 0.397       |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -0.649      |\n|    explained_variance   | 0.287       |\n|    learning_rate        | 0.01        |\n|    loss                 | 1.69        |\n|    n_updates            | 80          |\n|    policy_gradient_loss | -0.0543     |\n|    value_loss           | 4.11        |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 3.57        |\n|    ep_rew_mean          | 9.53        |\n| time/                   |             |\n|    fps                  | 494         |\n|    iterations           | 10          |\n|    time_elapsed         | 41          |\n|    total_timesteps      | 20480       |\n| train/                  |             |\n|    approx_kl            | 0.066877574 |\n|    clip_fraction        | 0.284       |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -0.476      |\n|    explained_variance   | 0.284       |\n|    learning_rate        | 0.01        |\n|    loss                 | 1.46        |\n|    n_updates            | 90          |\n|    policy_gradient_loss | -0.0522     |\n|    value_loss           | 2.45        |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 3.18        |\n|    ep_rew_mean          | 9.52        |\n| time/                   |             |\n|    fps                  | 493         |\n|    iterations           | 11          |\n|    time_elapsed         | 45          |\n|    total_timesteps      | 22528       |\n| train/                  |             |\n|    approx_kl            | 0.038478956 |\n|    clip_fraction        | 0.198       |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -0.267      |\n|    explained_variance   | 0.176       |\n|    learning_rate        | 0.01        |\n|    loss                 | 0.0995      |\n|    n_updates            | 100         |\n|    policy_gradient_loss | -0.0387     |\n|    value_loss           | 1.08        |\n-----------------------------------------\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 3.15       |\n|    ep_rew_mean          | 9.72       |\n| time/                   |            |\n|    fps                  | 492        |\n|    iterations           | 12         |\n|    time_elapsed         | 49         |\n|    total_timesteps      | 24576      |\n| train/                  |            |\n|    approx_kl            | 0.03208817 |\n|    clip_fraction        | 0.146      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -0.198     |\n|    explained_variance   | 0.359      |\n|    learning_rate        | 0.01       |\n|    loss                 | 0.0345     |\n|    n_updates            | 110        |\n|    policy_gradient_loss | -0.0326    |\n|    value_loss           | 0.553      |\n----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 3.4         |\n|    ep_rew_mean          | 9.98        |\n| time/                   |             |\n|    fps                  | 491         |\n|    iterations           | 13          |\n|    time_elapsed         | 54          |\n|    total_timesteps      | 26624       |\n| train/                  |             |\n|    approx_kl            | 0.039202437 |\n|    clip_fraction        | 0.101       |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -0.147      |\n|    explained_variance   | 0.0184      |\n|    learning_rate        | 0.01        |\n|    loss                 | 0.171       |\n|    n_updates            | 120         |\n|    policy_gradient_loss | -0.0338     |\n|    value_loss           | 0.453       |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 3.25        |\n|    ep_rew_mean          | 9.93        |\n| time/                   |             |\n|    fps                  | 491         |\n|    iterations           | 14          |\n|    time_elapsed         | 58          |\n|    total_timesteps      | 28672       |\n| train/                  |             |\n|    approx_kl            | 0.036057074 |\n|    clip_fraction        | 0.11        |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -0.115      |\n|    explained_variance   | 0.0671      |\n|    learning_rate        | 0.01        |\n|    loss                 | 0.176       |\n|    n_updates            | 130         |\n|    policy_gradient_loss | -0.0291     |\n|    value_loss           | 0.158       |\n-----------------------------------------\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 3.35       |\n|    ep_rew_mean          | 9.64       |\n| time/                   |            |\n|    fps                  | 490        |\n|    iterations           | 15         |\n|    time_elapsed         | 62         |\n|    total_timesteps      | 30720      |\n| train/                  |            |\n|    approx_kl            | 0.06544271 |\n|    clip_fraction        | 0.095      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -0.108     |\n|    explained_variance   | -0.0545    |\n|    learning_rate        | 0.01       |\n|    loss                 | -0.0291    |\n|    n_updates            | 140        |\n|    policy_gradient_loss | -0.0319    |\n|    value_loss           | 0.0394     |\n----------------------------------------\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 3.53       |\n|    ep_rew_mean          | 9.94       |\n| time/                   |            |\n|    fps                  | 489        |\n|    iterations           | 16         |\n|    time_elapsed         | 66         |\n|    total_timesteps      | 32768      |\n| train/                  |            |\n|    approx_kl            | 0.20111111 |\n|    clip_fraction        | 0.217      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -0.105     |\n|    explained_variance   | 0.0301     |\n|    learning_rate        | 0.01       |\n|    loss                 | 0.115      |\n|    n_updates            | 150        |\n|    policy_gradient_loss | -0.0451    |\n|    value_loss           | 0.458      |\n----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 3.49        |\n|    ep_rew_mean          | 10          |\n| time/                   |             |\n|    fps                  | 489         |\n|    iterations           | 17          |\n|    time_elapsed         | 71          |\n|    total_timesteps      | 34816       |\n| train/                  |             |\n|    approx_kl            | 0.090977035 |\n|    clip_fraction        | 0.225       |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -0.144      |\n|    explained_variance   | -0.197      |\n|    learning_rate        | 0.01        |\n|    loss                 | 0.242       |\n|    n_updates            | 160         |\n|    policy_gradient_loss | -0.0229     |\n|    value_loss           | 0.312       |\n-----------------------------------------\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 3.04       |\n|    ep_rew_mean          | 9.89       |\n| time/                   |            |\n|    fps                  | 489        |\n|    iterations           | 18         |\n|    time_elapsed         | 75         |\n|    total_timesteps      | 36864      |\n| train/                  |            |\n|    approx_kl            | 0.18536812 |\n|    clip_fraction        | 0.303      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -0.0893    |\n|    explained_variance   | -0.671     |\n|    learning_rate        | 0.01       |\n|    loss                 | -0.014     |\n|    n_updates            | 170        |\n|    policy_gradient_loss | -0.0158    |\n|    value_loss           | 0.0296     |\n----------------------------------------\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 3.41       |\n|    ep_rew_mean          | 9.85       |\n| time/                   |            |\n|    fps                  | 488        |\n|    iterations           | 19         |\n|    time_elapsed         | 79         |\n|    total_timesteps      | 38912      |\n| train/                  |            |\n|    approx_kl            | 0.10521521 |\n|    clip_fraction        | 0.0876     |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -0.059     |\n|    explained_variance   | 0.243      |\n|    learning_rate        | 0.01       |\n|    loss                 | -0.0228    |\n|    n_updates            | 180        |\n|    policy_gradient_loss | -0.0206    |\n|    value_loss           | 0.0727     |\n----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 3.25        |\n|    ep_rew_mean          | 10          |\n| time/                   |             |\n|    fps                  | 488         |\n|    iterations           | 20          |\n|    time_elapsed         | 83          |\n|    total_timesteps      | 40960       |\n| train/                  |             |\n|    approx_kl            | 0.016596358 |\n|    clip_fraction        | 0.0819      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -0.0724     |\n|    explained_variance   | -0.00991    |\n|    learning_rate        | 0.01        |\n|    loss                 | 0.122       |\n|    n_updates            | 190         |\n|    policy_gradient_loss | -0.0268     |\n|    value_loss           | 0.419       |\n-----------------------------------------\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 3.19       |\n|    ep_rew_mean          | 9.9        |\n| time/                   |            |\n|    fps                  | 488        |\n|    iterations           | 21         |\n|    time_elapsed         | 88         |\n|    total_timesteps      | 43008      |\n| train/                  |            |\n|    approx_kl            | 0.08435978 |\n|    clip_fraction        | 0.114      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -0.0923    |\n|    explained_variance   | 0.0832     |\n|    learning_rate        | 0.01       |\n|    loss                 | 0.211      |\n|    n_updates            | 200        |\n|    policy_gradient_loss | -0.0262    |\n|    value_loss           | 0.282      |\n----------------------------------------\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 3.39       |\n|    ep_rew_mean          | 10         |\n| time/                   |            |\n|    fps                  | 488        |\n|    iterations           | 22         |\n|    time_elapsed         | 92         |\n|    total_timesteps      | 45056      |\n| train/                  |            |\n|    approx_kl            | 0.04183776 |\n|    clip_fraction        | 0.096      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -0.0867    |\n|    explained_variance   | -0.162     |\n|    learning_rate        | 0.01       |\n|    loss                 | -0.0301    |\n|    n_updates            | 210        |\n|    policy_gradient_loss | -0.0126    |\n|    value_loss           | 0.149      |\n----------------------------------------\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 7.13       |\n|    ep_rew_mean          | 9.82       |\n| time/                   |            |\n|    fps                  | 488        |\n|    iterations           | 23         |\n|    time_elapsed         | 96         |\n|    total_timesteps      | 47104      |\n| train/                  |            |\n|    approx_kl            | 0.09685129 |\n|    clip_fraction        | 0.0554     |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -0.0725    |\n|    explained_variance   | -5.2       |\n|    learning_rate        | 0.01       |\n|    loss                 | -0.00313   |\n|    n_updates            | 220        |\n|    policy_gradient_loss | -0.01      |\n|    value_loss           | 0.0103     |\n----------------------------------------\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 3.39       |\n|    ep_rew_mean          | 9.95       |\n| time/                   |            |\n|    fps                  | 487        |\n|    iterations           | 24         |\n|    time_elapsed         | 100        |\n|    total_timesteps      | 49152      |\n| train/                  |            |\n|    approx_kl            | 0.05895762 |\n|    clip_fraction        | 0.311      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -0.293     |\n|    explained_variance   | 0.0875     |\n|    learning_rate        | 0.01       |\n|    loss                 | 0.164      |\n|    n_updates            | 230        |\n|    policy_gradient_loss | -0.0478    |\n|    value_loss           | 0.463      |\n----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 8.77        |\n|    ep_rew_mean          | 9.79        |\n| time/                   |             |\n|    fps                  | 487         |\n|    iterations           | 25          |\n|    time_elapsed         | 104         |\n|    total_timesteps      | 51200       |\n| train/                  |             |\n|    approx_kl            | 0.038354233 |\n|    clip_fraction        | 0.139       |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -0.172      |\n|    explained_variance   | -0.206      |\n|    learning_rate        | 0.01        |\n|    loss                 | 0.00502     |\n|    n_updates            | 240         |\n|    policy_gradient_loss | -0.0204     |\n|    value_loss           | 0.125       |\n-----------------------------------------\ntraining finished\n"},"children":[],"identifier":"kmnuk2hrs5np-outputs-2","html_id":"kmnuk2hrs5np-outputs-2","key":"TbZL4CDL5T"}],"identifier":"kmnuk2hrs5np-outputs","html_id":"kmnuk2hrs5np-outputs","key":"AzAyjAciPo"}],"identifier":"kmnuk2hrs5np","label":"KMnuK2hrs5np","html_id":"kmnuk2hrs5np","key":"XzjREM5T7p"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/","height":645},"executionInfo":{"elapsed":751,"status":"ok","timestamp":1765561513674,"user":{"displayName":"Jianguo Zhao","userId":"09996222056778326224"},"user_tz":420},"id":"1oGfhSSHqb0g","outputId":"65d31236-9e1b-4fc6-ced3-9cdea3cd7ee2"},"children":[{"type":"code","lang":"python","executable":true,"value":"## Now let us render the learned policy\n\n# --- 1. Prepare to extract the policy ---\npolicy_dict = {}\nR, C = env.grid_shape\n\n# --- 2. Iterate over all states and find the best action ---\nfor r in range(R):\n    for c in range(C):\n        s = (r, c)\n\n        # Skip the goal state, as no action is taken there\n        if s == env.goal:\n            continue\n\n        # The input to the model's predict() must be a NumPy array.\n        state_np = np.array([r, c], dtype=np.int32)\n\n        # model.predict returns the action (0-4) and the hidden state (which we ignore)\n        best_action, _ = model.predict(state_np, deterministic=True)\n\n        # Store the greedy action for the state\n        policy_dict[s] = int(best_action)\n\n# --- 3. Render the policy ---\nenv.render(policy_dict)","identifier":"1ogfhsshqb0g-code","enumerator":"5","html_id":"id-1ogfhsshqb0g-code","key":"llY5SVj9kQ"},{"type":"outputs","id":"hUvRrw5kDLiaeE2FVGwEx","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"97e7cd430e14705b566ec4cdaa00a3b7","path":"/97e7cd430e14705b566ec4cdaa00a3b7.png"},"text/plain":{"content":"<Figure size 700x600 with 1 Axes>","content_type":"text/plain"}}},"children":[],"identifier":"1ogfhsshqb0g-outputs-0","html_id":"id-1ogfhsshqb0g-outputs-0","key":"b53EbhUsN9"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":5,"metadata":{},"data":{"text/plain":{"content":"(<Figure size 700x600 with 1 Axes>,\n <Axes: title={'center': 'Policy and Values'}>)","content_type":"text/plain"}}},"children":[],"identifier":"1ogfhsshqb0g-outputs-1","html_id":"id-1ogfhsshqb0g-outputs-1","key":"OOcTHySYSR"}],"identifier":"1ogfhsshqb0g-outputs","html_id":"id-1ogfhsshqb0g-outputs","key":"tclOSixMxQ"}],"identifier":"1ogfhsshqb0g","label":"1oGfhSSHqb0g","html_id":"id-1ogfhsshqb0g","key":"VGFdx3kzbq"},{"type":"block","kind":"notebook-content","data":{"id":"nUYrozo4vyNW"},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The rendered policy shows that PPO successfully learns an optimal policy for each state, efficiently reaching the goal while avoiding forbidden cells. The training results are also consistent across runs.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ttOSlbI9U5"}],"key":"zdP7dnKX6V"}],"identifier":"nuyrozo4vynw","label":"nUYrozo4vyNW","html_id":"nuyrozo4vynw","key":"Q6jeRIH3m6"},{"type":"block","kind":"notebook-content","data":{"id":"-VqR8MH04GVt"},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Task 2: RL for the Torque-Limited Pendulum","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ILl41drLCP"}],"identifier":"task-2-rl-for-the-torque-limited-pendulum","label":"Task 2: RL for the Torque-Limited Pendulum","html_id":"task-2-rl-for-the-torque-limited-pendulum","implicit":true,"key":"jSnMBiUYvD"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Lets continue the reinforcement learning on a more complicated environment provided by Gymnasium, the ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"LbdUCW0Can"},{"type":"link","url":"https://gymnasium.farama.org/environments/classic_control/pendulum/","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Pendulum","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"NO1uU5WB2v"}],"urlSource":"https://gymnasium.farama.org/environments/classic_control/pendulum/","key":"uhsFq3iaz9"},{"type":"text","value":".","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"wFnr31JUAa"}],"key":"JJH41WjsEx"},{"type":"image","url":"/d73e0a9e95444d39642c29948aad2225.gif","alt":"the pendulum","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"pfW9IEujpi","urlSource":"https://gymnasium.farama.org/_images/pendulum.gif"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"The pendulum swingup problem is similar to what we have investigated before in previous labs (such as trajectory optimization). The system consists of a pendulum attached at one end to a fixed point, and the other end being free. The pendulum starts in a random position and the goal is to apply torque on the pendulum to swing it up into an upright position, with its center of gravity right above the fixed point.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"PfCI4ZryzI"}],"key":"u2dm4l8gsO"}],"identifier":"-vqr8mh04gvt","label":"-VqR8MH04GVt","html_id":"id-vqr8mh04gvt","key":"lSI7vvwbty"},{"type":"block","kind":"notebook-content","data":{"id":"plVW-ezuQj2v"},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"If we use SB3 and the PPO algorithm, the training code for the Pendulum environment is very similar to what we used for GridWorld. This is the beauty of the Gymnasium API — once an environment follows the standard interface (reset, step, observation_space, action_space, render), the same training code works on various environments with minimal changes. Whether the environment is a simple grid, a physics simulation, or a complex game, SB3 handles it seamlessly. This modularity makes it easy to experiment with different environments without rewriting the training loop.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Cyrou5FXJH"}],"key":"hw9mUYWYUq"}],"identifier":"plvw-ezuqj2v","label":"plVW-ezuQj2v","html_id":"plvw-ezuqj2v","key":"de0exukZ2X"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":112034,"status":"ok","timestamp":1765563088504,"user":{"displayName":"Jianguo Zhao","userId":"09996222056778326224"},"user_tz":420},"id":"0415d9bb","outputId":"a7823ed6-bb90-414a-920f-76d4f37885e9"},"children":[{"type":"code","lang":"python","executable":true,"value":"import imageio # for creating videos\nimport gymnasium as gym # Gymnasium library for reinforcement learning environments\nfrom stable_baselines3 import PPO # PPO algorithm\nfrom stable_baselines3.common.env_util import make_vec_env # for creating vectorized environments\nimport numpy as np\nfrom IPython.display import Video # for displaying videos in Jupyter notebooks\n\n# 1. Create training env\n# Pendulum-v1 comes from Gymnasium's built-in environments\nenv = make_vec_env(\"Pendulum-v1\", n_envs=4)\n\n# 2. Train PPO model\nmodel = PPO(\n    \"MlpPolicy\",\n    env,\n    verbose=1,\n    learning_rate=3e-4,\n    gamma=0.99,\n)\n\nmodel.learn(total_timesteps=100_000) # the number of training timesteps can be adjusted\n\n# 3. Save the trained model\nmodel.save(\"ppo_pendulum_v1\")\nprint(\"Training finished and model saved.\")","identifier":"0415d9bb-code","enumerator":"6","html_id":"id-0415d9bb-code","key":"NaCYegXqO0"},{"type":"outputs","id":"yBe6UdXp9O3C06ho9iGga","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"Using cuda device\n"},"children":[],"identifier":"0415d9bb-outputs-0","html_id":"id-0415d9bb-outputs-0","key":"URuzg6rCaX"},{"type":"output","jupyter_data":{"name":"stderr","output_type":"stream","text":"/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n  warnings.warn(\n"},"children":[],"identifier":"0415d9bb-outputs-1","html_id":"id-0415d9bb-outputs-1","key":"V7VfKHd8Pz"},{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 200       |\n|    ep_rew_mean     | -1.23e+03 |\n| time/              |           |\n|    fps             | 3007      |\n|    iterations      | 1         |\n|    time_elapsed    | 2         |\n|    total_timesteps | 8192      |\n----------------------------------\n------------------------------------------\n| rollout/                |              |\n|    ep_len_mean          | 200          |\n|    ep_rew_mean          | -1.24e+03    |\n| time/                   |              |\n|    fps                  | 1447         |\n|    iterations           | 2            |\n|    time_elapsed         | 11           |\n|    total_timesteps      | 16384        |\n| train/                  |              |\n|    approx_kl            | 0.0033566756 |\n|    clip_fraction        | 0.0186       |\n|    clip_range           | 0.2          |\n|    entropy_loss         | -1.42        |\n|    explained_variance   | -0.00278     |\n|    learning_rate        | 0.0003       |\n|    loss                 | 2.43e+03     |\n|    n_updates            | 10           |\n|    policy_gradient_loss | -0.00206     |\n|    std                  | 1            |\n|    value_loss           | 7.17e+03     |\n------------------------------------------\n------------------------------------------\n| rollout/                |              |\n|    ep_len_mean          | 200          |\n|    ep_rew_mean          | -1.25e+03    |\n| time/                   |              |\n|    fps                  | 1229         |\n|    iterations           | 3            |\n|    time_elapsed         | 19           |\n|    total_timesteps      | 24576        |\n| train/                  |              |\n|    approx_kl            | 0.0031612394 |\n|    clip_fraction        | 0.0148       |\n|    clip_range           | 0.2          |\n|    entropy_loss         | -1.43        |\n|    explained_variance   | 0.013        |\n|    learning_rate        | 0.0003       |\n|    loss                 | 2.49e+03     |\n|    n_updates            | 20           |\n|    policy_gradient_loss | -0.00153     |\n|    std                  | 1.01         |\n|    value_loss           | 7.1e+03      |\n------------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 200         |\n|    ep_rew_mean          | -1.21e+03   |\n| time/                   |             |\n|    fps                  | 1146        |\n|    iterations           | 4           |\n|    time_elapsed         | 28          |\n|    total_timesteps      | 32768       |\n| train/                  |             |\n|    approx_kl            | 0.003019421 |\n|    clip_fraction        | 0.0252      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.43       |\n|    explained_variance   | 0.00143     |\n|    learning_rate        | 0.0003      |\n|    loss                 | 2.39e+03    |\n|    n_updates            | 30          |\n|    policy_gradient_loss | -0.00187    |\n|    std                  | 1.02        |\n|    value_loss           | 6.96e+03    |\n-----------------------------------------\n------------------------------------------\n| rollout/                |              |\n|    ep_len_mean          | 200          |\n|    ep_rew_mean          | -1.18e+03    |\n| time/                   |              |\n|    fps                  | 1102         |\n|    iterations           | 5            |\n|    time_elapsed         | 37           |\n|    total_timesteps      | 40960        |\n| train/                  |              |\n|    approx_kl            | 0.0025276672 |\n|    clip_fraction        | 0.0103       |\n|    clip_range           | 0.2          |\n|    entropy_loss         | -1.45        |\n|    explained_variance   | 0.000668     |\n|    learning_rate        | 0.0003       |\n|    loss                 | 1.75e+03     |\n|    n_updates            | 40           |\n|    policy_gradient_loss | -0.000468    |\n|    std                  | 1.03         |\n|    value_loss           | 5.34e+03     |\n------------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 200         |\n|    ep_rew_mean          | -1.16e+03   |\n| time/                   |             |\n|    fps                  | 1075        |\n|    iterations           | 6           |\n|    time_elapsed         | 45          |\n|    total_timesteps      | 49152       |\n| train/                  |             |\n|    approx_kl            | 0.003500969 |\n|    clip_fraction        | 0.0185      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.44       |\n|    explained_variance   | 0.000198    |\n|    learning_rate        | 0.0003      |\n|    loss                 | 1.46e+03    |\n|    n_updates            | 50          |\n|    policy_gradient_loss | -0.00148    |\n|    std                  | 1.01        |\n|    value_loss           | 4.5e+03     |\n-----------------------------------------\n------------------------------------------\n| rollout/                |              |\n|    ep_len_mean          | 200          |\n|    ep_rew_mean          | -1.14e+03    |\n| time/                   |              |\n|    fps                  | 1057         |\n|    iterations           | 7            |\n|    time_elapsed         | 54           |\n|    total_timesteps      | 57344        |\n| train/                  |              |\n|    approx_kl            | 0.0024898297 |\n|    clip_fraction        | 0.0156       |\n|    clip_range           | 0.2          |\n|    entropy_loss         | -1.43        |\n|    explained_variance   | 0.000101     |\n|    learning_rate        | 0.0003       |\n|    loss                 | 1.44e+03     |\n|    n_updates            | 60           |\n|    policy_gradient_loss | -0.00125     |\n|    std                  | 1            |\n|    value_loss           | 4.32e+03     |\n------------------------------------------\n------------------------------------------\n| rollout/                |              |\n|    ep_len_mean          | 200          |\n|    ep_rew_mean          | -1.17e+03    |\n| time/                   |              |\n|    fps                  | 1042         |\n|    iterations           | 8            |\n|    time_elapsed         | 62           |\n|    total_timesteps      | 65536        |\n| train/                  |              |\n|    approx_kl            | 0.0039441055 |\n|    clip_fraction        | 0.0312       |\n|    clip_range           | 0.2          |\n|    entropy_loss         | -1.43        |\n|    explained_variance   | 5.25e-05     |\n|    learning_rate        | 0.0003       |\n|    loss                 | 900          |\n|    n_updates            | 70           |\n|    policy_gradient_loss | -0.00199     |\n|    std                  | 1.01         |\n|    value_loss           | 3.2e+03      |\n------------------------------------------\n------------------------------------------\n| rollout/                |              |\n|    ep_len_mean          | 200          |\n|    ep_rew_mean          | -1.16e+03    |\n| time/                   |              |\n|    fps                  | 1031         |\n|    iterations           | 9            |\n|    time_elapsed         | 71           |\n|    total_timesteps      | 73728        |\n| train/                  |              |\n|    approx_kl            | 0.0036542653 |\n|    clip_fraction        | 0.0224       |\n|    clip_range           | 0.2          |\n|    entropy_loss         | -1.43        |\n|    explained_variance   | 1.6e-05      |\n|    learning_rate        | 0.0003       |\n|    loss                 | 946          |\n|    n_updates            | 80           |\n|    policy_gradient_loss | -0.00153     |\n|    std                  | 1.02         |\n|    value_loss           | 3.53e+03     |\n------------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 200         |\n|    ep_rew_mean          | -1.17e+03   |\n| time/                   |             |\n|    fps                  | 1023        |\n|    iterations           | 10          |\n|    time_elapsed         | 80          |\n|    total_timesteps      | 81920       |\n| train/                  |             |\n|    approx_kl            | 0.003453842 |\n|    clip_fraction        | 0.0322      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.45       |\n|    explained_variance   | 1.62e-05    |\n|    learning_rate        | 0.0003      |\n|    loss                 | 959         |\n|    n_updates            | 90          |\n|    policy_gradient_loss | -0.00253    |\n|    std                  | 1.04        |\n|    value_loss           | 2.65e+03    |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 200         |\n|    ep_rew_mean          | -1.13e+03   |\n| time/                   |             |\n|    fps                  | 1015        |\n|    iterations           | 11          |\n|    time_elapsed         | 88          |\n|    total_timesteps      | 90112       |\n| train/                  |             |\n|    approx_kl            | 0.003765117 |\n|    clip_fraction        | 0.0237      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.44       |\n|    explained_variance   | 9.54e-06    |\n|    learning_rate        | 0.0003      |\n|    loss                 | 880         |\n|    n_updates            | 100         |\n|    policy_gradient_loss | -0.00172    |\n|    std                  | 1.01        |\n|    value_loss           | 2.54e+03    |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 200         |\n|    ep_rew_mean          | -1.09e+03   |\n| time/                   |             |\n|    fps                  | 1009        |\n|    iterations           | 12          |\n|    time_elapsed         | 97          |\n|    total_timesteps      | 98304       |\n| train/                  |             |\n|    approx_kl            | 0.004754668 |\n|    clip_fraction        | 0.0329      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.45       |\n|    explained_variance   | 9.06e-06    |\n|    learning_rate        | 0.0003      |\n|    loss                 | 688         |\n|    n_updates            | 110         |\n|    policy_gradient_loss | -0.00212    |\n|    std                  | 1.04        |\n|    value_loss           | 1.7e+03     |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 200         |\n|    ep_rew_mean          | -1.09e+03   |\n| time/                   |             |\n|    fps                  | 1004        |\n|    iterations           | 13          |\n|    time_elapsed         | 106         |\n|    total_timesteps      | 106496      |\n| train/                  |             |\n|    approx_kl            | 0.002065629 |\n|    clip_fraction        | 0.0131      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.46       |\n|    explained_variance   | 5.19e-06    |\n|    learning_rate        | 0.0003      |\n|    loss                 | 399         |\n|    n_updates            | 120         |\n|    policy_gradient_loss | -0.000521   |\n|    std                  | 1.04        |\n|    value_loss           | 1.55e+03    |\n-----------------------------------------\nTraining finished and model saved.\n"},"children":[],"identifier":"0415d9bb-outputs-2","html_id":"id-0415d9bb-outputs-2","key":"bBLvhxalgm"}],"identifier":"0415d9bb-outputs","html_id":"id-0415d9bb-outputs","key":"xzx2DhmpJk"}],"identifier":"0415d9bb","label":"0415d9bb","html_id":"id-0415d9bb","key":"kXE5HpUTpA"},{"type":"block","kind":"notebook-content","data":{"id":"4Ht9Il4KR2oF"},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Now that training is complete, we can evaluate the learned policy.\nThe most intuitive way to do this is to run a full episode with the trained agent, record its behavior, and watch the video. This gives us a direct sense of how well the agent behaves.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gjkTSw7WB4"}],"key":"TOxhIqEB85"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"In Gymnasium-compatible environments, video recording is supported through the render() method, which allows us to capture each frame as the agent interacts with the environment. By saving these frames, we can create a short animation of the agent’s trajectory and assess the quality of the learned policy visually.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"dzuH5uXoGK"}],"key":"fyikmWROzR"}],"identifier":"4ht9il4kr2of","label":"4Ht9Il4KR2oF","html_id":"id-4ht9il4kr2of","key":"vfnPwnFzbO"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/","height":658},"executionInfo":{"elapsed":2281,"status":"ok","timestamp":1765567621012,"user":{"displayName":"Jianguo Zhao","userId":"09996222056778326224"},"user_tz":420},"id":"O3v29XR52bYs","outputId":"c20edfbe-efce-4515-a59e-9983c6b99a30"},"children":[{"type":"code","lang":"python","executable":true,"value":"# Create rendering env\nrender_env = gym.make(\n    \"Pendulum-v1\",\n    render_mode=\"rgb_array\", # to enable image rendering\n    max_episode_steps=3000\n)\n\n# Load the trained model in the previous cell\nmodel = PPO.load(\"ppo_pendulum_v1\")\n\n# Record video using the imageio library\nwriter = imageio.get_writer(\"pendulum_video.mp4\", fps=30)\n\n# Reset the environment\nobs, info = render_env.reset()\n\n# Run the trained model and record frames\nfor _ in range(300): # generate 300 frames\n    action, _ = model.predict(obs, deterministic=True) # get action from the trained model\n    obs, reward, terminated, truncated, info = render_env.step(action) # take action in env\n\n    frame = render_env.render()\n    writer.append_data(frame)\n\n    if terminated or truncated:\n        break\n\nwriter.close()\nrender_env.close()\nVideo(\"pendulum_video.mp4\", embed=True, width=600)","identifier":"o3v29xr52bys-code","enumerator":"7","html_id":"o3v29xr52bys-code","key":"PwcNYXpkSm"},{"type":"outputs","id":"fzmcuJiwaeWVKmJJ20LBg","children":[{"type":"output","jupyter_data":{"name":"stderr","output_type":"stream","text":"WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (500, 500) to (512, 512) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"},"children":[],"identifier":"o3v29xr52bys-outputs-0","html_id":"o3v29xr52bys-outputs-0","key":"C5MSAE3BKA"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":11,"metadata":{},"data":{"text/html":{"content_type":"text/html","hash":"e9d43cb760a3a717dbc5f03c8929e422","path":"/e9d43cb760a3a717dbc5f03c8929e422.html"},"text/plain":{"content":"<IPython.core.display.Video object>","content_type":"text/plain"}}},"children":[],"identifier":"o3v29xr52bys-outputs-1","html_id":"o3v29xr52bys-outputs-1","key":"rTUxXdGfOi"}],"identifier":"o3v29xr52bys-outputs","html_id":"o3v29xr52bys-outputs","key":"UGg5s0LHVa"}],"identifier":"o3v29xr52bys","label":"O3v29XR52bYs","html_id":"o3v29xr52bys","key":"HWLyDYinc2"},{"type":"block","kind":"notebook-content","data":{"id":"nEx7uNrrj7Vb"},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"SB3 allows you to customize many internal parameters of its algorithms. For example, you can modify the structure of the policy neural networks, adjust activation functions, or change other policy-related settings. These options are passed through the policy_kwargs argument.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xEI90xO7fA"}],"key":"u8Qhl5lkQX"},{"type":"code","lang":"python","value":"policy_kwargs = dict(\n    net_arch=[256, 256]   # two hidden layers of size 256 each\n)\n\nmodel = PPO(\n    \"MlpPolicy\",\n    env,\n    verbose=1,\n    learning_rate=3e-4,\n    gamma=0.99,\n    policy_kwargs=policy_kwargs\n)","position":{"start":{"line":3,"column":1},"end":{"line":16,"column":1}},"identifier":"nex7unrrj7vb-code","enumerator":"8","html_id":"nex7unrrj7vb-code","key":"ZmZ7FMPiTG"},{"type":"paragraph","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"It is also useful to experiment with key hyperparameters—such as the learning rate and the entropy coefficient—when creating the PPO model. Adjusting these values can significantly influence exploration and training stability.","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"ELaXlz8kwM"}],"key":"PrZ1VunUgC"},{"type":"paragraph","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"strong","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"text","value":"Guidelines for choosing hyperparameters:","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"AQC9nGaKYr"}],"key":"sMg4Z2dAna"}],"key":"VYN38rAmoy"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":22,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":22,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"strong","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"Learning rate (","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"OqJirwoLVL"},{"type":"inlineCode","value":"learning_rate","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"TBBKdvYElf"},{"type":"text","value":")","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"iA18wjTW4I"}],"key":"q0Jp20ueDs"},{"type":"text","value":": Controls how quickly the policy is updated. Typical values range from 1e-5 to 1e-2. Start with 3e-4 (the default) and decrease if training is unstable or increase if learning is too slow.","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"LWcG43fZg1"}],"key":"T9YAy5DcEW"}],"key":"fly4mC0By6"},{"type":"listItem","spread":true,"position":{"start":{"line":24,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"strong","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"text","value":"Discount factor (","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"zXXSYlRcjE"},{"type":"inlineCode","value":"gamma","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"cUeFawsi03"},{"type":"text","value":")","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"HoVSFsBEvM"}],"key":"CeNZdxTqvu"},{"type":"text","value":": Determines how much future rewards are valued. Use values close to 1.0 (e.g., 0.95–0.99) for tasks requiring long-term planning, and lower values (e.g., 0.9) for tasks with immediate rewards.","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"kwLqQaHdpc"}],"key":"WuNt72HQvg"}],"key":"hBx5QBrWmq"},{"type":"listItem","spread":true,"position":{"start":{"line":26,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"children":[{"type":"strong","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"children":[{"type":"text","value":"Entropy coefficient (","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"pDHgYm2s22"},{"type":"inlineCode","value":"ent_coef","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"yCeNetIA4K"},{"type":"text","value":")","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"kA3pw3hHgS"}],"key":"m2t7ITgf6M"},{"type":"text","value":": Encourages exploration by adding randomness to the policy. Use small positive values (e.g., 0.01–0.1) to promote exploration early in training, or 0.0 if the agent already explores sufficiently.","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"CHDMoedTQb"}],"key":"ecmW5ZOfG8"}],"key":"PFHip1IQcV"},{"type":"listItem","spread":true,"position":{"start":{"line":28,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"strong","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"text","value":"Network architecture (","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"cYEVPUUs90"},{"type":"inlineCode","value":"net_arch","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"QxpgG2B4zX"},{"type":"text","value":")","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"loeuY81AC6"}],"key":"UECiyEitWE"},{"type":"text","value":": Larger networks (e.g., [256, 256]) can model complex policies but may overfit or train slowly. Smaller networks (e.g., [64, 64]) are faster and work well for simpler tasks. Adjust based on task complexity.","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"dH0KyVMOSn"}],"key":"n3ZPxKYXOh"}],"key":"spXKSHxMJp"}],"key":"cfBFTaWE8Z"},{"type":"paragraph","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"text","value":"Tuning these parameters often requires experimentation. Start with defaults, observe training behavior, and adjust one parameter at a time based on performance and stability.","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"bHjJi4LCje"}],"key":"vEZyRS0d84"},{"type":"code","lang":"python","value":"model = PPO(\n    \"MlpPolicy\",\n    env,\n    verbose=1,\n    learning_rate=3e-4,\n    gamma=0.99,\n    ent_coef = 0.01\n)","position":{"start":{"line":32,"column":1},"end":{"line":41,"column":1}},"key":"HQCNUsqbQ9"}],"identifier":"nex7unrrj7vb","label":"nEx7uNrrj7Vb","html_id":"nex7unrrj7vb","key":"AQ375b0Pxp"},{"type":"block","kind":"notebook-content","data":{"id":"dyfqTWT14b_k"},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Task 3: Learning to stabilize the Cart Pole system","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LbMLU304i5"}],"identifier":"task-3-learning-to-stabilize-the-cart-pole-system","label":"Task 3: Learning to stabilize the Cart Pole system","html_id":"task-3-learning-to-stabilize-the-cart-pole-system","implicit":true,"key":"LKYpmdKyay"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"We have used model-based approach (e.g., LQR) to stabilize the cart pole system in previous labs. Now let’s try to use RL to stabilize it. For the cart-pole system, a pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pole is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart. The detailed description for the system can be found at ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"zFWmSyqp3j"},{"type":"link","url":"https://gymnasium.farama.org/environments/classic_control/cart_pole/","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"https://​gymnasium​.farama​.org​/environments​/classic​_control​/cart​_pole/","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"RY1RqFojkT"}],"urlSource":"https://gymnasium.farama.org/environments/classic_control/cart_pole/","key":"YWdMT99P5p"},{"type":"text","value":". The code implementation for the Cart-Pole Dynamics can be found here: ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"jen8BHgC2y"},{"type":"link","url":"https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Cart-Pole System","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"aNYrnRQXHB"}],"urlSource":"https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py","data":{"kind":"file","org":"Farama-Foundation","repo":"Gymnasium","reference":"main","file":"gymnasium/envs/classic_control/cartpole.py","raw":"https://raw.githubusercontent.com/Farama-Foundation/Gymnasium/main/gymnasium/envs/classic_control/cartpole.py"},"internal":false,"protocol":"github","key":"wLU659nO7y"}],"key":"unvrhjuOG9"},{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"The default Gym CartPole environment (e.g., CartPole-v1) is a stabilization task: the pole starts nearly upright, and the objective is to keep it balanced while keeping the cart near the center. Since the goal is to keep the pole upright for as long as possible, by default, a reward of +1 is given for every step taken, including the termination step.  An episode terminates when the pole falls beyond a small angle or the cart moves too far from the origin. We will learn to swing-up the pole in the next task.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"CjetMzRH0p"}],"key":"VeqVYhQ7dk"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"For this environment provided by Gymnasium, the default action discrete. In fact, is a ndarray with shape (1,) which can take values {0, 1} indicating the direction of the fixed force applied horizontly on the cart.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"ouO9UL9v74"}],"key":"dtITNpg9g1"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":10,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"0: Push cart to the left","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"bD2lRRVkVY"}],"key":"LjWsxKNJJo"}],"key":"xvEAPr1JYu"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"1: Push cart to the right","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"gArfFLcpVD"}],"key":"susYM07780"}],"key":"K7UtnkHkcP"}],"key":"q7TC6y8t7S"}],"identifier":"dyfqtwt14b_k","label":"dyfqTWT14b_k","html_id":"dyfqtwt14b-k","key":"bn0QRNaEGm"},{"type":"block","kind":"notebook-code","data":{"id":"-gzeXN2N6ENP"},"children":[{"type":"code","lang":"python","executable":true,"value":"import imageio\nimport gymnasium as gym\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\nimport numpy as np\nfrom IPython.display import Video\n\n\n# 1. Create training env\nenv = make_vec_env(\"CartPole-v1\", n_envs=1) #n_envs=1 means no vectorization\n\n# 2. Train PPO model\nmodel = PPO(\n    \"MlpPolicy\",\n    env,\n    verbose=1,\n    learning_rate=3e-4,\n    gamma=0.99,\n)\n\nmodel.learn(total_timesteps=300_000)\n\n# Save model\nmodel.save(\"ppo_CartPole_v1\")\nprint(\"Training finished and model saved.\")","identifier":"-gzexn2n6enp-code","enumerator":"9","html_id":"id-gzexn2n6enp-code","key":"NN69mkK91q"},{"type":"outputs","id":"AzQDX6nkIME0-3-lzL4et","children":[],"identifier":"-gzexn2n6enp-outputs","html_id":"id-gzexn2n6enp-outputs","key":"oH94xMhGG6"}],"identifier":"-gzexn2n6enp","label":"-gzeXN2N6ENP","html_id":"id-gzexn2n6enp","key":"UA3UPPdzGw"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/","height":453},"executionInfo":{"elapsed":1956,"status":"ok","timestamp":1764643352152,"user":{"displayName":"Han River","userId":"11034725958969210509"},"user_tz":420},"id":"bK3WzlBP78WW","outputId":"cb646e0f-fde5-4309-9de1-2c62cca189a4"},"children":[{"type":"code","lang":"python","executable":true,"value":"# 3. Create rendering env\nrender_env = gym.make(\n    \"CartPole-v1\",\n    render_mode=\"rgb_array\",\n    max_episode_steps=2000\n)\n\n# Load model\nmodel = PPO.load(\"ppo_CartPole_v1\")\n\n# 4. Record video\nwriter = imageio.get_writer(\"pendulum_video.mp4\", fps=30)\n\nobs, info = render_env.reset()\n\nfor _ in range(300):\n    action, _ = model.predict(obs, deterministic=True)\n    obs, reward, terminated, truncated, info = render_env.step(action)\n\n    frame = render_env.render()\n    writer.append_data(frame)\n\n    if terminated or truncated:\n        break\n\nwriter.close()\nrender_env.close()\nVideo(\"pendulum_video.mp4\", embed=True, width=600)","identifier":"bk3wzlbp78ww-code","enumerator":"10","html_id":"bk3wzlbp78ww-code","key":"YekajupBO0"},{"type":"outputs","id":"xWOFv86vvmYhmPGlWkwQw","children":[{"type":"output","jupyter_data":{"name":"stderr","output_type":"stream","text":"WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"},"children":[],"identifier":"bk3wzlbp78ww-outputs-0","html_id":"bk3wzlbp78ww-outputs-0","key":"YezHjBxoB2"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":41,"metadata":{},"data":{"text/html":{"content_type":"text/html","hash":"59ecec8ffb753a42e1289296cbe1d270","path":"/59ecec8ffb753a42e1289296cbe1d270.html"},"text/plain":{"content":"<IPython.core.display.Video object>","content_type":"text/plain"}}},"children":[],"identifier":"bk3wzlbp78ww-outputs-1","html_id":"bk3wzlbp78ww-outputs-1","key":"e2q5i9ELKR"}],"identifier":"bk3wzlbp78ww-outputs","html_id":"bk3wzlbp78ww-outputs","key":"INDqjyork5"}],"identifier":"bk3wzlbp78ww","label":"bK3WzlBP78WW","html_id":"bk3wzlbp78ww","key":"UmrwAu7uH4"},{"type":"block","kind":"notebook-content","data":{"id":"1PMFnat78nZ_"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Using Wrappers to covnert discrete actions to continous actions","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tIGXvSEiyo"}],"identifier":"using-wrappers-to-covnert-discrete-actions-to-continous-actions","label":"Using Wrappers to covnert discrete actions to continous actions","html_id":"using-wrappers-to-covnert-discrete-actions-to-continous-actions","implicit":true,"key":"DAK1CHx17p"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Now we can try to revise the default setting to make it similar to model-based approach. To do this, we will do two things through the wrapper of gym:","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"nMQrf91260"}],"key":"gJCSlpPsM1"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Convert the problem to be continuous","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"aDyfu6kmYX"}],"key":"YjieB0qxlD"}],"key":"z27WuJQSS5"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Use a new reward function similar to the cost function we have used in trajectory optimization","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"ccORCSydCW"}],"key":"WOIucO5Dv8"}],"key":"cc7e8sXc1t"}],"key":"WgbGyjX9Lm"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"strong","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"What is a Gym ","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"ngQDRN0sur"},{"type":"link","url":"https://gymnasium.farama.org/introduction/basic_usage/#modifying-the-environment","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Wrapper","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"zE0esWmvpF"}],"urlSource":"https://gymnasium.farama.org/introduction/basic_usage/#modifying-the-environment","key":"lF7JhzUP0x"},{"type":"text","value":"?","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"V9Sr8hkW9V"}],"key":"sZHCT6VVn8"}],"key":"f5wkzXcAXw"},{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"A wrapper is a design pattern that allows us to modify the behavior of an existing environment without creating an entirely new one from scratch. Wrappers “wrap around” the original environment and can modify:","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"yLLzez3FLy"}],"key":"khhdFTCiDX"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":11,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Action spaces (discrete → continuous)","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"zPEqlbt6pX"}],"key":"Vh68IGZoYm"}],"key":"li67VsR2z7"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Observation spaces","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"tI4F7wPGTv"}],"key":"xobOg5q5lm"}],"key":"wqsBRdUd2M"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Reward functions","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"GGFVjLsKaQ"}],"key":"k0902a5nsJ"}],"key":"PCo9DNS5RY"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Episode termination conditions","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"SNxiefXPuE"}],"key":"CMk5neT0uz"}],"key":"kZ1VWMrLr4"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Any other environment behavior","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"QRE41NuvNm"}],"key":"bG9s2gzf2P"}],"key":"NUxH8HSSxm"}],"key":"DumQhhcgeC"},{"type":"paragraph","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"strong","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"Why Use Wrappers Instead of Creating New Environments?","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"dIeSeEcuHE"}],"key":"EAGlaWvoLG"}],"key":"OY4f9iCQ96"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":18,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"Reusability","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"F6DLgF20hj"}],"key":"JJyUaYZKvF"},{"type":"text","value":": We can apply the same modifications to different base environments","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"pqtPy9RaCF"}],"key":"werKK0930h"}],"key":"CNdY4z7ull"},{"type":"listItem","spread":true,"position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"Modularity","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"xDaVIMRBJ2"}],"key":"TMV73PGhH3"},{"type":"text","value":": Each wrapper handles one specific modification, making code more organized","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"e24WdOMf4P"}],"key":"UEibaXCGto"}],"key":"ggXPGn3Vao"},{"type":"listItem","spread":true,"position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"text","value":"Maintainability","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"T6QAzwXrHM"}],"key":"u54Xjgpj8s"},{"type":"text","value":": Changes to the base environment automatically propagate to wrapped versions","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"zJQc2L0ZvL"}],"key":"GIpZv27IvO"}],"key":"HxSEFSc7xY"},{"type":"listItem","spread":true,"position":{"start":{"line":21,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"Efficiency","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"lylNHB7TqO"}],"key":"kksvvck8Z4"},{"type":"text","value":": We leverage the existing, well-tested Gymnasium implementation rather than reimplementing dynamics","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"GwCfBbIvYv"}],"key":"s5LO9l9AQH"}],"key":"Z2ov53C8ip"}],"key":"WDgNZ4Gi8j"},{"type":"paragraph","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"In our case, we’ll first use :","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"e1r6YGqqj3"}],"key":"UiDV49vuZN"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"ContinuousCartPoleWrapper","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"yjg6IyJZUB"},{"type":"text","value":": Converts discrete actions {0, 1} to continuous torque values","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"nd67e9NAqW"}],"key":"V5aIoYBgzO"}],"key":"CwQmHPVibk"}],"key":"HZMuO4dltL"}],"identifier":"1pmfnat78nz_","label":"1PMFnat78nZ_","html_id":"id-1pmfnat78nz","key":"erQnbW2NZY"},{"type":"block","kind":"notebook-code","data":{"id":"6af_YHrH9IeC"},"children":[{"type":"code","lang":"python","executable":true,"value":"import numpy as np\nimport gymnasium as gym\nfrom gymnasium import spaces\n\n\nclass ContinuousCartPoleWrapper(gym.Wrapper):\n    \"\"\"\n    Wrapper that turns CartPole-v1 into a continuous-control,\n    LQR-style stabilization task.\n\n    - Action: u in [-u_max, u_max]  (continuous force)\n    - State:  [x, x_dot, theta, theta_dot]\n    - Reward: r = -(x^T Q x + u^T R u) * dt, with optional terminal penalty.\n    \"\"\"\n\n    def __init__(\n        self,\n        env: gym.Env,\n        u_max: float = 10.0,\n        Q: np.ndarray | None = None,\n        R: float | None = None,\n        scale_with_dt: bool = True,\n        terminal_cost: float = 100.0,\n    ):\n        super().__init__(env)\n\n        # Continuous 1D action (force)\n        self.u_max = float(u_max)\n        self.action_space = spaces.Box(\n            low=np.array([-self.u_max], dtype=np.float32),\n            high=np.array([self.u_max], dtype=np.float32),\n            dtype=np.float32,\n        )\n\n        # Keep the original observation space (4-dim Box)\n        self.observation_space = env.observation_space\n\n        # Default LQR weights if not provided\n        # Emphasize pole angle θ and cart position x\n        if Q is None:\n            # state = [x, x_dot, theta, theta_dot]\n            Q = np.diag([1.0, 0.1, 10.0, 0.1])\n        if R is None:\n            R = 0.01  # scalar for u^2\n\n        self.Q = np.array(Q, dtype=np.float64)\n        self.R = float(R)\n\n        self.scale_with_dt = scale_with_dt\n        self.terminal_cost = float(terminal_cost)\n\n    # Just delegate reset to inner env and return same observation\n    def reset(self, **kwargs):\n        obs, info = self.env.reset(**kwargs)\n        return obs, info\n\n    def step(self, action):\n        # ---- 1) Continuous control input u ----\n        if isinstance(action, np.ndarray):\n            u = float(action.squeeze())\n        else:\n            u = float(action)\n        u = np.clip(u, -self.u_max, self.u_max)\n\n        # Use the *unwrapped* CartPole to access physics params and state\n        env = self.env.unwrapped\n\n        # ---- 2) Current state ----\n        x, x_dot, theta, theta_dot = env.state\n\n        # ---- 3) Dynamics from Gymnasium's CartPole (but with continuous u) ----\n        gravity = env.gravity\n        masscart = env.masscart\n        masspole = env.masspole\n        total_mass = masscart + masspole\n        length = env.length          # actually half the pole length\n        polemass_length = masspole * length\n        tau = env.tau                # time step\n\n        costheta = np.cos(theta)\n        sintheta = np.sin(theta)\n\n        # This is exactly CartPole's internal derivation, but with \"force = u\"\n        temp = (u + polemass_length * theta_dot**2 * sintheta) / total_mass\n        theta_acc = (gravity * sintheta - costheta * temp) / (\n            length * (4.0 / 3.0 - masspole * costheta**2 / total_mass)\n        )\n        x_acc = temp - polemass_length * theta_acc * costheta / total_mass\n\n        # Euler integration\n        x = x + tau * x_dot\n        x_dot = x_dot + tau * x_acc\n        theta = theta + tau * theta_dot\n        theta_dot = theta_dot + tau * theta_acc\n\n        env.state = (x, x_dot, theta, theta_dot)\n\n        # ---- 4) Termination condition (same as CartPole) ----\n        terminated = (\n            x < -env.x_threshold\n            or x > env.x_threshold\n            or theta < -env.theta_threshold_radians\n            or theta > env.theta_threshold_radians\n        )\n        # We don't enforce a time limit here; TimeLimit wrapper can be applied outside if you like\n        truncated = False\n\n        \"\"\"# ---- 5) LQR-like reward ----\n        state_vec = np.array([x, x_dot, theta, theta_dot], dtype=np.float64)\n        state_cost = state_vec @ self.Q @ state_vec\n        control_cost = self.R * (u ** 2)\n\n        cost = state_cost + control_cost\n        if self.scale_with_dt:\n            cost *= tau\n        reward = -float(cost)\n        \"\"\"\n        # ---- LQR-like reward with boundary shaping ----\n        state_vec = np.array([x, x_dot, theta, theta_dot], dtype=np.float64)\n        state_cost = state_vec @ self.Q @ state_vec\n        control_cost = self.R * (u ** 2)\n\n        # Soft barrier on cart position near limits\n        x_norm = abs(x) / env.x_threshold  # 0 at center, 1 at boundary\n        boundary_cost = 5.0 * (x_norm ** 4)  # smooth, grows steeply near boundary\n\n        # Extra angle robustness term (helps with large initial angles)\n        angle_cost = 0.5 * (1.0 - np.cos(theta)) * 20.0  # ~10 when upside down\n\n        cost = state_cost + control_cost + boundary_cost + angle_cost\n        if self.scale_with_dt:\n            cost *= tau\n\n        reward = -float(cost) # total reward is negative cost\n\n\n        # Add terminal cost when it fails\n        if terminated:\n            reward -= self.terminal_cost\n\n        # Add terminal cost when it fails\n        if terminated:\n            reward -= self.terminal_cost\n\n        obs = state_vec.astype(np.float32)\n        info = {\n            \"u\": u,\n            \"state_vec\": obs,\n            \"state_cost\": state_cost,\n            \"control_cost\": control_cost,\n            \"instant_cost\": cost,\n        }\n\n        return obs, reward, terminated, truncated, info\n\n    # For rendering, just delegate\n    def render(self):\n        return self.env.render()","identifier":"6af_yhrh9iec-code","enumerator":"11","html_id":"id-6af-yhrh9iec-code","key":"ouTdQFeJU0"},{"type":"outputs","id":"d_O9-oqTYKyZo2ACLJSdp","children":[],"identifier":"6af_yhrh9iec-outputs","html_id":"id-6af-yhrh9iec-outputs","key":"j2NYVzpNGr"}],"identifier":"6af_yhrh9iec","label":"6af_YHrH9IeC","html_id":"id-6af-yhrh9iec","key":"G99pse7A2c"},{"type":"block","kind":"notebook-content","data":{"id":"aeb07a5b"},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"With the wrapper, can can implement a continous version for the balance task.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wBEIuhMiSV"}],"key":"nOHZ9YfU94"}],"identifier":"aeb07a5b","label":"aeb07a5b","html_id":"aeb07a5b","key":"ieQVZ2ZWqj"},{"type":"block","kind":"notebook-code","data":{"id":"niW_nkS59tig","outputId":"1b05835c-10e7-4df5-f69a-7281ed9a2484"},"children":[{"type":"code","lang":"python","executable":true,"value":"import gymnasium as gym\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\n\n# Create the environment with continuous action wrapper\ndef make_env():\n    base_env = gym.make(\"CartPole-v1\")  # standard env\n    env = ContinuousCartPoleWrapper(base_env, u_max=20.0)\n    return env\n\nenv = make_vec_env(make_env, n_envs=1)\n\nmodel = PPO(\n    \"MlpPolicy\",\n    env,\n    verbose=1,\n    learning_rate=3e-4,\n    gamma=0.99\n)\n\nmodel.learn(total_timesteps=150_000)\nmodel.save(\"ppo_continuous_cartpole\")","identifier":"niw_nks59tig-code","enumerator":"12","html_id":"niw-nks59tig-code","key":"aY9KptFmAV"},{"type":"outputs","id":"k6AtmN3BYZdZP27N1w-a2","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"Using cpu device\n---------------------------------\n| rollout/...","hash":"9b60e534535b9a58fe466888dcbc49f0","path":"/9b60e534535b9a58fe466888dcbc49f0.txt"},"children":[],"identifier":"niw_nks59tig-outputs-0","html_id":"niw-nks59tig-outputs-0","key":"lpaqYUUKvr"}],"identifier":"niw_nks59tig-outputs","html_id":"niw-nks59tig-outputs","key":"eFByxOc3MF"}],"identifier":"niw_nks59tig","label":"niW_nkS59tig","html_id":"niw-nks59tig","key":"EfX6jpJj74"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/","height":453},"executionInfo":{"elapsed":1750,"status":"ok","timestamp":1764644183577,"user":{"displayName":"Han River","userId":"11034725958969210509"},"user_tz":420},"id":"QD8_5Hkr-rP7","outputId":"6f83d00a-09e9-4866-d1f1-b3f70125193d"},"children":[{"type":"code","lang":"python","executable":true,"value":"import imageio\nfrom IPython.display import Video\n\nrender_env = gym.make(\n    \"CartPole-v1\",\n    render_mode=\"rgb_array\",  # important for frame capture\n)\nrender_env = ContinuousCartPoleWrapper(render_env, u_max=20.0)\n\nmodel = PPO.load(\"ppo_continuous_cartpole\")\n\nwriter = imageio.get_writer(\"CartPole_LQR.mp4\", fps=30)\n\nobs, info = render_env.reset()\nfor _ in range(300):\n    action, _ = model.predict(obs, deterministic=True)\n    obs, reward, terminated, truncated, info = render_env.step(action)\n\n    frame = render_env.render()\n    writer.append_data(frame)\n\n    if terminated or truncated:\n        break\n\nwriter.close()\nrender_env.close()\n\nVideo(\"CartPole_LQR.mp4\", embed=True, width=600)","identifier":"qd8_5hkr-rp7-code","enumerator":"13","html_id":"qd8-5hkr-rp7-code","key":"ZTPgnNv7aB"},{"type":"outputs","id":"COle_cgEX9rfmoquV4iAs","children":[{"type":"output","jupyter_data":{"name":"stderr","output_type":"stream","text":"IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"},"children":[],"identifier":"qd8_5hkr-rp7-outputs-0","html_id":"qd8-5hkr-rp7-outputs-0","key":"cbLMCH8Y45"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":26,"metadata":{},"data":{"text/html":{"content":"<video controls  width=\"600\" >\n <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n Your browser does not support the video tag.\n </video>","content_type":"text/html"},"text/plain":{"content":"<IPython.core.display.Video object>","content_type":"text/plain"}}},"children":[],"identifier":"qd8_5hkr-rp7-outputs-1","html_id":"qd8-5hkr-rp7-outputs-1","key":"Fuq0d1SC3M"}],"identifier":"qd8_5hkr-rp7-outputs","html_id":"qd8-5hkr-rp7-outputs","key":"SWEEQVwBlZ"}],"identifier":"qd8_5hkr-rp7","label":"QD8_5Hkr-rP7","html_id":"qd8-5hkr-rp7","key":"LTEwEoH9Tw"},{"type":"block","kind":"notebook-content","data":{"id":"2f60563e"},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"From the video, you may notice that the motion of the cart pole does not vary too much. This is because in the default Gym env, the intial condition starts very close to the upright configuration. In fact:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"K3YDFqJKAb"}],"key":"AHK7jDyu3d"},{"type":"code","lang":"python","value":"high = np.array([0.05, 0.05, 0.05, 0.05])\nself.state = self.np_random.uniform(low=-high, high=high, size=(4,))","position":{"start":{"line":3,"column":1},"end":{"line":6,"column":1}},"identifier":"2f60563e-code","enumerator":"14","html_id":"id-2f60563e-code","key":"SJNPIJB8bb"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"This means:","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"NMYMsNnTlr"}],"key":"b3mVdGOYvA"},{"type":"table","position":{"start":{"line":10,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"tableRow","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"tableCell","header":true,"align":"center","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"State variable","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"K4AaEKVAUQ"}],"key":"h3EzxJYYMy"},{"type":"tableCell","header":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Meaning","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"cQ7k9OneAE"}],"key":"Y16pETWVgd"},{"type":"tableCell","header":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Initial range","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"llMQYGXiuX"}],"key":"UZHXzt7br2"}],"key":"MjKt3Exipp"},{"type":"tableRow","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"tableCell","align":"center","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"x","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"kZzBSqwTap"}],"key":"Ym4Im7Ox7c"},{"type":"tableCell","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"cart position","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"GPYyejykB0"}],"key":"ANjkhzRJqT"},{"type":"tableCell","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"uniform in [-0.05, +0.05]","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"SzG1INg2W4"}],"key":"iFWKWxwktk"}],"key":"CH74q3wi6J"},{"type":"tableRow","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"tableCell","align":"center","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"x_dot","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"V8DieEgGfM"}],"key":"DWqio8qSbR"},{"type":"tableCell","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"cart velocity","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"kCRhDCE0iY"}],"key":"gqv57a8O4m"},{"type":"tableCell","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"uniform in [-0.05, +0.05]","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"SqvdY65bEr"}],"key":"aWEl18G5FP"}],"key":"lsAxp2Efzc"},{"type":"tableRow","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"tableCell","align":"center","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"theta","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"vf3CCBwljd"}],"key":"NBg8lLuBzX"},{"type":"tableCell","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"pole angle","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"BakVLaFgxc"}],"key":"NZmTm4xeeY"},{"type":"tableCell","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"uniform in [-0.05, +0.05] rad","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"vmrgrR3SFh"}],"key":"Bun1U9qa9z"}],"key":"R1NOoW3mrb"},{"type":"tableRow","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"tableCell","align":"center","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"theta_dot","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"dlgl1lkR5O"}],"key":"x2wzhkPD2r"},{"type":"tableCell","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"pole angular velocity","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"uA0UdD0Dbg"}],"key":"PUeKFysTvh"},{"type":"tableCell","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"uniform in [-0.05, +0.05]","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"AOff1VPnXP"}],"key":"goNb8gxNhd"}],"key":"OpXEOMcuPj"}],"key":"By11IJpD0u"},{"type":"paragraph","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"And 0.05 rad ≈ 2.8° from vertical. This means the PPO agent is learning local stabilization very near the equilibrium.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"eSYl3frTXj"}],"key":"RtqxIk3X9J"},{"type":"paragraph","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"Let’s try to use another wrapper ","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"gXAsbUegwi"},{"type":"inlineCode","value":"WideInitWrapper","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"fcaXhaaBMk"},{"type":"text","value":" to train a policy that can handle large initial ranges.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"WBYPb3DMh3"}],"key":"Sr93ALcTSQ"}],"identifier":"2f60563e","label":"2f60563e","html_id":"id-2f60563e","key":"ednJi8tqZe"},{"type":"block","kind":"notebook-code","data":{"id":"c8554468"},"children":[{"type":"code","lang":"python","executable":true,"value":"class WideInitWrapper(gym.Wrapper):\n    \"\"\"\n    Reset with wider ranges so PPO sees large initial angles.\n    Ranges are tunable.\n    \"\"\"\n    def __init__(\n        self,\n        env,\n        x_range=1, # range for cart position\n        xdot_range=1, # range for cart velocity\n        theta_range=np.pi/10, # range for pole angle (up to ±18°)\n        thetadot_range=2.0, # range for pole angular velocity\n    ):\n        super().__init__(env)\n        self.x_range = x_range\n        self.xdot_range = xdot_range\n        self.theta_range = theta_range\n        self.thetadot_range = thetadot_range\n\n    def reset(self, **kwargs):\n        obs, info = self.env.reset(**kwargs)\n        rng = self.env.unwrapped.np_random\n\n        x = rng.uniform(-self.x_range, self.x_range)\n        x_dot = rng.uniform(-self.xdot_range, self.xdot_range)\n        theta = rng.uniform(-self.theta_range, self.theta_range)\n        theta_dot = rng.uniform(-self.thetadot_range, self.thetadot_range)\n\n        self.env.unwrapped.state = (x, x_dot, theta, theta_dot)\n        obs = np.array([x, x_dot, theta, theta_dot], dtype=np.float32)\n        return obs, info","identifier":"c8554468-code","enumerator":"15","html_id":"c8554468-code","key":"HX1OAxUZoy"},{"type":"outputs","id":"HscZKVJCpz2QkXAjLl5ux","children":[],"identifier":"c8554468-outputs","html_id":"c8554468-outputs","key":"OU9NlkVRCL"}],"identifier":"c8554468","label":"c8554468","html_id":"c8554468","key":"v9M515m0kE"},{"type":"block","kind":"notebook-code","data":{"id":"a2bd3c93","outputId":"8c55de49-a1c4-40d3-b5f1-1d09e9a223fb"},"children":[{"type":"code","lang":"python","executable":true,"value":"import gymnasium as gym\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\n\n# Create the environment with continuous action wrapper\ndef make_env():\n    base = gym.make(\"CartPole-v1\")\n    base = ContinuousCartPoleWrapper(base, u_max=20.0)  # or ContinuousLQRCartPoleWrapper\n    base = WideInitWrapper(base, theta_range=np.pi/10, thetadot_range=1.0)\n    return base\n\nenv = make_vec_env(make_env, n_envs=4)\n\nmodel = PPO(\n    \"MlpPolicy\",\n    env,\n    verbose=1,\n    learning_rate=3e-4,\n    gamma=0.99\n)\n\nmodel.learn(total_timesteps=500_000)\nmodel.save(\"ppo_continuous_cartpole\")","identifier":"a2bd3c93-code","enumerator":"16","html_id":"a2bd3c93-code","key":"GukTEyej4a"},{"type":"outputs","id":"MgEnDtA9NQwOvOFyW_Rnf","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"Using cpu device\n---------------------------------\n| rollout/...","hash":"a35eabb39db6f123e77cf733f7b81948","path":"/a35eabb39db6f123e77cf733f7b81948.txt"},"children":[],"identifier":"a2bd3c93-outputs-0","html_id":"a2bd3c93-outputs-0","key":"oDOnK8ROzU"}],"identifier":"a2bd3c93-outputs","html_id":"a2bd3c93-outputs","key":"mFdgECtVjl"}],"identifier":"a2bd3c93","label":"a2bd3c93","html_id":"a2bd3c93","key":"InASqXErLu"},{"type":"block","kind":"notebook-code","data":{"id":"7e14456c","outputId":"fd89d81a-64f0-42a1-c6ea-4b519de19caf"},"children":[{"type":"code","lang":"python","executable":true,"value":"# Now render the trained model with wide init\nimport imageio\nfrom IPython.display import Video\n\nrender_env = gym.make(\n    \"CartPole-v1\",\n    render_mode=\"rgb_array\",  # important for frame capture\n)\nrender_env = ContinuousCartPoleWrapper(render_env, u_max=20.0)\nrender_env = WideInitWrapper(render_env, theta_range=np.pi/10, thetadot_range=1.0)  # match training init\n\nmodel = PPO.load(\"ppo_continuous_cartpole\")\n\nwriter = imageio.get_writer(\"CartPole.mp4\", fps=30)\n\nobs, info = render_env.reset()\nfor _ in range(300):\n    action, _ = model.predict(obs, deterministic=True)\n    obs, reward, terminated, truncated, info = render_env.step(action)\n\n    frame = render_env.render()\n    writer.append_data(frame)\n\n    if terminated or truncated:\n        break\n\nwriter.close()\nrender_env.close()\n\nVideo(\"CartPole_LQR.mp4\", embed=True, width=600)","identifier":"7e14456c-code","enumerator":"17","html_id":"id-7e14456c-code","key":"EKI715nosG"},{"type":"outputs","id":"TqUgT8BYfRJoWM0cOi3tr","children":[{"type":"output","jupyter_data":{"name":"stderr","output_type":"stream","text":"IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"},"children":[],"identifier":"7e14456c-outputs-0","html_id":"id-7e14456c-outputs-0","key":"nwh7evCiVz"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":35,"metadata":{},"data":{"text/html":{"content":"<video controls  width=\"600\" >\n <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n Your browser does not support the video tag.\n </video>","content_type":"text/html"},"text/plain":{"content":"<IPython.core.display.Video object>","content_type":"text/plain"}}},"children":[],"identifier":"7e14456c-outputs-1","html_id":"id-7e14456c-outputs-1","key":"DqsqThqzmA"}],"identifier":"7e14456c-outputs","html_id":"id-7e14456c-outputs","key":"lQCM6kenau"}],"identifier":"7e14456c","label":"7e14456c","html_id":"id-7e14456c","key":"amNb4UNUB1"},{"type":"block","kind":"notebook-content","data":{"id":"947194b3"},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Task 4: Learning Swing-up for the Cart Pole System","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SSbCujMj25"}],"identifier":"task-4-learning-swing-up-for-the-cart-pole-system","label":"Task 4: Learning Swing-up for the Cart Pole System","html_id":"task-4-learning-swing-up-for-the-cart-pole-system","implicit":true,"key":"VNfF5z8raV"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"In this task, we train a RL agent to swing up a CartPole from the downward position to upright, then stabilize it. Unlike Task 3 (which focuses on stabilization from a near-upright starting position), this task addresses the more challenging swing-up problem.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"UROzy8ixUr"}],"key":"dTXhE407uk"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"strong","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Goal","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"iOco8chIyH"}],"key":"uJp8cUwPJj"},{"type":"text","value":": Learn a policy that swings the pole from hanging down (θ ≈ π) to upright (θ ≈ 0) and keeps it balanced.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"m9wvY3zT22"}],"key":"lY62Tk9Z3w"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"strong","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Approach","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"dR9FoyggHt"}],"key":"FTSdMwfF2s"},{"type":"text","value":":","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"nDSLpy4RWQ"}],"key":"Ov7WevApz8"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":8,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Design a custom reward function that encourages upright orientation (cos(θ)) while penalizing cart displacement, velocities, and control effort","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"rhx680KDr4"}],"key":"RI0QP740q2"}],"key":"uEnbCRRcXW"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Use ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"oG2ePGtAPi"},{"type":"inlineCode","value":"ContinuousSwingUpCartPoleWrapper","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"hOHgdVXU9b"},{"type":"text","value":" to apply continuous forces and define success/failure conditions","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"awDHofmnnM"}],"key":"hdP6yxW31S"}],"key":"LbWNx0FnCm"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Use ","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"uYoojXFDyl"},{"type":"inlineCode","value":"SwingUpInitWrapper","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"IvKQDoQxsM"},{"type":"text","value":" to initialize episodes with the pole near the downward position","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"HZ3H0Ss7bZ"}],"key":"WPXwBsF3HD"}],"key":"xkHTF8P51L"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Train PPO with appropriate hyperparameters and multiple parallel environments for stable learning","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"k3HerkaZEz"}],"key":"YRXYFCbdpi"}],"key":"oBXLpJvJ7P"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Visualize the learned behavior through video rendering","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"vTdUMLNLcE"}],"key":"qsBLkje4QL"}],"key":"s8zMf3iiGg"}],"key":"ezfRnhqynM"},{"type":"paragraph","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"This demonstrates how RL can solve complex non-linear control problems without explicit trajectory optimization, learning a smooth swing-up motion purely from experience.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"XGHNBfWjH1"}],"key":"lWI1xJFMft"}],"identifier":"947194b3","label":"947194b3","html_id":"id-947194b3","key":"XWnuBR3MTS"},{"type":"block","kind":"notebook-code","data":{"id":"5e12ef93"},"children":[{"type":"code","lang":"python","executable":true,"value":"import numpy as np\nimport gymnasium as gym\nfrom gymnasium import spaces\n\n\nclass ContinuousSwingUpCartPoleWrapper(gym.Wrapper):\n    \"\"\"\n    Continuous-action CartPole with swing-up style reward.\n\n    - Action: u in [-u_max, u_max]\n    - State:  [x, x_dot, theta, theta_dot]\n    - Reward: encourage upright (cos(theta)) and penalize big x, velocities, and control.\n    - Done:\n        * success when near-upright for several consecutive steps\n        * failure if cart goes out of bounds\n        * episode time limit is handled by an outer TimeLimit wrapper.\n    \"\"\"\n\n    def __init__(\n        self,\n        env: gym.Env,\n        u_max: float = 10.0,  # max force\n        x_fail_threshold: float = 2.4,  # cart position failure threshold\n        upright_theta_thresh: float = np.deg2rad(12.0),  # setpoint for \"upright\"\n        upright_theta_dot_thresh: float = 1.0,  # velocity threshold for \"upright\"\n        upright_x_thresh: float = 0.5,  # position threshold for \"upright\"\n        success_steps: int = 2,  # number of consecutive upright steps for success\n        success_bonus: float = 50.0,  # reward for achieving upright\n        failure_penalty: float = 50.0,  # penalty for failure\n    ):\n        super().__init__(env)\n\n        # Continuous 1D action space\n        self.u_max = float(u_max)\n        self.action_space = spaces.Box(\n            low=np.array([-self.u_max], dtype=np.float32),\n            high=np.array([self.u_max], dtype=np.float32),\n            dtype=np.float32,\n        )\n\n        # Same observation space as CartPole\n        self.observation_space = env.observation_space\n\n        # Swing-up termination thresholds\n        self.x_fail_threshold = float(x_fail_threshold)\n        self.upright_theta_thresh = float(upright_theta_thresh)\n        self.upright_theta_dot_thresh = float(upright_theta_dot_thresh)\n        self.upright_x_thresh = float(upright_x_thresh)\n\n        # Success condition (must hold upright for N consecutive steps)\n        self.success_steps = int(success_steps)\n        self.steps_upright = 0  # counter\n\n        self.success_bonus = float(success_bonus)\n        self.failure_penalty = float(failure_penalty)\n\n    def reset(self, **kwargs):\n        # Let another wrapper handle initial conditions if present\n        obs, info = self.env.reset(**kwargs)\n        self.steps_upright = 0\n        return obs, info\n\n    def step(self, action):\n        # ---- 1) Continuous control input u ----\n        if isinstance(action, np.ndarray):\n            u = float(action.squeeze())\n        else:\n            u = float(action)\n        u = np.clip(u, -self.u_max, self.u_max)\n\n        env = self.env.unwrapped\n\n        # ---- 2) Current state ----\n        x, x_dot, theta, theta_dot = env.state\n\n        # ---- 3) Dynamics (same as CartPole, but with force = u) ----\n        gravity = env.gravity\n        masscart = env.masscart\n        masspole = env.masspole\n        total_mass = masscart + masspole\n        length = env.length          # actually half pole length\n        polemass_length = masspole * length\n        tau = env.tau                # time step\n\n        costheta = np.cos(theta)\n        sintheta = np.sin(theta)\n\n        temp = (u + polemass_length * theta_dot**2 * sintheta) / total_mass\n        theta_acc = (gravity * sintheta - costheta * temp) / (\n            length * (4.0 / 3.0 - masspole * costheta**2 / total_mass)\n        )\n        x_acc = temp - polemass_length * theta_acc * costheta / total_mass\n\n        # Euler integration\n        x = x + tau * x_dot\n        x_dot = x_dot + tau * x_acc\n        theta = theta + tau * theta_dot\n        theta_dot = theta_dot + tau * theta_acc\n\n        env.state = (x, x_dot, theta, theta_dot)\n\n        # ---- 4) Termination: success/failure ----\n        # \"Upright\" region: near upright and reasonably calm\n        upright = (\n            abs(theta) < self.upright_theta_thresh\n            and abs(theta_dot) < self.upright_theta_dot_thresh\n            and abs(x) < self.upright_x_thresh\n        )\n\n        if upright:\n            self.steps_upright += 1\n        else:\n            self.steps_upright = 0\n\n        # Success if we've stayed upright long enough\n        success = self.steps_upright >= self.success_steps\n\n        # Failure: cart leaves the track\n        failure = abs(x) > self.x_fail_threshold\n\n        terminated = success or failure\n        truncated = False  # TimeLimit wrapper outside can handle max steps\n\n        # ---- 5) Swing-up reward ----\n        # Main term: cos(theta) is highest at upright (1) and lowest at downward (-1)\n        r_angle = np.cos(theta)\n\n        # Penalize cart displacement\n        r_x = -0.01 * (x**2)\n\n        # Stronger penalty on angular velocity\n        w_xdot = 0.01\n        w_thetadot = 0.03\n        r_vel = -(w_xdot * x_dot**2 + w_thetadot * theta_dot**2)\n\n        # Penalize control effort\n        r_u = -0.001 * (u**2)\n\n        # Penalize approaching boundaries\n        r_boundary = -5.0 * (abs(x) / self.env.unwrapped.x_threshold) ** 4\n\n        reward = float(r_angle + r_x + r_vel + r_u + r_boundary)\n\n        # Terminal bonuses/penalties\n        if success:\n            reward += self.success_bonus\n        if failure:\n            reward -= self.failure_penalty\n\n        obs = np.array([x, x_dot, theta, theta_dot], dtype=np.float32)\n        info = {\n            \"u\": u,\n            \"upright\": upright,\n            \"success\": success,\n            \"failure\": failure,\n            \"steps_upright\": self.steps_upright,\n        }\n\n        return obs, reward, terminated, truncated, info\n\n    def render(self):\n        return self.env.render()","identifier":"5e12ef93-code","enumerator":"18","html_id":"id-5e12ef93-code","key":"Ap6A3BtAax"},{"type":"outputs","id":"SGxF-UAyVCzd1logZyN7m","children":[],"identifier":"5e12ef93-outputs","html_id":"id-5e12ef93-outputs","key":"JVfkMfKYlf"}],"identifier":"5e12ef93","label":"5e12ef93","html_id":"id-5e12ef93","key":"d6EfmsY9Ys"},{"type":"block","kind":"notebook-code","data":{"id":"01e50731"},"children":[{"type":"code","lang":"python","executable":true,"value":"class SwingUpInitWrapper(gym.Wrapper):\n    \"\"\"\n    Override reset() so the pole starts near the downward position (theta ~ pi).\n    \"\"\"\n\n    # define the angle noise in degrees, default is 10 degrees\n    def __init__(self, env, angle_noise_deg: float = 10.0):\n        super().__init__(env)\n        self.angle_noise = np.deg2rad(angle_noise_deg)\n\n    def reset(self, **kwargs):\n        # Call base reset to set up RNG etc.\n        obs, info = self.env.reset(**kwargs)\n\n        rng = self.env.unwrapped.np_random\n\n        x = 0.0\n        x_dot = 0.0\n        # Set theta near pi (downward) with some noise\n        theta = np.pi + rng.uniform(-self.angle_noise, self.angle_noise)\n        theta_dot = 0.0\n\n        self.env.unwrapped.state = (x, x_dot, theta, theta_dot)\n\n        obs = np.array([x, x_dot, theta, theta_dot], dtype=np.float32)\n        return obs, info\n","identifier":"01e50731-code","enumerator":"19","html_id":"id-01e50731-code","key":"VtPPLlA6Dl"},{"type":"outputs","id":"U30p3F28ihoSotVn2kkoB","children":[],"identifier":"01e50731-outputs","html_id":"id-01e50731-outputs","key":"CPY9jleguv"}],"identifier":"01e50731","label":"01e50731","html_id":"id-01e50731","key":"Ka7N05c2ox"},{"type":"block","kind":"notebook-code","data":{"id":"b687d3d3","outputId":"3d1d7ded-21d9-48b9-c48a-46bea9a5055c"},"children":[{"type":"code","lang":"python","executable":true,"value":"import gymnasium as gym\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.monitor import Monitor\nimport matplotlib.pyplot as plt\n\n\n# 1) Build vectorized environment\ndef make_env():\n    base_env = gym.make(\"CartPole-v1\")\n    base_env = Monitor(base_env, log_dir)   # <-- log episodic stats\n    env = ContinuousSwingUpCartPoleWrapper(base_env, u_max=20.0)\n    env = SwingUpInitWrapper(env, angle_noise_deg=15.0)\n    return env\n\nenv = make_vec_env(make_env, n_envs=4)  # multiple envs helps PPO\n\n# 2) Train PPO\nmodel = PPO(\n    \"MlpPolicy\",\n    env,\n    verbose=1,\n    learning_rate=3e-4,\n    gamma=0.99,\n    n_steps=1024,\n    batch_size=64,\n)\n\nmodel.learn(total_timesteps=1_000_000)\nmodel.save(\"ppo_cartpole_swingup\")\nenv.close()","identifier":"b687d3d3-code","enumerator":"20","html_id":"b687d3d3-code","key":"WpeVZVz4YF"},{"type":"outputs","id":"lW5VRxNLIavhQ3U155bgw","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"Using cpu device\n----------------------------------\n| rollout...","hash":"cf0b47ac9ed4e5ae28fbede862be5f52","path":"/cf0b47ac9ed4e5ae28fbede862be5f52.txt"},"children":[],"identifier":"b687d3d3-outputs-0","html_id":"b687d3d3-outputs-0","key":"I2kjLa3hyG"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"d6f6b0e5ed7e0475b796405c63ced42f","path":"/d6f6b0e5ed7e0475b796405c63ced42f.png"},"text/plain":{"content":"<Figure size 600x400 with 1 Axes>","content_type":"text/plain"}}},"children":[],"identifier":"b687d3d3-outputs-1","html_id":"b687d3d3-outputs-1","key":"dasVYwkAvG"}],"identifier":"b687d3d3-outputs","html_id":"b687d3d3-outputs","key":"AojY92mEBk"}],"identifier":"b687d3d3","label":"b687d3d3","html_id":"b687d3d3","key":"eTS2PG4CKR"},{"type":"block","kind":"notebook-code","data":{"id":"d60de62f","outputId":"d120aacf-2bc4-44cf-8d54-3859ba47d9ea"},"children":[{"type":"code","lang":"python","executable":true,"value":"import imageio\nfrom stable_baselines3 import PPO\nfrom IPython.display import Video\n\n# 1) Rebuild render env with same wrappers\nrender_env = gym.make(\n    \"CartPole-v1\",\n    render_mode=\"rgb_array\",\n    max_episode_steps=5000,   # allow enough time for swing-up\n)\n\nrender_env = ContinuousSwingUpCartPoleWrapper(render_env, u_max=20.0)\nrender_env = SwingUpInitWrapper(render_env, angle_noise_deg=15.0)\n\n# 2) Load trained model\nmodel = PPO.load(\"ppo_cartpole_swingup\")\n\n# 3) Record video\nwriter = imageio.get_writer(\"CartPole_swingup.mp4\", fps=30)\n\nobs, info = render_env.reset()\n\nfor _ in range(5000):\n    action, _ = model.predict(obs, deterministic=True)\n    obs, reward, terminated, truncated, info = render_env.step(action)\n\n    frame = render_env.render()\n    writer.append_data(frame)\n\n    if terminated or truncated:\n        break\n\nwriter.close()\nrender_env.close()\n\nVideo(\"CartPole_swingup.mp4\", embed=True, width=600)\n","identifier":"d60de62f-code","enumerator":"21","html_id":"d60de62f-code","key":"P1J7YuwPos"},{"type":"outputs","id":"_zniy6v97625noIXhgIBk","children":[{"type":"output","jupyter_data":{"name":"stderr","output_type":"stream","text":"IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"},"children":[],"identifier":"d60de62f-outputs-0","html_id":"d60de62f-outputs-0","key":"Byx4VwzZEX"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":14,"metadata":{},"data":{"text/html":{"content_type":"text/html","hash":"04ff81cc8b425200832e3263912690ea","path":"/04ff81cc8b425200832e3263912690ea.html"},"text/plain":{"content":"<IPython.core.display.Video object>","content_type":"text/plain"}}},"children":[],"identifier":"d60de62f-outputs-1","html_id":"d60de62f-outputs-1","key":"IwTGttKPbG"}],"identifier":"d60de62f-outputs","html_id":"d60de62f-outputs","key":"Cx7HXkPgpL"}],"identifier":"d60de62f","label":"d60de62f","html_id":"d60de62f","key":"AECRqq0dzj"},{"type":"block","kind":"notebook-content","data":{"id":"11aeeb1c"},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Task 5: Hybrid Policy to Swing Up and then Stay","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Yx8XzbLkDj"}],"identifier":"task-5-hybrid-policy-to-swing-up-and-then-stay","label":"Task 5: Hybrid Policy to Swing Up and then Stay","html_id":"task-5-hybrid-policy-to-swing-up-and-then-stay","implicit":true,"key":"yDgJfzwERt"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"In this task, we combine two trained PPO policies: a swing-up policy and a stabilizer policy. We run the environment with ContinuousSwingUpCartPoleWrapper (continuous force, swing-up reward/termination) and initialize near the downward position using SwingUpInitWrapper. During the episode, we start in “swingup” mode and switch to “stabilize” when the state meets the upright criteria. After switching, actions come from stabilizer_model; before switching, actions come from swingup_model. We record the full run to a video (combined_cartpole.mp4) to visualize the handoff and final stabilization.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"HkIobj8efb"}],"key":"jl6U7bqNV5"}],"identifier":"11aeeb1c","label":"11aeeb1c","html_id":"id-11aeeb1c","key":"dWzMk7ZaDl"},{"type":"block","kind":"notebook-code","data":{"id":"f11b1401","outputId":"17582be1-edc3-4a2b-d5ec-b5259873abdf"},"children":[{"type":"code","lang":"python","executable":true,"value":"import numpy as np\nimport gymnasium as gym\nimport imageio\nfrom stable_baselines3 import PPO\nfrom IPython.display import Video\n\n\n# ---------- 1. Upright condition used for switching ----------\n\n# Tunable thresholds for the *handoff* to stabilizer\nTHETA_THRESH = np.deg2rad(3.0)   # ~3 degrees\nTHETA_DOT_THRESH = 1            # rad/s\nX_THRESH = 0.7                    # meters\n\n\ndef is_upright(obs) -> bool:\n    \"\"\"\n    Check if the current state is 'upright enough' to hand off\n    from the swing-up policy to the stabilizer policy.\n\n    obs = [x, x_dot, theta, theta_dot]\n    \"\"\"\n    x, x_dot, theta, theta_dot = obs\n    return (\n        abs(theta) < THETA_THRESH\n        and abs(theta_dot) < THETA_DOT_THRESH\n        and abs(x) < X_THRESH\n    )\n\n\n# ---------- 2. Load trained models ----------\nprint(\"Loading models...\")\nswingup_model = PPO.load(\"ppo_cartpole_swingup\")\nstabilizer_model = PPO.load(\"ppo_continuous_cartpole\")\nprint(\"Models loaded successfully\")\n\n\n# ---------- 3. Create evaluation environment ----------\nrender_env = gym.make(\n    \"CartPole-v1\",\n    render_mode=\"rgb_array\",\n    max_episode_steps=1000,\n)\n\n# Wrap with swing-up wrapper for physics\nrender_env = ContinuousSwingUpCartPoleWrapper(\n    render_env,\n    u_max=20.0,\n    x_fail_threshold=2.4,\n    success_steps=10**9,   # disable automatic success termination\n)\n\n# Start from downward position\nrender_env = SwingUpInitWrapper(render_env, angle_noise_deg=5.0)\n\n\n# ---------- 4. Run combined episode and record video ----------\nwriter = imageio.get_writer(\"combined_cartpole.mp4\", fps=30)\n\nobs, info = render_env.reset()\nmode = \"swingup\"\nprint(f\"Starting episode in mode: {mode}\")\nprint(f\"Initial state: x={obs[0]:.3f}, theta={obs[2]:.3f} rad ({np.rad2deg(obs[2]):.1f} deg)\")\n\nfor t in range(1000):\n    # Check if we should switch to stabilizer\n    if mode == \"swingup\" and is_upright(obs):\n        mode = \"stabilize\"\n        print(f\"[step {t}] Switching to STABILIZER policy\")\n        print(f\"  State: x={obs[0]:.3f}, theta={obs[2]:.3f} rad ({np.rad2deg(obs[2]):.1f} deg)\")\n\n    # Choose action based on current mode\n    if mode == \"swingup\":\n        action, _ = swingup_model.predict(obs, deterministic=True)\n    else:\n        action, _ = stabilizer_model.predict(obs, deterministic=True)\n\n    obs, reward, terminated, truncated, info = render_env.step(action)\n\n    # Record frame\n    frame = render_env.render()\n    writer.append_data(frame)\n\n    if terminated or truncated:\n        print(f\"Episode ended at step {t}\")\n        print(f\"  terminated={terminated}, truncated={truncated}\")\n        print(f\"  Final state: x={obs[0]:.3f}, theta={obs[2]:.3f} rad\")\n        break\n\nwriter.close()\nrender_env.close()\n\nprint(\"Video saved to combined_cartpole.mp4\")\nVideo(\"combined_cartpole.mp4\", embed=True, width=600)","identifier":"f11b1401-code","enumerator":"22","html_id":"f11b1401-code","key":"qVF7SXsHYc"},{"type":"outputs","id":"OHQj4ujacqACNYQrFgnyW","children":[{"type":"output","jupyter_data":{"name":"stderr","output_type":"stream","text":"IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"},"children":[],"identifier":"f11b1401-outputs-0","html_id":"f11b1401-outputs-0","key":"vgVEo3TZRF"},{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"Loading models...\nModels loaded successfully\nStarting episode in mode: swingup\nInitial state: x=0.000, theta=3.120 rad (178.8 deg)\n[step 339] Switching to STABILIZER policy\n  State: x=-0.510, theta=0.046 rad (2.6 deg)\nVideo saved to combined_cartpole.mp4\n"},"children":[],"identifier":"f11b1401-outputs-1","html_id":"f11b1401-outputs-1","key":"QNcBuXCj3R"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":41,"metadata":{},"data":{"text/html":{"content_type":"text/html","hash":"4d27e210c5c43dae5908586671d3a419","path":"/4d27e210c5c43dae5908586671d3a419.html"},"text/plain":{"content":"<IPython.core.display.Video object>","content_type":"text/plain"}}},"children":[],"identifier":"f11b1401-outputs-2","html_id":"f11b1401-outputs-2","key":"ICsoxS23Yo"}],"identifier":"f11b1401-outputs","html_id":"f11b1401-outputs","key":"EYqHsnZNEw"}],"identifier":"f11b1401","label":"f11b1401","html_id":"f11b1401","key":"n59BnA2aYf"}],"key":"vPT8hJfDue"},"references":{"cite":{"order":[],"data":{}}}}