{"version":"1","records":[{"hierarchy":{"lvl1":"Lab1: Introduction to Python"},"type":"lvl1","url":"/lab1-pythonbasics","position":0},{"hierarchy":{"lvl1":"Lab1: Introduction to Python"},"content":"","type":"content","url":"/lab1-pythonbasics","position":1},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl2":"Introduction:"},"type":"lvl2","url":"/lab1-pythonbasics#introduction","position":2},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl2":"Introduction:"},"content":"Python is a powerful programming language that has become a standard tool in machine learning, data science, and scientific computing. With excellent libraries like NumPy, SciPy, and Matplotlib, Python is ideal for engineering applications. In this course, we will explore how to use Python for simulating and controlling dynamic systems.\n\nIf your experience is mainly with Arduino or MATLAB, don’t worry—many concepts will feel familiar, and Python’s syntax is straightforward and beginner-friendly. With the support of modern tools and resources such as AI-assisted coding (e.g., Copilot), you’ll find that learning Python is both accessible and rewarding. Every mechanical engineer can become a confident Python coder!\n\nThis lab will only provide the basics for Python. You can try to find other resources online to learn more. For instance, this webpage provide more examples in the context of numerical methods:\n\n\nhttps://​numericalmethodssullivan​.github​.io​/ch​-python​.html\nHere is a more thorough tutorial from Python\n\n\nhttps://​docs​.python​.org​/3​/tutorial​/index​.html","type":"content","url":"/lab1-pythonbasics#introduction","position":3},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl2":"Learning Objectives:"},"type":"lvl2","url":"/lab1-pythonbasics#learning-objectives","position":4},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl2":"Learning Objectives:"},"content":"Learn the structures of data and commands in Python with NumPy\n\nGain knowledge and familiarity with Jupyter notebooks and Python scientific libraries\n\nLearn how to get help with Python and its libraries\n\nSolve mathematical problems with Python and plot the solutions\n\nWork with arrays, functions, and visualization tools\n\n","type":"content","url":"/lab1-pythonbasics#learning-objectives","position":5},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl2":"Task 1 – Basic Python Syntax"},"type":"lvl2","url":"/lab1-pythonbasics#task-1-basic-python-syntax","position":6},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl2":"Task 1 – Basic Python Syntax"},"content":"Before diving into NumPy and scientific computing, let’s cover some essential Python syntax that every engineer should know. Python’s syntax is clean and readable, making it an excellent choice for engineering applications.\n\n","type":"content","url":"/lab1-pythonbasics#task-1-basic-python-syntax","position":7},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Key Python Concepts:","lvl2":"Task 1 – Basic Python Syntax"},"type":"lvl3","url":"/lab1-pythonbasics#key-python-concepts","position":8},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Key Python Concepts:","lvl2":"Task 1 – Basic Python Syntax"},"content":"Variables and Data Types: Python automatically determines data types based on the value assigned, so you don’t need to declare types explicitly. This makes it easy to work with numbers, strings, booleans, and even complex numbers.\n\nLists and Dictionaries: Lists are ordered, mutable collections ideal for storing sequences of data, while dictionaries store key-value pairs for fast lookups and structured information. Both are fundamental for organizing and manipulating engineering data.\n\nControl Flow: if/else statements and loops (for, while) allow you to make decisions and repeat actions in your code. These constructs are essential for automating calculations and processing data sets.\n\nFunctions: Functions let you encapsulate reusable code blocks, making your programs modular and easier to maintain. Well-designed functions improve clarity and enable you to test engineering calculations independently.\n\nString Formatting: Modern f-string syntax (e.g., f\"Value: {variable}\") allows you to embed variables directly in strings for clear, readable output. This is especially useful for reporting results and debugging.\n\n# Variables and Basic Data Types\n# Python automatically determines the type based on the value assigned\n\n# Numbers\ninteger_var = 42\nfloat_var = 3.14159\ncomplex_var = 3 + 4j\n\n# Strings\nname = \"Mechanical Engineer\"\ncourse = 'MECH 529'\n\n# Boolean\nis_python_awesome = True\n\n# print the result: the 'f' before the string enables f-string formatting for variable substitution\nprint(f\"Integer: {integer_var}, type: {type(integer_var)}\")\nprint(f\"Float: {float_var}, type: {type(float_var)}\")\nprint(f\"Complex: {complex_var}, type: {type(complex_var)}\")\nprint(f\"String: {name}, type: {type(name)}\")\nprint(f\"Boolean: {is_python_awesome}, type: {type(is_python_awesome)}\")\n\n\n\n# Lists - ordered, mutable collections (like MATLAB cell arrays)\nforces = [100, 250, 75, 300]  # List of forces in Newtons\nmaterials = [\"Steel\", \"Aluminum\", \"Titanium\", \"Carbon Fiber\"]\n\nprint(f\"Forces: {forces}\")\nprint(f\"First force: {forces[0]} N\")  # Python uses 0-based indexing\nprint(f\"Last force: {forces[-1]} N\")  # Negative indexing from the end\n\n# Adding elements\nforces.append(150)  # Add to end\nprint(f\"After adding 150 N: {forces}\")\n\n# List slicing (very powerful!)\nprint(f\"First three forces: {forces[0:3]}\")  # Elements 0, 1, 2\nprint(f\"All forces: {forces[:]} or {forces}\")\n\n\n\n# Dictionaries - key-value pairs (like MATLAB structures)\nmaterial_properties = {\n    \"Steel\": {\"density\": 7850, \"yield_strength\": 250},  # kg/m³, MPa\n    \"Aluminum\": {\"density\": 2700, \"yield_strength\": 70},\n    \"Titanium\": {\"density\": 4500, \"yield_strength\": 880}\n}\n\nprint(\"Material Properties:\")\nfor material, props in material_properties.items():\n    density = props[\"density\"]\n    strength = props[\"yield_strength\"]\n    print(f\"  {material}: ρ = {density} kg/m³, σy = {strength:.0f} MPa\")\n\n\n\n# Control Flow: if/else statements\nforce = 500  # Newtons, you can change this value to test the if/else statements\nmax_allowable = 400  # Newtons\n\nif force > max_allowable:\n    safety_factor = force / max_allowable\n    print(f\"⚠️  Force {force} N exceeds limit of {max_allowable} N\")\n    print(f\"   Safety factor: {safety_factor:.2f}\")\nelif force == max_allowable:\n    print(f\"✓ Force exactly at limit: {force} N\")\nelse:\n    margin = max_allowable - force\n    print(f\"✓ Force {force} N is safe (margin: {margin} N)\")\n\n\n\n# Loops: for and while\nprint(\"For loop example - analyzing different loads:\")\nloads = [100, 250, 75, 450, 200]  # Newtons\n\nfor i, load in enumerate(loads):  # enumerate gives both index and value\n    status = \"SAFE\" if load < 400 else \"OVER LIMIT\"\n    print(f\"  Load {i+1}: {load} N - {status}\")\n\nprint(\"\\nWhile loop example - finding critical load:\")\nload = 100\nincrement = 50\nlimit = 400\n\nwhile load <= limit:\n    print(f\"  Testing load: {load} N\")\n    load += increment\n\nprint(f\"  Critical load exceeded at: {load} N\")\n\n\n\n# Functions - essential for reusable codes\ndef calculate_stress(force, area):\n    \"\"\"\n    This function calculates stress given force and area.\n\n    Parameters:\n    force (float): Applied force in Newtons\n    area (float): Cross-sectional area in m²\n\n    Returns:\n    float: Stress in Pascals\n    \"\"\"\n    if area <= 0:\n        raise ValueError(\"Area must be positive\")\n\n    stress = force / area\n    return stress\n\ndef safety_check(stress, yield_strength, safety_factor=2.0):\n    \"\"\"Check if design meets safety requirements\"\"\"\n    allowable_stress = yield_strength / safety_factor\n    is_safe = stress <= allowable_stress  # True if stress is less than or equal to allowable\n\n    return {\n        \"is_safe\": is_safe,\n        \"stress\": stress,\n        \"allowable\": allowable_stress,\n        \"utilization\": stress / allowable_stress\n    }\n\n# Example usage\nforce = 10000  # N\ndiameter = 0.02  # m\narea = 3.14159 * (diameter/2)**2  # m²\n\nstress = calculate_stress(force, area)\nresult = safety_check(stress, 250e6)  # Steel yield strength\n\nprint(f\"Engineering Analysis:\")\nprint(f\"  Applied force: {force} N\")\nprint(f\"  Cross-sectional area: {area*1e6:.1f} mm²\")\nprint(f\"  Calculated stress: {stress/1e6:.1f} MPa\")\nprint(f\"  Safety status: {'✓ SAFE' if result['is_safe'] else '⚠️ UNSAFE'}\")\n\n\n\n","type":"content","url":"/lab1-pythonbasics#key-python-concepts","position":9},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Key Takeaways:","lvl2":"Task 1 – Basic Python Syntax"},"type":"lvl3","url":"/lab1-pythonbasics#key-takeaways","position":10},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Key Takeaways:","lvl2":"Task 1 – Basic Python Syntax"},"content":"Python is readable: Code should be clear and self-documenting\n\nIndentation matters: Python uses indentation instead of braces {}\n\nZero-based indexing: Arrays/lists start at index 0, not 1 (unlike MATLAB)\n\nDynamic typing: Variables can change type, but be careful in engineering calculations\n\nF-strings: Use f\"Value: {variable}\" for modern string formatting\n\nFunctions: Always document with docstrings for engineering work (docstrings are multi-line string comments placed right after the function definition to describe its purpose, inputs, and outputs; this helps others understand and maintain your code)","type":"content","url":"/lab1-pythonbasics#key-takeaways","position":11},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Error Handling and Debugging","lvl2":"Task 1 – Basic Python Syntax"},"type":"lvl3","url":"/lab1-pythonbasics#error-handling-and-debugging","position":12},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Error Handling and Debugging","lvl2":"Task 1 – Basic Python Syntax"},"content":"Engineers need robust code that handles unexpected situations:\n\n# Error handling with try/except blocks\ndef safe_divide(a, b):\n    \"\"\"Safely divide two numbers with error handling\"\"\"\n    try:\n        result = a / b\n        return result\n    except ZeroDivisionError:\n        print(f\"Error: Cannot divide {a} by zero!\")\n        return None\n    except TypeError:\n        print(f\"Error: Invalid input types - need numbers, got {type(a)} and {type(b)}\")\n        return None\n\n# Test error handling\nprint(\"Testing error handling:\")\nprint(f\"10 / 2 = {safe_divide(10, 2)}\")\nprint(f\"10 / 0 = {safe_divide(10, 0)}\")\nprint(f\"'10' / 2 = {safe_divide('10', 2)}\")\n\n# Common debugging technique: assertions\ndef calculate_beam_deflection(force, length, E, I):\n    \"\"\"Calculate beam deflection with input validation\"\"\"\n    assert force > 0, \"Force must be positive\"\n    assert length > 0, \"Length must be positive\"\n    assert E > 0, \"Young's modulus must be positive\"\n    assert I > 0, \"Moment of inertia must be positive\"\n\n    # Simply supported beam, center load: δ = FL³/(48EI)\n    deflection = (force * length**3) / (48 * E * I)  # '**' is the exponentiation operator in Python (length cubed)\n    return deflection\n\n# Test with valid inputs\ntry:\n    # Steel beam, you may change the values to negative in the following line to oberserve the output\n    delta = calculate_beam_deflection(1000, 2, 200e9, 8.33e-6)\n    print(f\"Beam deflection: {delta*1000:.2f} mm\")\nexcept AssertionError as e:\n    print(f\"Input validation failed: {e}\")\n\n\n\n","type":"content","url":"/lab1-pythonbasics#error-handling-and-debugging","position":13},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl2":"Task 2 – Data Structures and Basic Operations"},"type":"lvl2","url":"/lab1-pythonbasics#task-2-data-structures-and-basic-operations","position":14},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl2":"Task 2 – Data Structures and Basic Operations"},"content":"NumPy is a fundamental Python library for numerical computing. It provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays efficiently. NumPy is widely used in scientific computing, data analysis, and engineering applications. Learn more: \n\nhttps://numpy.org/\n\nMatplotlib is a powerful Python library for creating static, animated, and interactive visualizations. It is widely used for plotting data, generating figures, and customizing charts in scientific and engineering applications. Learn more: \n\nhttps://​matplotlib​.org/. Here is a link with very useful cheetsheets for Matplotlib: \n\nhttps://​matplotlib​.org​/cheatsheets/\n\nLet’s start by importing these three libraries.\n\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n#import math\n\n\"\"\"\"\"\"\n# Set up matplotlib for inline plotting,\n# \"%\" is Jupyter magic command:\n# a special command that provides convenient shortcuts for common operations.\n# it is not a standard part of Python, but only for Jupyter notebooks,\n# which is how this lab is written with\n\"\"\"\"\"\"\n%matplotlib inline\n\n\n\n","type":"content","url":"/lab1-pythonbasics#task-2-data-structures-and-basic-operations","position":15},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Basic Mathematical Operations","lvl2":"Task 2 – Data Structures and Basic Operations"},"type":"lvl3","url":"/lab1-pythonbasics#basic-mathematical-operations","position":16},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Basic Mathematical Operations","lvl2":"Task 2 – Data Structures and Basic Operations"},"content":"You can use Python like a calculator:\n\n# Basic calculations\nresult1 = -1 - 1\nresult2 = 2**2\nprint(f\"-1 - 1 = {result1}\")\nprint(f\"2^2 = {result2}\")\n\n\n\n","type":"content","url":"/lab1-pythonbasics#basic-mathematical-operations","position":17},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Creating Arrays (equivalent to MATLAB matrices)","lvl2":"Task 2 – Data Structures and Basic Operations"},"type":"lvl3","url":"/lab1-pythonbasics#creating-arrays-equivalent-to-matlab-matrices","position":18},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Creating Arrays (equivalent to MATLAB matrices)","lvl2":"Task 2 – Data Structures and Basic Operations"},"content":"You can save answers by assigning them to variables and create arrays:\n\n# Create 1D arrays (equivalent to MATLAB row vectors)\nx1 = np.array([1, 2, 3])  # Creates a 1D array with 3 elements\nx2 = np.array([2j, 1+4j, np.pi, -1])  # Complex numbers and pi\nx3 = np.array([1+1, 2+2, 3+3])  # Array with calculations\n\nprint(\"x1:\", x1)\nprint(\"x2:\", x2)\nprint(\"x3:\", x3)\n\n\n\n# Create 2D arrays (equivalent to MATLAB matrices)\ny1 = np.array([[1, 2, 3], [4, 5, 6]])  # 2x3 matrix\ny2 = np.array([[1, 2, 3], [4, 5, 6]])  # Same as y1\ny3 = np.array([[1, 2, 3.0], [-4.0, 2-5, 6.00]])  # 2x3 matrix with mixed operations\n\nprint(\"y1:\")\nprint(y1)\nprint(\"\\ny2:\")\nprint(y2)\nprint(\"\\ny3:\")\nprint(y3)\n\n\n\n","type":"content","url":"/lab1-pythonbasics#creating-arrays-equivalent-to-matlab-matrices","position":19},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Matrix operations","lvl2":"Task 2 – Data Structures and Basic Operations"},"type":"lvl3","url":"/lab1-pythonbasics#matrix-operations","position":20},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Matrix operations","lvl2":"Task 2 – Data Structures and Basic Operations"},"content":"Now let’s do some basic matrix operations. NumPy provides essential matrix operations for engineering calculations.\n\nKey Matrix Operations:\n\nMatrix multiplication: A @ B\n\nElement-wise multiplication: A * B\n\nTranspose: A.T\n\nInverse: np.linalg.inv(A)\n\nDeterminant: np.linalg.det(A)\n\n# Basic Matrix Operations\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[2, 0], [1, 3]])\nvector = np.array([1, 2])\n\nprint(\"Matrix A:\")\nprint(A)\nprint(\"\\nMatrix B:\")\nprint(B)\n\n# Matrix multiplication (use @ operator)\nprint(\"\\nA @ B (matrix multiplication):\")\nprint(A @ B)\n\n# Element-wise multiplication (use * operator)\nprint(\"\\nA * B (element-wise multiplication):\")\nprint(A * B)\n\n# Matrix-vector multiplication\nprint(\"\\nA @ vector:\")\nprint(A @ vector)\n\n# Transpose\nprint(\"\\nA transpose (A.T):\")\nprint(A.T)\n\n# Matrix inverse and determinant\nprint(f\"\\nDeterminant of A: {np.linalg.det(A):.2f}\")\nA_inv = np.linalg.inv(A)\nprint(f\"Inverse of A:\")\nprint(A_inv)\n\n# Matrix eigenvalues and eigenvectors\neigvals, eigvecs = np.linalg.eig(A)\nprint(f\"\\nEigenvalues of A:\")\nprint(eigvals)\nprint(\"Eigenvectors of A (columns):\")\nprint(eigvecs)\n\n\n\n","type":"content","url":"/lab1-pythonbasics#matrix-operations","position":21},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl2":"Task 3 – Built-in Functions and Plotting"},"type":"lvl2","url":"/lab1-pythonbasics#task-3-built-in-functions-and-plotting","position":22},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl2":"Task 3 – Built-in Functions and Plotting"},"content":"Python with NumPy has many built-in functions similar to MATLAB. The general structure is:\nresult = function_name(input1, input2, ...)\n\n# Array operations\na = np.array([4, 9])\ny = np.sqrt(a)  # Square root\nz = np.array([np.max(a), np.min(a), np.mean(a)])  # Max, min, mean\n\nprint(f\"a = {a}\")\nprint(f\"sqrt(a) = {y}\")\nprint(f\"[max, min, mean] = {z}\")\nprint(f\"Size of a: {a.shape}\")\n\n\n\n","type":"content","url":"/lab1-pythonbasics#task-3-built-in-functions-and-plotting","position":23},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Common Mathematical Functions","lvl2":"Task 3 – Built-in Functions and Plotting"},"type":"lvl3","url":"/lab1-pythonbasics#common-mathematical-functions","position":24},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Common Mathematical Functions","lvl2":"Task 3 – Built-in Functions and Plotting"},"content":"Here are some commonly used functions:\n\n# Arc tangent\nangle = np.arctan(1)  # arctan(1) = π/4\nprint(f\"arctan(1) = {angle} radians = {np.degrees(angle)} degrees\")\n\n# Real and imaginary parts of complex numbers\ncomplex_num = 3 + 4j\nreal_part = np.real(complex_num)\nimag_part = np.imag(complex_num)\nprint(f\"Complex number: {complex_num}\")\nprint(f\"Real part: {real_part}\")\nprint(f\"Imaginary part: {imag_part}\")\n\n# Random numbers\nrandom_num = np.random.rand()  # Single random number between 0 and 1\nrandom_array = np.random.rand(3, 3)  # 3x3 array of random numbers\nprint(f\"Random number: {random_num}\")\nprint(f\"Random 3x3 array:\\n{random_array}\")\n\n\n\n","type":"content","url":"/lab1-pythonbasics#common-mathematical-functions","position":25},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"2D Plotting","lvl2":"Task 3 – Built-in Functions and Plotting"},"type":"lvl3","url":"/lab1-pythonbasics#id-2d-plotting","position":26},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"2D Plotting","lvl2":"Task 3 – Built-in Functions and Plotting"},"content":"\n\n# Create time array and plot sine function\nt = np.linspace(0, 10, 100) # Creates exactly 100 points from 0 to 10 (inclusive)\ny = 2 * np.sin(t)\n\n# Create a new figure with a specific size (10 inches wide by 6 inches tall)\nplt.figure(figsize=(10, 6))\nplt.plot(t, y)\n# plt.plot(t,y,'b-', linewidth=2)\nplt.xlabel('Time (t)')\nplt.ylabel('y = 2*sin(t)')\nplt.title('Sine Wave Plot')\n# Add a text annotation to the plot showing the equation\nplt.text(5, 1.5, r'y(t) = 2sin(t)', fontsize=12,\n         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\nplt.grid(True)\n\n\n\n","type":"content","url":"/lab1-pythonbasics#id-2d-plotting","position":27},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"3D Plotting","lvl2":"Task 3 – Built-in Functions and Plotting"},"type":"lvl3","url":"/lab1-pythonbasics#id-3d-plotting","position":28},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"3D Plotting","lvl2":"Task 3 – Built-in Functions and Plotting"},"content":"\n\n# 3D plotting example\n# Import 3D plotting toolkit for matplotlib (required for 3D surface plots)\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(10, 8)) # Create a new figure with a specific size\nax = fig.add_subplot(111, projection='3d')  # Add a 3D subplot to the figure\n\n# Create meshgrid for 3D surface\nx = np.linspace(-5, 5, 50)\ny = np.linspace(-5, 5, 50)\nX, Y = np.meshgrid(x, y)\nZ = np.sin(np.sqrt(X**2 + Y**2))\n\n# Create 3D surface plot\n# Plot the 3D surface; cmap sets the color map, alpha sets transparency\n# cmap='viridis' uses a blue-green-yellow color gradient for the surface\n# alpha=0.8 makes the surface slightly transparent (1.0 is fully opaque)\nsurf = ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.set_title('3D Surface Plot: Z = sin(√(X² + Y²))')\n# Add color bar to indicate the mapping of colors to Z values\nplt.colorbar(surf)\n\n\n\n","type":"content","url":"/lab1-pythonbasics#id-3d-plotting","position":29},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Plot with Custom Functions","lvl2":"Task 3 – Built-in Functions and Plotting"},"type":"lvl3","url":"/lab1-pythonbasics#plot-with-custom-functions","position":30},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Plot with Custom Functions","lvl2":"Task 3 – Built-in Functions and Plotting"},"content":"In Python, you can define custom functions easily:\n\ndef ME529_fun(fxn_input):\n    \"\"\"Custom function that combines sine wave with random noise\"\"\"\n    return 10 * np.sin(fxn_input * 10) + 5 * np.random.rand(*fxn_input.shape)\n\n# Use the custom function\ny_529 = ME529_fun(t)\n\nplt.figure(figsize=(10, 6))\nplt.plot(t, y_529, 'r-', linewidth=1.5)\nplt.xlabel('Time (t), sec')\nplt.ylabel('Function, f(t), -')\nplt.title('Custom Function: 10*sin(10t) + 5*random')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n","type":"content","url":"/lab1-pythonbasics#plot-with-custom-functions","position":31},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl2":"Task 4 – Debugging Techniques in Python"},"type":"lvl2","url":"/lab1-pythonbasics#task-4-debugging-techniques-in-python","position":32},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl2":"Task 4 – Debugging Techniques in Python"},"content":"Effective debugging is crucial for engineering applications where accuracy is paramount. Python offers several powerful debugging tools and techniques that every engineer should know. We will go through two methods.\n\n","type":"content","url":"/lab1-pythonbasics#task-4-debugging-techniques-in-python","position":33},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Method 1: Print statements and logging","lvl2":"Task 4 – Debugging Techniques in Python"},"type":"lvl3","url":"/lab1-pythonbasics#method-1-print-statements-and-logging","position":34},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Method 1: Print statements and logging","lvl2":"Task 4 – Debugging Techniques in Python"},"content":"\n\n# Method 1: Strategic Print Statements and Logging\n\ndef calculate_trajectory(v0, angle_deg, g=9.81):\n    \"\"\"Calculate projectile trajectory with debugging prints\"\"\"\n    print(f\"DEBUG: Input parameters - v0={v0}, angle={angle_deg}°, g={g}\")\n\n    # Convert angle to radians\n    angle_rad = np.radians(angle_deg)\n    print(f\"DEBUG: Angle in radians: {angle_rad:.4f}\")\n\n    # Calculate components\n    v0x = v0 * np.cos(angle_rad)\n    v0y = v0 * np.sin(angle_rad)\n    print(f\"DEBUG: Initial velocities - vx={v0x:.2f}, vy={v0y:.2f}\")\n\n    # Time of flight\n    t_flight = 2 * v0y / g\n    print(f\"DEBUG: Time of flight: {t_flight:.2f} seconds\")\n\n    # Range\n    range_x = v0x * t_flight\n    print(f\"DEBUG: Calculated range: {range_x:.2f} meters\")\n\n    return range_x, t_flight\n\n# Test with debugging output\nresult_range, result_time = calculate_trajectory(50, 45)\nprint(f\"\\nFINAL RESULT: Range = {result_range:.1f} m, Flight time = {result_time:.1f} s\")\n\n\n\n","type":"content","url":"/lab1-pythonbasics#method-1-print-statements-and-logging","position":35},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Method 2: Python Debugger (pdb) - Interactive Debugging","lvl2":"Task 4 – Debugging Techniques in Python"},"type":"lvl3","url":"/lab1-pythonbasics#method-2-python-debugger-pdb-interactive-debugging","position":36},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Method 2: Python Debugger (pdb) - Interactive Debugging","lvl2":"Task 4 – Debugging Techniques in Python"},"content":"The Python debugger allows you to pause execution, inspect variables, and step through code line by line.\n\nKey pdb commands:\n\nn (next): Execute next line\n\ns (step): Step into function calls\n\nl (list): Show current code\n\np variable_name: Print variable value\n\npp variable_name: Pretty print variable\n\nc (continue): Continue execution\n\nq (quit): Quit debugger\n\n# Method 2: Using pdb for interactive debugging\nimport pdb\n\ndef stress_analysis(force, area, material_strength):\n    \"\"\"Analyze stress with pdb debugging capabilities\"\"\"\n\n    # Uncomment the next line to set a breakpoint\n    #pdb.set_trace()  # Execution will pause here when uncommented\n\n    if area <= 0:\n        print(\"ERROR: Area must be positive!\")\n        return None\n\n    stress = force / area\n    safety_factor = material_strength / stress\n\n    # You can also use breakpoint() in Python 3.7+\n    # breakpoint()  # Modern way to set breakpoint\n\n    result = {\n        'stress': stress,\n        'safety_factor': safety_factor,\n        'is_safe': safety_factor >= 2.0\n    }\n\n    return result\n\n# Example usage (uncomment pdb.set_trace() above to see debugger in action)\nanalysis = stress_analysis(10000, 0.001, 250e6)  # 10kN on 1cm², steel\nif analysis:\n    print(f\"Stress: {analysis['stress']/1e6:.1f} MPa\")\n    print(f\"Safety factor: {analysis['safety_factor']:.2f}\")\n    print(f\"Safe design: {analysis['is_safe']}\")\n\n\n\n","type":"content","url":"/lab1-pythonbasics#method-2-python-debugger-pdb-interactive-debugging","position":37},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl2":"HW Problems"},"type":"lvl2","url":"/lab1-pythonbasics#hw-problems","position":38},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl2":"HW Problems"},"content":"Now that you’ve learned the basics, practice with these engineering-focused problems. Work through them step by step, using the debugging techniques you’ve learned. You need to submit your solutions to these HW problems with your source code.\n\n","type":"content","url":"/lab1-pythonbasics#hw-problems","position":39},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Problem 1: Basic Python Syntax and Control Flow","lvl2":"HW Problems"},"type":"lvl3","url":"/lab1-pythonbasics#problem-1-basic-python-syntax-and-control-flow","position":40},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Problem 1: Basic Python Syntax and Control Flow","lvl2":"HW Problems"},"content":"In this task, you will analyze material properties for design selection.\n\nTask: Write a program that:\n\nCreates a dictionary of materials with their properties (density, yield strength, cost per kg)\n\nFinds the material with the highest strength-to-weight ratio\n\nDetermines which materials are suitable for a given application (yield strength > 200 MPa, cost < $10/kg)\n\nUses proper error handling for invalid inputs\n\nGiven data:\n\nSteel: density=7850 kg/m³, yield=250 MPa, cost=$2/kg\n\nAluminum: density=2700 kg/m³, yield=70 MPa, cost=$8/kg\n\nTitanium: density=4500 kg/m³, yield=880 MPa, cost=$30/kg\n\nCarbon Fiber: density=1600 kg/m³, yield=600 MPa, cost=$50/kg\n\n# Problem 1 Solution Space\n# Write your solution here\n\n# Hint: Start by creating the materials dictionary\nmaterials = {\n    # Fill in the material properties\n}\n\n# Your code here...\n\n\n\n","type":"content","url":"/lab1-pythonbasics#problem-1-basic-python-syntax-and-control-flow","position":41},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Problem 2: NumPy Arrays and Mathematical Operations","lvl2":"HW Problems"},"type":"lvl3","url":"/lab1-pythonbasics#problem-2-numpy-arrays-and-mathematical-operations","position":42},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Problem 2: NumPy Arrays and Mathematical Operations","lvl2":"HW Problems"},"content":"In this problem, you will analyze experimental strain gauge data and plot them.\n\nTask: Given strain measurements from 5 gauges over 10 time steps:\n\nCreate a 2D NumPy array (10 time steps × 5 gauges) with realistic strain data\n\nCalculate the maximum, minimum, and average strain for each gauge\n\nFind the time step where the maximum overall strain occurred\n\nCalculate the stress using σ = E × ε (E = 200 GPa for steel)\n\nPlot strain vs time for all gauges on the same plot with different colors\n\nAdd proper labels, legend, and title\n\nHints:\n\nUse np.random.normal() to generate realistic strain data around 0.001 (1000 microstrain)\n\nUse np.argmax() to find indices of maximum values\n\nRemember to convert units properly (strain is dimensionless, stress in Pa)\n\n# Problem 2 Solution Space\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create time array\ntime = np.linspace(0, 10, 10)  # 10 time steps from 0 to 10 seconds\n\n# Generate strain data (your code here)\n# Hint: strain_data = np.random.normal(loc=0.001, scale=0.0002, size=(10, 5))\n\n# Your analysis code here...\n\n\n\n","type":"content","url":"/lab1-pythonbasics#problem-2-numpy-arrays-and-mathematical-operations","position":43},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Problem 3: Functions and Engineering Calculations","lvl2":"HW Problems"},"type":"lvl3","url":"/lab1-pythonbasics#problem-3-functions-and-engineering-calculations","position":44},{"hierarchy":{"lvl1":"Lab1: Introduction to Python","lvl3":"Problem 3: Functions and Engineering Calculations","lvl2":"HW Problems"},"content":"In this problem, you will practice how to define functions for designing a simply supported beam.\n\nTask: Create functions to analyze beam deflection and stress:\n\nFunction 1: calculate_beam_properties(width, height, length, material)\n\nInput: beam dimensions (m) and material name\n\nOutput: dictionary with area, moment of inertia, weight\n\nInclude error checking for positive dimensions\n\nFunction 2: analyze_point_load(load, position, beam_length, E, I)\n\nInput: point load (N), position (m), beam properties\n\nOutput: maximum deflection and stress\n\nUse: δ_max = (P×a×b×(L²-b²-a²)) / (6×E×I×L) for deflection\n\nUse: σ_max = M_max×c / I for stress (M_max = P×a×b/L, c = height/2)\n\nFunction 3: safety_check(actual_stress, actual_deflection, beam_props)\n\nCheck if stress < yield_strength/safety_factor (use SF=2.5)\n\nCheck if deflection < L/250 (standard deflection limit)\n\nReturn detailed safety analysis\n\nTest your functions with a steel beam (200×300 mm, 4m long) with 10kN load at 1.5m from left support.\n\nMaterial properties: Steel E=200 GPa, yield strength=250 MPa, density=7850 kg/m³\n\n# Problem 3 Solution Space\n\ndef calculate_beam_properties(width, height, length, material=\"steel\"):\n    \"\"\"\n    Calculate basic beam properties\n\n    Parameters:\n    width, height, length: beam dimensions in meters\n    material: material name (currently only steel implemented)\n\n    Returns:\n    dict: beam properties\n    \"\"\"\n    # Your code here\n    pass\n\ndef analyze_point_load(load, position, beam_length, E, I):\n    \"\"\"\n    Analyze beam under point load\n\n    Returns:\n    dict: deflection and stress analysis\n    \"\"\"\n    # Your code here\n    pass\n\ndef safety_check(actual_stress, actual_deflection, beam_props):\n    \"\"\"\n    Check beam safety factors\n\n    Returns:\n    dict: safety analysis results\n    \"\"\"\n    # Your code here\n    pass\n\n# Test case\nbeam_width = 0.2      # 200 mm\nbeam_height = 0.3     # 300 mm\nbeam_length = 4.0     # 4 m\napplied_load = 10000  # 10 kN\nload_position = 1.5   # 1.5 m from left\n\n# Your test code here...\n\n","type":"content","url":"/lab1-pythonbasics#problem-3-functions-and-engineering-calculations","position":45},{"hierarchy":{"lvl1":"Lab 10: RL: Policy Gradient Methods"},"type":"lvl1","url":"/lab10-rl","position":0},{"hierarchy":{"lvl1":"Lab 10: RL: Policy Gradient Methods"},"content":"This lab explores reinforcement learning (RL) techniques for learning control policies. We progress from simple policy gradient methods to advanced algorithms, applying them to increasingly complex control tasks.","type":"content","url":"/lab10-rl","position":1},{"hierarchy":{"lvl1":"Lab 10: RL: Policy Gradient Methods","lvl2":"Overview of Tasks"},"type":"lvl2","url":"/lab10-rl#overview-of-tasks","position":2},{"hierarchy":{"lvl1":"Lab 10: RL: Policy Gradient Methods","lvl2":"Overview of Tasks"},"content":"Task 1: REINFORCE on GridWorld: Implement the REINFORCE algorithm, a Monte-Carlo policy gradient method. We train a neural network policy using one-hot state encoding to navigate a 5×5 grid while avoiding obstacles and reaching a goal. This task will directly use Pytorch libabary to create neural networks.\n\nTask 2: PPO on Continuous Control (Pendulum): Apply Proximal Policy Optimization (PPO) from Stable Baselines3 to the pendulum swing-up problem. PPO offers more stable learning than REINFORCE through variance reduction.\n\nTask 3: CartPole Stabilization: Use PPO with custom wrappers to convert the discrete CartPole environment to continuous control. We learn a stabilization policy similar to LQR control, then extend it to handle larger initial conditions.\n\nTask 4: CartPole Swing-Up: Train a policy to swing the pole from the downward position to upright. This demonstrates how RL can solve complex non-linear control problems without explicit trajectory optimization.\n\nTask 5: Hybrid Policy (Swing-Up + Stabilization): Combine the swing-up and stabilization policies using a mode-switching strategy. The agent swings up using one policy, then switches to a stabilizer policy once near the goal—demonstrating hybrid control.\n\n","type":"content","url":"/lab10-rl#overview-of-tasks","position":3},{"hierarchy":{"lvl1":"Lab 10: RL: Policy Gradient Methods","lvl2":"Task1: REINFORCE on a Grid Environment"},"type":"lvl2","url":"/lab10-rl#task1-reinforce-on-a-grid-environment","position":4},{"hierarchy":{"lvl1":"Lab 10: RL: Policy Gradient Methods","lvl2":"Task1: REINFORCE on a Grid Environment"},"content":"This task introduces the policy gradient method: REINFORCE algorithm. We will use the same environment file (the GridWorld class) as defined in Lab 9. For policy learning, it is better that we randomize the starting position, instead of always starting at (0,0) as in Lab 9. This ensures the agent experiences all parts of the grid during training. If we always start from (0,0), the agent only learns a good policy for states along the path from (0,0) to the goal — it may never visit other cells and thus never learn what to do there. By randomizing the starting position, every cell gets visited, and the agent learns a complete policy for the entire grid. This becomes essential for larger and more complex environments, where a fixed starting position would leave most states unexplored.\n\nCheck the reset method in the GridWorld class to see how we randomize the starting position.\n\n# @title\nimport gymnasium as gym\nfrom gymnasium import spaces # spaces are used to define action and observation spaces\nimport numpy as np\n\n# Define the set of possible actions and their visual representations\nACTIONS = [0, 1, 2, 3, 4]  # up, right, down, left, stay\nACTION_ARROWS = {0:\"↑\", 1:\"→\", 2:\"↓\", 3:\"←\", 4:\"*\"}\n\n\nclass GridWorld(gym.Env):\n\n    \"\"\"Custom GridWorld environment compatible with Gymnasium API.\n    Note that Gymnasium environments are typically implemented as classes that inherit from gym.Env.\n    This allows them to integrate seamlessly with the Gymnasium framework,\n    which provides tools for reinforcement learning research and development.\n    \"\"\"\n\n    def __init__(self,\n                 grid_shape=(5, 5),\n                 forbidden=None,\n                 goal=None,\n                 goal_reward=10.0,\n                 step_reward=-0.01,\n                 forbidden_reward=-5.0,\n                 boundary_reward=-1.0,\n                 render_mode=None):\n        \"\"\"\n        Initializes the GridWorld environment.\n\n        Args:\n            grid_shape (tuple): The shape of the grid (rows, columns).\n            forbidden (list of tuples): A list of coordinates for forbidden states.\n            goal (tuple): The coordinates of the goal state.\n            goal_reward (float): The reward for reaching the goal.\n            step_reward (float): The reward for a regular step.\n            forbidden_reward (float): The reward for entering a forbidden state.\n            boundary_reward (float): The reward for hitting a boundary.\n            render_mode (str, optional): The mode for rendering the environment.\n        \"\"\"\n        super().__init__()\n\n        self.grid_shape = grid_shape\n        # Default locations for forbidden cells and the goal if not provided\n        self.forbidden = forbidden if forbidden is not None else [(1,1), (1,2),(3,1), (3,3) ,(4,1)]\n        # Example forbidden positions\n        self.goal = goal if goal is not None else (3,2)\n        # Example goal position\n\n        # Reward structure\n        self.goal_reward = goal_reward\n        self.step_reward = step_reward\n        self.forbidden_reward = forbidden_reward\n        self.boundary_reward = boundary_reward\n\n        # Discount factor\n        self.render_mode = render_mode\n\n        # Agent's current position, initialized in reset()\n        self.agent_pos = None\n\n        # Define Gymnasium spaces\n        # The action space is discrete with a size equal to the number of actions.\n        self.action_space = spaces.Discrete(len(ACTIONS))\n        # The observation space represents the agent's position on the grid.\n        self.observation_space = spaces.MultiDiscrete(list(grid_shape))\n\n\n    def in_bounds(self, s):\n        \"\"\"Check if a state 's' is within the grid boundaries.\"\"\"\n        r, c = s\n        R, C = self.grid_shape\n        return 0 <= r < R and 0 <= c < C\n\n    def is_forbidden(self, s):\n        \"\"\"Check if a state 's' is a forbidden state.\"\"\"\n        return s in self.forbidden\n\n    def states(self):\n        \"\"\"Return a list of all possible states (coordinates) in the grid.\"\"\"\n        valid_states = []\n        R, C = self.grid_shape\n        for r in range(R):\n            for c in range(C):\n                valid_states.append((r,c))\n        return valid_states\n\n    def reset(self, seed=None, options=None):\n        \"\"\"\n        Reset the environment to a RANDOM valid position.\n        \"\"\"\n        super().reset(seed=seed)\n\n        # Loop until we find a valid starting position\n        while True:\n            # Randomly pick a row and column\n            r = np.random.randint(self.grid_shape[0])\n            c = np.random.randint(self.grid_shape[1])\n            s = (r, c)\n\n            # Ensure we don't start in a forbidden cell or on the goal\n            # if not self.is_forbidden(s) and s != self.goal:\n            #     self.agent_pos = s\n            #     break\n            if s != self.goal:\n                self.agent_pos = s\n                break\n\n        obs = np.array(self.agent_pos, dtype=np.int32)\n        return obs, {}\n\n    def step(self, action):\n        \"\"\"\n        Execute one timestep in the environment.\n        The agent takes an action, and the environment transitions to a new state.\n\n        Args:\n            action (int): The action to be taken by the agent.\n\n        Returns:\n            tuple: A tuple containing the new observation, the reward,\n                   a boolean indicating if the episode is terminated,\n                   a boolean indicating if the episode is truncated,\n                   and an info dictionary.\n        \"\"\"\n        assert self.action_space.contains(action), f\"Invalid action {action}\"\n\n        r, c = self.agent_pos\n\n        # Map the action to a new position (nr, nc for new row, new col)\n        if action == 0:    nr, nc = r - 1, c     # up\n        elif action == 1:  nr, nc = r, c + 1     # right\n        elif action == 2:  nr, nc = r + 1, c     # down\n        elif action == 3:  nr, nc = r, c - 1     # left\n        else:              nr, nc = r, c         # stay\n\n        # Check for environment boundaries\n        terminated, truncated, info = False, False, {}\n        next_pos = (nr, nc)\n        if not self.in_bounds(next_pos):\n            next_pos = self.agent_pos  # Agent stays in the same position\n            reward = self.boundary_reward\n        elif self.is_forbidden(next_pos):\n            # next_pos = self.agent_pos  # Agent stays in the same position\n            reward = self.forbidden_reward\n        elif next_pos == self.goal:\n            reward = self.goal_reward\n            terminated = True\n        elif action == 4:\n            reward = -1\n        else:\n            reward = 0\n\n        # Update the agent's position\n        self.agent_pos = next_pos\n\n        # Prepare the return values\n        obs = np.array(self.agent_pos, dtype=np.int32)\n\n        return obs, reward, terminated, truncated, info\n\n    def render(self, P, V=None, ax=None, show=True):\n        \"\"\"\n        Renders the greedy policy as arrows and overlays the value for each state in a matplotlib plot.\n        This function will be used as a class method for the env class.\n\n        Args:\n            P (dict): The policy mapping states (r, c) to actions (int).\n            V (dict): Optional. A dictionary mapping states to their values.\n            ax: Optional matplotlib axes object. If provided, plots on this axes instead of creating a new figure.\n            show (bool): Whether to call plt.show() at the end. Default True. Set False when creating subplots.\n\n        Returns:\n            tuple: (fig, ax) - The figure and axes objects.\n        \"\"\"\n        import matplotlib.pyplot as plt\n        import numpy as np\n\n        R, C = self.grid_shape\n        if ax is None:\n            fig, ax = plt.subplots(figsize=(7, 6))\n        else:\n            fig = ax.figure\n        ax.set_xlim(-0.5, C - 0.5)\n        ax.set_ylim(-0.5, R - 0.5)\n        ax.set_xticks(np.arange(-0.5, C, 1))\n        ax.set_yticks(np.arange(-0.5, R, 1))\n        ax.grid(True, color='gray', linewidth=1, linestyle='-')\n        ax.set_aspect('equal')\n        ax.invert_yaxis()  # Match array indexing\n\n        # Draw cell highlights for forbidden/goal/start\n        for r in range(R):\n            for c in range(C):\n                s = (r, c)\n                if self.is_forbidden(s):\n                    rect = plt.Rectangle((c-0.5, r-0.5), 1, 1, color='gray', alpha=0.5)\n                    ax.add_patch(rect)\n                    ax.text(c, r, \"X\", ha='center', va='center', color='black', fontsize=14)\n                elif s == self.goal:\n                    rect = plt.Rectangle((c-0.5, r-0.5), 1, 1, color='yellow', alpha=0.45)\n                    ax.add_patch(rect)\n                    ax.text(c, r, \"G\", ha='center', va='center', color='black', fontsize=14)\n\n        # Print arrows and values in every cell (except forbidden)\n        for r in range(R):\n            for c in range(C):\n                s = (r, c)\n                # Print policy arrow\n                if (r, c) in P:\n                    arrow = ACTION_ARROWS[P[(r, c)]]\n                else:\n                    arrow = \"\"\n                if (V is not None) and ((r, c) in V):\n                    valstr = f\"{V[(r, c)]:.2f}\"\n                else:\n                    valstr = \"\"\n\n                # Place arrow in cell\n                ax.text(\n                    c, r-0.18, arrow,\n                    ha='center', va='center', color='darkblue', fontsize=22, fontweight='bold', zorder=10\n                )\n\n                ax.text(\n                    c, r+0.22, valstr,\n                    ha='center', va='center', color='crimson', fontsize=11, alpha=0.99,\n                    zorder=10\n                )\n\n        # Label grid\n        for x in range(C):\n            ax.text(x, -0.75, str(x), ha='center', va='center', color='black')\n        for y in range(R):\n            ax.text(-0.75, y, str(y), ha='center', va='center', color='black')\n\n        ax.set_title(\"Policy and Values\")\n        ax.tick_params(bottom=False, left=False, right=False, top=False,\n                      labelbottom=False, labelleft=False, labeltop=False, labelright=False)\n        if show:\n            plt.tight_layout()\n            plt.show()\n\n        return fig, ax\n\n\n\n","type":"content","url":"/lab10-rl#task1-reinforce-on-a-grid-environment","position":5},{"hierarchy":{"lvl1":"Lab 10: RL: Policy Gradient Methods","lvl3":"REINFORCE (Monte-Carlo Policy Gradient)","lvl2":"Task1: REINFORCE on a Grid Environment"},"type":"lvl3","url":"/lab10-rl#reinforce-monte-carlo-policy-gradient","position":6},{"hierarchy":{"lvl1":"Lab 10: RL: Policy Gradient Methods","lvl3":"REINFORCE (Monte-Carlo Policy Gradient)","lvl2":"Task1: REINFORCE on a Grid Environment"},"content":"The REINFORCE algorithm directly learns a stochastic policy \\pi(a \\mid s). It updates the policy so that actions leading to high returns become more likely, and actions leading to low returns become less likely.\n\nParameterizing the Policy\n\nWe use a neural network with parameters \\theta to represent the policy. The network outputs logits — raw, unnormalized scores for each action. These logits are then converted to a probability distribution using softmax:\\pi_\\theta(a \\mid s) = \\frac{\\exp(f_\\theta(s, a))}{\\sum_{a'} \\exp(f_\\theta(s, a'))}\n\nwhere f_\\theta(s, a) is the network’s output (logit) for action a in state s.\n\nWhy logits instead of direct action probabilities or actions? Logits provide numerical stability, allow flexible real-valued outputs that softmax normalizes into valid probabilities, enable efficient gradient computation through log_prob(), and naturally support exploration by sampling from a distribution rather than greedily selecting a single action.\n\nIn our GridWorld:\n\neach state is a grid cell (r, c), represented using one-hot encoding\n\navailable actions: up, right, down, left, stay\n\nthe policy samples actions using these probabilities\n\nThe algorithm is like the following:\n\nGenerating an Episode\n\nStarting from a start state s_0, the agent repeatedly:samples an action a_t \\sim \\pi_\\theta(a \\mid s_t); transitions to the next state s_{t+1} via the environment; receives reward r_{t+1}. This creates a trajectory:\n\\{s_0, a_0, r_1, s_1, a_1, r_2, \\dots, s_{T-1}, a_{T-1}, r_T\\}. The episode ends when the agent reaches the goal or when a step limit is reached.\n\nAction value update using Monte-Carlo Return\n\nFor each timestep t, the action value is the cumulative discounted reward from that point onward:q_t(s_t,a_t)=G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots + \\gamma^{T-t-1} r_{T}= \\sum_{k=t+1}^{T} \\gamma^{k-t-1} r_{k}\n\nIn our GridWorld, we can set the following (you can change this to other values)\n\nreward +10 for reaching the goal\n\nreward -10 for entering forbidden cells\n\nreward -1 for hitting walls\n\nreward -0.01 for each step\n\nPolicy Gradient Update\n\nREINFORCE increases the probability of actions that produced high returns. The policy gradient theorem gives us:\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\ln \\pi(a_t \\mid s_t, \\theta)\\, q_t(s_t, a_t)\n\nwhere \\alpha is the learning rate. Note that the gradient \\nabla_\\theta \\ln \\pi(a_t \\mid s_t, \\theta) can be directly computed is a neural network is used for the policy. In this case, the log-probability \\ln \\pi(a_t \\mid s_t, \\theta)\nis differentiable with respect to the network parameters \\theta, so automatic differentiation (backpropagation) provides the gradient without requiring any manual derivation.\n\n\n\nWe use a neural network to represent the policy. The policy takes the current state as input and produces the probabilities of each action as the output. Intuitively, we might use the raw state coordinates, e.g., (0,1) or (0,3), as the input to the policy network. However, we use one-hot encoding on the state instead — so (0,1) becomes [0,1,0,0,0,...,0] and (0,3) becomes [0,0,0,1,0,...,0] — since raw coordinates make learning difficult for REINFORCE. The following section explains why.\n\nWhat is One-Hot Encoding?\n\nOne-hot encoding represents a categorical value as a binary vector where only one element is 1 and all others are 0.\n\nFor our 5×5 GridWorld, each cell/state becomes a vector of length 25:\n\nState\n\nIndex\n\nOne-hot vector\n\n(0,0)\n\n0\n\n[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n\n(0,1)\n\n1\n\n[0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n\n(2,3)\n\n13\n\n[0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0]\n\nWhy One-Hot Encoding Helps\n\nWith raw coordinates [2,3] vs [3,2], the network sees these as similar inputs — both contain a 2 and a 3. The network has to figure out that swapping row and column completely changes which cell we’re in. This is hard to learn.\n\nWith one-hot encoding, every cell has its own unique “slot” in the input vector. Cell (2,3) lights up position 13, while cell (3,2) lights up position 17. There’s no confusion — the network sees completely different inputs for different cells.\n\nThis is especially useful — and often necessary — for basic algorithms like REINFORCE. Because REINFORCE has high variance and no value function to stabilize learning, it struggles when the input representation is ambiguous. One-hot encoding removes this ambiguity, allowing the simple policy network to learn effectively. For this 5×5 GridWorld environment, REINFORCE struggles to converge without one-hot encoding.\n\nTradeoff\n\nOne-hot encoding works well for small discrete spaces like our 5×5 grid. For larger spaces (e.g., 100×100 grid or continuous states), it becomes impractical — that’s when you need raw coordinates with more sophisticated learning algorithms.\n\nimport torch # PyTorch library for building and training neural networks\nimport torch.nn as nn # Neural network module from PyTorch\nimport torch.optim as optim # Optimization algorithms from PyTorch\n\n# Create a function to convert state to one-hot:\ndef state_to_onehot(state, grid_shape=(5, 5)):\n    \"\"\"Convert (row, col) to one-hot vector\"\"\"\n    r, c = state\n    index = r * grid_shape[1] + c  # Flatten to single index\n    onehot = torch.zeros(grid_shape[0] * grid_shape[1])\n    onehot[index] = 1.0\n    return onehot.unsqueeze(0)\n\nLR = 0.0003 # Learning rate\nGAMMA = 0.99 # Discount factor\nEPISODES = 20000 # Number of training episodes\nMAX_STEPS = 50 # Max steps per episode\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n#----------------------------------------------\n# Initialize environment, policy network, and optimizer\n#-----------------------\nenv = GridWorld()\n\nstate_dim = env.grid_shape[0] * env.grid_shape[1]  # state dimensions: 25 for 5x5 grid\naction_dim = env.action_space.n # action dimensions: 5 actions\n\n# Define policy network using nn.Sequential\n# Input: 25-dimensional one-hot vector\n# Output: 5 logits (one per action)\n# Architecture: Linear(25 → 128) → ReLU → Linear(128 → 5)\npolicy_net = nn.Sequential(\n    nn.Linear(state_dim, 128),\n    nn.ReLU(),\n    nn.Linear(128, action_dim)\n).to(device)\n\n# Define optimizer which is used to update the policy network parameters\noptimizer = optim.Adam(policy_net.parameters(), lr=LR)\nreturn_list = []\n\n#----------------------------------------------\n# Training loop\n#----------------------------------------------\nprint(\"Training started...\")\nfor episode in range(EPISODES):\n    log_probs = []\n    rewards = []\n\n    state, info = env.reset()\n    episode_return = 0\n\n    # Generate an episode\n    for t in range(MAX_STEPS):\n        state_tensor = state_to_onehot(state, env.grid_shape).to(device)\n\n        # Get action logits from policy network\n        # logits = [f_θ(s,0), f_θ(s,1), ..., f_θ(s,4)] are raw scores for each action\n        logits = policy_net(state_tensor)\n\n        # Categorical(logits=logits) applies softmax internally:\n        # π_θ(a|s) = exp(f_θ(s,a)) / Σ_a' exp(f_θ(s,a'))\n        # This converts logits into a valid probability distribution over actions\n        action_dist = torch.distributions.Categorical(logits=logits)\n\n        # Sample an action from the distribution π_θ(a|s)\n        action = action_dist.sample()\n\n        # Compute log π_θ(a_t|s_t) for the sampled action (needed for policy gradient)\n        log_prob = action_dist.log_prob(action)\n\n        # Take the action in the environment\n        next_state, reward, terminated, truncated, _ = env.step(action.item())\n\n        # Store log probability and reward\n        log_probs.append(log_prob)\n        rewards.append(reward)\n\n        # Move to the next state\n        state = next_state\n        # Accumulate episode return\n        episode_return += reward\n\n        if terminated or truncated:\n            break\n\n    # Compute action values (returns) G_t for each time step t\n    returns = []\n    G = 0\n    for r in reversed(rewards):\n        G = r + GAMMA * G\n        returns.insert(0, G)\n    # Convert returns to tensor\n    returns = torch.tensor(returns, dtype=torch.float32).to(device)\n\n    # Compute loss -Σ log π(a_t|s_t) * G_t\n    loss = 0\n    for log_prob, G_t in zip(log_probs, returns):\n        loss += -log_prob * G_t # loss is a tensor in PyTorch since log_prob is a tensor\n\n    # Update policy (i.e., update the parameters of the policy network)\n    optimizer.zero_grad() # Clear previous gradients\n    loss.backward() # Backpropagate to compute gradients (since loss is a tensor, it has a .backward() method)\n    optimizer.step() # Update policy network parameters\n\n    return_list.append(episode_return)\n\n    if (episode+1) % 1000 == 0:\n        print(f\"Episode {episode+1}/{EPISODES}, Return: {episode_return:.2f}\")\n\n\n\n\n\n\n\n# Visualization\npolicy_dict = {}\nwith torch.no_grad():\n    for r in range(env.grid_shape[0]):\n        for c in range(env.grid_shape[1]):\n            s = (r, c)\n            if s == env.goal:\n                continue\n\n            s_tensor = state_to_onehot(s, env.grid_shape).to(device)\n\n            logits = policy_net(s_tensor)\n            best_action = torch.argmax(logits).item()\n            policy_dict[s] = best_action\n\nenv.render(policy_dict)\n\n\n\n\n\n","type":"content","url":"/lab10-rl#reinforce-monte-carlo-policy-gradient","position":7},{"hierarchy":{"lvl1":"Lab 10: RL: Policy Gradient Methods","lvl3":"RL using PPO (Proximal Policy Optimization)","lvl2":"Task1: REINFORCE on a Grid Environment"},"type":"lvl3","url":"/lab10-rl#rl-using-ppo-proximal-policy-optimization","position":8},{"hierarchy":{"lvl1":"Lab 10: RL: Policy Gradient Methods","lvl3":"RL using PPO (Proximal Policy Optimization)","lvl2":"Task1: REINFORCE on a Grid Environment"},"content":"The REINFORCE is simple and intuitive, which can be used to illustrate the basic concepts. We use it to illustrate how to use Pytorch to create neural networks. But REINFORCE generally suffers from high variance — the returns can vary significantly between episodes, making learning unstable and slow. Nowadays people normally used more advanced agorithms such as PPO (Proximal Policy Optimization) that addresses this issue. It uses a value function to reduce variance and includes a clipping mechanism to prevent overly large policy updates, resulting in more stable and efficient learning.\n\nInstead of implementing PPO from scratch, we will use \n\nStable Baselines3 (SB3), a popular library that provides reliable implementations of many reinforcement learning algorithms, including PPO, A2C, DQN, and more. With SB3, we can train an agent in just a few lines of code.\n\nColab comes with many commonly used libraries preinstalled—such as NumPy, PyTorch, pandas, and FFmpeg—but Stable Baselines3 is not included by default. So we need to install it manually using the command !pip install stable_baselines3.\n\ntry:\n    import stable_baselines3\nexcept ImportError:\n    !pip install stable_baselines3\n    import stable_baselines3\n\n# With the SB3 library and a Gymnasium-compatible environment, training can be done in just a few lines of code.\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\n\n# 1. create the environment\nenv = GridWorld()\n\n\n# 2. Initialize PPO model\npolicy_kwargs = dict(\n    net_arch=[16]  # Single hidden layer with 16 neurons\n)\nmodel = PPO(\"MlpPolicy\",  # Multi-layer perceptron policy\n            env,  # the environment\n            verbose=1,  # print out info during training\n            gamma=0.99,  # discount factor, if not provided, default is 0.99\n            learning_rate=1e-2,  # learning rate, if not provided, default is 3e-4\n            policy_kwargs=policy_kwargs # policy network architecture: if this is not provided, default is [64, 64]\n            )\n\n\n# 3. Train model\nmodel.learn(total_timesteps=50_000)\nprint(\"training finished\")\nmodel.save(\"ppo_gridworld\")\n\n\n\n\n\n\n\n## Now let us render the learned policy\n\n# --- 1. Prepare to extract the policy ---\npolicy_dict = {}\nR, C = env.grid_shape\n\n# --- 2. Iterate over all states and find the best action ---\nfor r in range(R):\n    for c in range(C):\n        s = (r, c)\n\n        # Skip the goal state, as no action is taken there\n        if s == env.goal:\n            continue\n\n        # The input to the model's predict() must be a NumPy array.\n        state_np = np.array([r, c], dtype=np.int32)\n\n        # model.predict returns the action (0-4) and the hidden state (which we ignore)\n        best_action, _ = model.predict(state_np, deterministic=True)\n\n        # Store the greedy action for the state\n        policy_dict[s] = int(best_action)\n\n# --- 3. Render the policy ---\nenv.render(policy_dict)\n\n\n\n\n\nThe rendered policy shows that PPO successfully learns an optimal policy for each state, efficiently reaching the goal while avoiding forbidden cells. The training results are also consistent across runs.\n\n","type":"content","url":"/lab10-rl#rl-using-ppo-proximal-policy-optimization","position":9},{"hierarchy":{"lvl1":"Lab 10: RL: Policy Gradient Methods","lvl2":"Task 2: RL for the Torque-Limited Pendulum"},"type":"lvl2","url":"/lab10-rl#task-2-rl-for-the-torque-limited-pendulum","position":10},{"hierarchy":{"lvl1":"Lab 10: RL: Policy Gradient Methods","lvl2":"Task 2: RL for the Torque-Limited Pendulum"},"content":"Lets continue the reinforcement learning on a more complicated environment provided by Gymnasium, the \n\nPendulum.\n\nThe pendulum swingup problem is similar to what we have investigated before in previous labs (such as trajectory optimization). The system consists of a pendulum attached at one end to a fixed point, and the other end being free. The pendulum starts in a random position and the goal is to apply torque on the pendulum to swing it up into an upright position, with its center of gravity right above the fixed point.\n\nIf we use SB3 and the PPO algorithm, the training code for the Pendulum environment is very similar to what we used for GridWorld. This is the beauty of the Gymnasium API — once an environment follows the standard interface (reset, step, observation_space, action_space, render), the same training code works on various environments with minimal changes. Whether the environment is a simple grid, a physics simulation, or a complex game, SB3 handles it seamlessly. This modularity makes it easy to experiment with different environments without rewriting the training loop.\n\nimport imageio # for creating videos\nimport gymnasium as gym # Gymnasium library for reinforcement learning environments\nfrom stable_baselines3 import PPO # PPO algorithm\nfrom stable_baselines3.common.env_util import make_vec_env # for creating vectorized environments\nimport numpy as np\nfrom IPython.display import Video # for displaying videos in Jupyter notebooks\n\n# 1. Create training env\n# Pendulum-v1 comes from Gymnasium's built-in environments\nenv = make_vec_env(\"Pendulum-v1\", n_envs=4)\n\n# 2. Train PPO model\nmodel = PPO(\n    \"MlpPolicy\",\n    env,\n    verbose=1,\n    learning_rate=3e-4,\n    gamma=0.99,\n)\n\nmodel.learn(total_timesteps=100_000) # the number of training timesteps can be adjusted\n\n# 3. Save the trained model\nmodel.save(\"ppo_pendulum_v1\")\nprint(\"Training finished and model saved.\")\n\n\n\n\n\n\n\nNow that training is complete, we can evaluate the learned policy.\nThe most intuitive way to do this is to run a full episode with the trained agent, record its behavior, and watch the video. This gives us a direct sense of how well the agent behaves.\n\nIn Gymnasium-compatible environments, video recording is supported through the render() method, which allows us to capture each frame as the agent interacts with the environment. By saving these frames, we can create a short animation of the agent’s trajectory and assess the quality of the learned policy visually.\n\n# Create rendering env\nrender_env = gym.make(\n    \"Pendulum-v1\",\n    render_mode=\"rgb_array\", # to enable image rendering\n    max_episode_steps=3000\n)\n\n# Load the trained model in the previous cell\nmodel = PPO.load(\"ppo_pendulum_v1\")\n\n# Record video using the imageio library\nwriter = imageio.get_writer(\"pendulum_video.mp4\", fps=30)\n\n# Reset the environment\nobs, info = render_env.reset()\n\n# Run the trained model and record frames\nfor _ in range(300): # generate 300 frames\n    action, _ = model.predict(obs, deterministic=True) # get action from the trained model\n    obs, reward, terminated, truncated, info = render_env.step(action) # take action in env\n\n    frame = render_env.render()\n    writer.append_data(frame)\n\n    if terminated or truncated:\n        break\n\nwriter.close()\nrender_env.close()\nVideo(\"pendulum_video.mp4\", embed=True, width=600)\n\n\n\n\n\nSB3 allows you to customize many internal parameters of its algorithms. For example, you can modify the structure of the policy neural networks, adjust activation functions, or change other policy-related settings. These options are passed through the policy_kwargs argument.policy_kwargs = dict(\n    net_arch=[256, 256]   # two hidden layers of size 256 each\n)\n\nmodel = PPO(\n    \"MlpPolicy\",\n    env,\n    verbose=1,\n    learning_rate=3e-4,\n    gamma=0.99,\n    policy_kwargs=policy_kwargs\n)\n\nIt is also useful to experiment with key hyperparameters—such as the learning rate and the entropy coefficient—when creating the PPO model. Adjusting these values can significantly influence exploration and training stability.\n\nGuidelines for choosing hyperparameters:\n\nLearning rate (learning_rate): Controls how quickly the policy is updated. Typical values range from 1e-5 to 1e-2. Start with 3e-4 (the default) and decrease if training is unstable or increase if learning is too slow.\n\nDiscount factor (gamma): Determines how much future rewards are valued. Use values close to 1.0 (e.g., 0.95–0.99) for tasks requiring long-term planning, and lower values (e.g., 0.9) for tasks with immediate rewards.\n\nEntropy coefficient (ent_coef): Encourages exploration by adding randomness to the policy. Use small positive values (e.g., 0.01–0.1) to promote exploration early in training, or 0.0 if the agent already explores sufficiently.\n\nNetwork architecture (net_arch): Larger networks (e.g., [256, 256]) can model complex policies but may overfit or train slowly. Smaller networks (e.g., [64, 64]) are faster and work well for simpler tasks. Adjust based on task complexity.\n\nTuning these parameters often requires experimentation. Start with defaults, observe training behavior, and adjust one parameter at a time based on performance and stability.model = PPO(\n    \"MlpPolicy\",\n    env,\n    verbose=1,\n    learning_rate=3e-4,\n    gamma=0.99,\n    ent_coef = 0.01\n)\n\n","type":"content","url":"/lab10-rl#task-2-rl-for-the-torque-limited-pendulum","position":11},{"hierarchy":{"lvl1":"Lab 10: RL: Policy Gradient Methods","lvl2":"Task 3: Learning to stabilize the Cart Pole system"},"type":"lvl2","url":"/lab10-rl#task-3-learning-to-stabilize-the-cart-pole-system","position":12},{"hierarchy":{"lvl1":"Lab 10: RL: Policy Gradient Methods","lvl2":"Task 3: Learning to stabilize the Cart Pole system"},"content":"We have used model-based approach (e.g., LQR) to stabilize the cart pole system in previous labs. Now let’s try to use RL to stabilize it. For the cart-pole system, a pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pole is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart. The detailed description for the system can be found at \n\nhttps://​gymnasium​.farama​.org​/environments​/classic​_control​/cart​_pole/. The code implementation for the Cart-Pole Dynamics can be found here: \n\nCart-Pole System\n\nThe default Gym CartPole environment (e.g., CartPole-v1) is a stabilization task: the pole starts nearly upright, and the objective is to keep it balanced while keeping the cart near the center. Since the goal is to keep the pole upright for as long as possible, by default, a reward of +1 is given for every step taken, including the termination step.  An episode terminates when the pole falls beyond a small angle or the cart moves too far from the origin. We will learn to swing-up the pole in the next task.\n\nFor this environment provided by Gymnasium, the default action discrete. In fact, is a ndarray with shape (1,) which can take values {0, 1} indicating the direction of the fixed force applied horizontly on the cart.\n\n0: Push cart to the left\n\n1: Push cart to the right\n\nimport imageio\nimport gymnasium as gym\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\nimport numpy as np\nfrom IPython.display import Video\n\n\n# 1. Create training env\nenv = make_vec_env(\"CartPole-v1\", n_envs=1) #n_envs=1 means no vectorization\n\n# 2. Train PPO model\nmodel = PPO(\n    \"MlpPolicy\",\n    env,\n    verbose=1,\n    learning_rate=3e-4,\n    gamma=0.99,\n)\n\nmodel.learn(total_timesteps=300_000)\n\n# Save model\nmodel.save(\"ppo_CartPole_v1\")\nprint(\"Training finished and model saved.\")\n\n\n\n# 3. Create rendering env\nrender_env = gym.make(\n    \"CartPole-v1\",\n    render_mode=\"rgb_array\",\n    max_episode_steps=2000\n)\n\n# Load model\nmodel = PPO.load(\"ppo_CartPole_v1\")\n\n# 4. Record video\nwriter = imageio.get_writer(\"pendulum_video.mp4\", fps=30)\n\nobs, info = render_env.reset()\n\nfor _ in range(300):\n    action, _ = model.predict(obs, deterministic=True)\n    obs, reward, terminated, truncated, info = render_env.step(action)\n\n    frame = render_env.render()\n    writer.append_data(frame)\n\n    if terminated or truncated:\n        break\n\nwriter.close()\nrender_env.close()\nVideo(\"pendulum_video.mp4\", embed=True, width=600)\n\n\n\n\n\n","type":"content","url":"/lab10-rl#task-3-learning-to-stabilize-the-cart-pole-system","position":13},{"hierarchy":{"lvl1":"Lab 10: RL: Policy Gradient Methods","lvl3":"Using Wrappers to covnert discrete actions to continous actions","lvl2":"Task 3: Learning to stabilize the Cart Pole system"},"type":"lvl3","url":"/lab10-rl#using-wrappers-to-covnert-discrete-actions-to-continous-actions","position":14},{"hierarchy":{"lvl1":"Lab 10: RL: Policy Gradient Methods","lvl3":"Using Wrappers to covnert discrete actions to continous actions","lvl2":"Task 3: Learning to stabilize the Cart Pole system"},"content":"Now we can try to revise the default setting to make it similar to model-based approach. To do this, we will do two things through the wrapper of gym:\n\nConvert the problem to be continuous\n\nUse a new reward function similar to the cost function we have used in trajectory optimization\n\nWhat is a Gym \n\nWrapper?\n\nA wrapper is a design pattern that allows us to modify the behavior of an existing environment without creating an entirely new one from scratch. Wrappers “wrap around” the original environment and can modify:\n\nAction spaces (discrete → continuous)\n\nObservation spaces\n\nReward functions\n\nEpisode termination conditions\n\nAny other environment behavior\n\nWhy Use Wrappers Instead of Creating New Environments?\n\nReusability: We can apply the same modifications to different base environments\n\nModularity: Each wrapper handles one specific modification, making code more organized\n\nMaintainability: Changes to the base environment automatically propagate to wrapped versions\n\nEfficiency: We leverage the existing, well-tested Gymnasium implementation rather than reimplementing dynamics\n\nIn our case, we’ll first use :\n\nContinuousCartPoleWrapper: Converts discrete actions {0, 1} to continuous torque values\n\nimport numpy as np\nimport gymnasium as gym\nfrom gymnasium import spaces\n\n\nclass ContinuousCartPoleWrapper(gym.Wrapper):\n    \"\"\"\n    Wrapper that turns CartPole-v1 into a continuous-control,\n    LQR-style stabilization task.\n\n    - Action: u in [-u_max, u_max]  (continuous force)\n    - State:  [x, x_dot, theta, theta_dot]\n    - Reward: r = -(x^T Q x + u^T R u) * dt, with optional terminal penalty.\n    \"\"\"\n\n    def __init__(\n        self,\n        env: gym.Env,\n        u_max: float = 10.0,\n        Q: np.ndarray | None = None,\n        R: float | None = None,\n        scale_with_dt: bool = True,\n        terminal_cost: float = 100.0,\n    ):\n        super().__init__(env)\n\n        # Continuous 1D action (force)\n        self.u_max = float(u_max)\n        self.action_space = spaces.Box(\n            low=np.array([-self.u_max], dtype=np.float32),\n            high=np.array([self.u_max], dtype=np.float32),\n            dtype=np.float32,\n        )\n\n        # Keep the original observation space (4-dim Box)\n        self.observation_space = env.observation_space\n\n        # Default LQR weights if not provided\n        # Emphasize pole angle θ and cart position x\n        if Q is None:\n            # state = [x, x_dot, theta, theta_dot]\n            Q = np.diag([1.0, 0.1, 10.0, 0.1])\n        if R is None:\n            R = 0.01  # scalar for u^2\n\n        self.Q = np.array(Q, dtype=np.float64)\n        self.R = float(R)\n\n        self.scale_with_dt = scale_with_dt\n        self.terminal_cost = float(terminal_cost)\n\n    # Just delegate reset to inner env and return same observation\n    def reset(self, **kwargs):\n        obs, info = self.env.reset(**kwargs)\n        return obs, info\n\n    def step(self, action):\n        # ---- 1) Continuous control input u ----\n        if isinstance(action, np.ndarray):\n            u = float(action.squeeze())\n        else:\n            u = float(action)\n        u = np.clip(u, -self.u_max, self.u_max)\n\n        # Use the *unwrapped* CartPole to access physics params and state\n        env = self.env.unwrapped\n\n        # ---- 2) Current state ----\n        x, x_dot, theta, theta_dot = env.state\n\n        # ---- 3) Dynamics from Gymnasium's CartPole (but with continuous u) ----\n        gravity = env.gravity\n        masscart = env.masscart\n        masspole = env.masspole\n        total_mass = masscart + masspole\n        length = env.length          # actually half the pole length\n        polemass_length = masspole * length\n        tau = env.tau                # time step\n\n        costheta = np.cos(theta)\n        sintheta = np.sin(theta)\n\n        # This is exactly CartPole's internal derivation, but with \"force = u\"\n        temp = (u + polemass_length * theta_dot**2 * sintheta) / total_mass\n        theta_acc = (gravity * sintheta - costheta * temp) / (\n            length * (4.0 / 3.0 - masspole * costheta**2 / total_mass)\n        )\n        x_acc = temp - polemass_length * theta_acc * costheta / total_mass\n\n        # Euler integration\n        x = x + tau * x_dot\n        x_dot = x_dot + tau * x_acc\n        theta = theta + tau * theta_dot\n        theta_dot = theta_dot + tau * theta_acc\n\n        env.state = (x, x_dot, theta, theta_dot)\n\n        # ---- 4) Termination condition (same as CartPole) ----\n        terminated = (\n            x < -env.x_threshold\n            or x > env.x_threshold\n            or theta < -env.theta_threshold_radians\n            or theta > env.theta_threshold_radians\n        )\n        # We don't enforce a time limit here; TimeLimit wrapper can be applied outside if you like\n        truncated = False\n\n        \"\"\"# ---- 5) LQR-like reward ----\n        state_vec = np.array([x, x_dot, theta, theta_dot], dtype=np.float64)\n        state_cost = state_vec @ self.Q @ state_vec\n        control_cost = self.R * (u ** 2)\n\n        cost = state_cost + control_cost\n        if self.scale_with_dt:\n            cost *= tau\n        reward = -float(cost)\n        \"\"\"\n        # ---- LQR-like reward with boundary shaping ----\n        state_vec = np.array([x, x_dot, theta, theta_dot], dtype=np.float64)\n        state_cost = state_vec @ self.Q @ state_vec\n        control_cost = self.R * (u ** 2)\n\n        # Soft barrier on cart position near limits\n        x_norm = abs(x) / env.x_threshold  # 0 at center, 1 at boundary\n        boundary_cost = 5.0 * (x_norm ** 4)  # smooth, grows steeply near boundary\n\n        # Extra angle robustness term (helps with large initial angles)\n        angle_cost = 0.5 * (1.0 - np.cos(theta)) * 20.0  # ~10 when upside down\n\n        cost = state_cost + control_cost + boundary_cost + angle_cost\n        if self.scale_with_dt:\n            cost *= tau\n\n        reward = -float(cost) # total reward is negative cost\n\n\n        # Add terminal cost when it fails\n        if terminated:\n            reward -= self.terminal_cost\n\n        # Add terminal cost when it fails\n        if terminated:\n            reward -= self.terminal_cost\n\n        obs = state_vec.astype(np.float32)\n        info = {\n            \"u\": u,\n            \"state_vec\": obs,\n            \"state_cost\": state_cost,\n            \"control_cost\": control_cost,\n            \"instant_cost\": cost,\n        }\n\n        return obs, reward, terminated, truncated, info\n\n    # For rendering, just delegate\n    def render(self):\n        return self.env.render()\n\n\n\nWith the wrapper, can can implement a continous version for the balance task.\n\nimport gymnasium as gym\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\n\n# Create the environment with continuous action wrapper\ndef make_env():\n    base_env = gym.make(\"CartPole-v1\")  # standard env\n    env = ContinuousCartPoleWrapper(base_env, u_max=20.0)\n    return env\n\nenv = make_vec_env(make_env, n_envs=1)\n\nmodel = PPO(\n    \"MlpPolicy\",\n    env,\n    verbose=1,\n    learning_rate=3e-4,\n    gamma=0.99\n)\n\nmodel.learn(total_timesteps=150_000)\nmodel.save(\"ppo_continuous_cartpole\")\n\n\n\nimport imageio\nfrom IPython.display import Video\n\nrender_env = gym.make(\n    \"CartPole-v1\",\n    render_mode=\"rgb_array\",  # important for frame capture\n)\nrender_env = ContinuousCartPoleWrapper(render_env, u_max=20.0)\n\nmodel = PPO.load(\"ppo_continuous_cartpole\")\n\nwriter = imageio.get_writer(\"CartPole_LQR.mp4\", fps=30)\n\nobs, info = render_env.reset()\nfor _ in range(300):\n    action, _ = model.predict(obs, deterministic=True)\n    obs, reward, terminated, truncated, info = render_env.step(action)\n\n    frame = render_env.render()\n    writer.append_data(frame)\n\n    if terminated or truncated:\n        break\n\nwriter.close()\nrender_env.close()\n\nVideo(\"CartPole_LQR.mp4\", embed=True, width=600)\n\n\n\n\n\nFrom the video, you may notice that the motion of the cart pole does not vary too much. This is because in the default Gym env, the intial condition starts very close to the upright configuration. In fact:high = np.array([0.05, 0.05, 0.05, 0.05])\nself.state = self.np_random.uniform(low=-high, high=high, size=(4,))\n\nThis means:\n\nState variable\n\nMeaning\n\nInitial range\n\nx\n\ncart position\n\nuniform in [-0.05, +0.05]\n\nx_dot\n\ncart velocity\n\nuniform in [-0.05, +0.05]\n\ntheta\n\npole angle\n\nuniform in [-0.05, +0.05] rad\n\ntheta_dot\n\npole angular velocity\n\nuniform in [-0.05, +0.05]\n\nAnd 0.05 rad ≈ 2.8° from vertical. This means the PPO agent is learning local stabilization very near the equilibrium.\n\nLet’s try to use another wrapper WideInitWrapper to train a policy that can handle large initial ranges.\n\nclass WideInitWrapper(gym.Wrapper):\n    \"\"\"\n    Reset with wider ranges so PPO sees large initial angles.\n    Ranges are tunable.\n    \"\"\"\n    def __init__(\n        self,\n        env,\n        x_range=1, # range for cart position\n        xdot_range=1, # range for cart velocity\n        theta_range=np.pi/10, # range for pole angle (up to ±18°)\n        thetadot_range=2.0, # range for pole angular velocity\n    ):\n        super().__init__(env)\n        self.x_range = x_range\n        self.xdot_range = xdot_range\n        self.theta_range = theta_range\n        self.thetadot_range = thetadot_range\n\n    def reset(self, **kwargs):\n        obs, info = self.env.reset(**kwargs)\n        rng = self.env.unwrapped.np_random\n\n        x = rng.uniform(-self.x_range, self.x_range)\n        x_dot = rng.uniform(-self.xdot_range, self.xdot_range)\n        theta = rng.uniform(-self.theta_range, self.theta_range)\n        theta_dot = rng.uniform(-self.thetadot_range, self.thetadot_range)\n\n        self.env.unwrapped.state = (x, x_dot, theta, theta_dot)\n        obs = np.array([x, x_dot, theta, theta_dot], dtype=np.float32)\n        return obs, info\n\n\n\nimport gymnasium as gym\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\n\n# Create the environment with continuous action wrapper\ndef make_env():\n    base = gym.make(\"CartPole-v1\")\n    base = ContinuousCartPoleWrapper(base, u_max=20.0)  # or ContinuousLQRCartPoleWrapper\n    base = WideInitWrapper(base, theta_range=np.pi/10, thetadot_range=1.0)\n    return base\n\nenv = make_vec_env(make_env, n_envs=4)\n\nmodel = PPO(\n    \"MlpPolicy\",\n    env,\n    verbose=1,\n    learning_rate=3e-4,\n    gamma=0.99\n)\n\nmodel.learn(total_timesteps=500_000)\nmodel.save(\"ppo_continuous_cartpole\")\n\n\n\n# Now render the trained model with wide init\nimport imageio\nfrom IPython.display import Video\n\nrender_env = gym.make(\n    \"CartPole-v1\",\n    render_mode=\"rgb_array\",  # important for frame capture\n)\nrender_env = ContinuousCartPoleWrapper(render_env, u_max=20.0)\nrender_env = WideInitWrapper(render_env, theta_range=np.pi/10, thetadot_range=1.0)  # match training init\n\nmodel = PPO.load(\"ppo_continuous_cartpole\")\n\nwriter = imageio.get_writer(\"CartPole.mp4\", fps=30)\n\nobs, info = render_env.reset()\nfor _ in range(300):\n    action, _ = model.predict(obs, deterministic=True)\n    obs, reward, terminated, truncated, info = render_env.step(action)\n\n    frame = render_env.render()\n    writer.append_data(frame)\n\n    if terminated or truncated:\n        break\n\nwriter.close()\nrender_env.close()\n\nVideo(\"CartPole_LQR.mp4\", embed=True, width=600)\n\n\n\n\n\n","type":"content","url":"/lab10-rl#using-wrappers-to-covnert-discrete-actions-to-continous-actions","position":15},{"hierarchy":{"lvl1":"Lab 10: RL: Policy Gradient Methods","lvl2":"Task 4: Learning Swing-up for the Cart Pole System"},"type":"lvl2","url":"/lab10-rl#task-4-learning-swing-up-for-the-cart-pole-system","position":16},{"hierarchy":{"lvl1":"Lab 10: RL: Policy Gradient Methods","lvl2":"Task 4: Learning Swing-up for the Cart Pole System"},"content":"In this task, we train a RL agent to swing up a CartPole from the downward position to upright, then stabilize it. Unlike Task 3 (which focuses on stabilization from a near-upright starting position), this task addresses the more challenging swing-up problem.\n\nGoal: Learn a policy that swings the pole from hanging down (θ ≈ π) to upright (θ ≈ 0) and keeps it balanced.\n\nApproach:\n\nDesign a custom reward function that encourages upright orientation (cos(θ)) while penalizing cart displacement, velocities, and control effort\n\nUse ContinuousSwingUpCartPoleWrapper to apply continuous forces and define success/failure conditions\n\nUse SwingUpInitWrapper to initialize episodes with the pole near the downward position\n\nTrain PPO with appropriate hyperparameters and multiple parallel environments for stable learning\n\nVisualize the learned behavior through video rendering\n\nThis demonstrates how RL can solve complex non-linear control problems without explicit trajectory optimization, learning a smooth swing-up motion purely from experience.\n\nimport numpy as np\nimport gymnasium as gym\nfrom gymnasium import spaces\n\n\nclass ContinuousSwingUpCartPoleWrapper(gym.Wrapper):\n    \"\"\"\n    Continuous-action CartPole with swing-up style reward.\n\n    - Action: u in [-u_max, u_max]\n    - State:  [x, x_dot, theta, theta_dot]\n    - Reward: encourage upright (cos(theta)) and penalize big x, velocities, and control.\n    - Done:\n        * success when near-upright for several consecutive steps\n        * failure if cart goes out of bounds\n        * episode time limit is handled by an outer TimeLimit wrapper.\n    \"\"\"\n\n    def __init__(\n        self,\n        env: gym.Env,\n        u_max: float = 10.0,  # max force\n        x_fail_threshold: float = 2.4,  # cart position failure threshold\n        upright_theta_thresh: float = np.deg2rad(12.0),  # setpoint for \"upright\"\n        upright_theta_dot_thresh: float = 1.0,  # velocity threshold for \"upright\"\n        upright_x_thresh: float = 0.5,  # position threshold for \"upright\"\n        success_steps: int = 2,  # number of consecutive upright steps for success\n        success_bonus: float = 50.0,  # reward for achieving upright\n        failure_penalty: float = 50.0,  # penalty for failure\n    ):\n        super().__init__(env)\n\n        # Continuous 1D action space\n        self.u_max = float(u_max)\n        self.action_space = spaces.Box(\n            low=np.array([-self.u_max], dtype=np.float32),\n            high=np.array([self.u_max], dtype=np.float32),\n            dtype=np.float32,\n        )\n\n        # Same observation space as CartPole\n        self.observation_space = env.observation_space\n\n        # Swing-up termination thresholds\n        self.x_fail_threshold = float(x_fail_threshold)\n        self.upright_theta_thresh = float(upright_theta_thresh)\n        self.upright_theta_dot_thresh = float(upright_theta_dot_thresh)\n        self.upright_x_thresh = float(upright_x_thresh)\n\n        # Success condition (must hold upright for N consecutive steps)\n        self.success_steps = int(success_steps)\n        self.steps_upright = 0  # counter\n\n        self.success_bonus = float(success_bonus)\n        self.failure_penalty = float(failure_penalty)\n\n    def reset(self, **kwargs):\n        # Let another wrapper handle initial conditions if present\n        obs, info = self.env.reset(**kwargs)\n        self.steps_upright = 0\n        return obs, info\n\n    def step(self, action):\n        # ---- 1) Continuous control input u ----\n        if isinstance(action, np.ndarray):\n            u = float(action.squeeze())\n        else:\n            u = float(action)\n        u = np.clip(u, -self.u_max, self.u_max)\n\n        env = self.env.unwrapped\n\n        # ---- 2) Current state ----\n        x, x_dot, theta, theta_dot = env.state\n\n        # ---- 3) Dynamics (same as CartPole, but with force = u) ----\n        gravity = env.gravity\n        masscart = env.masscart\n        masspole = env.masspole\n        total_mass = masscart + masspole\n        length = env.length          # actually half pole length\n        polemass_length = masspole * length\n        tau = env.tau                # time step\n\n        costheta = np.cos(theta)\n        sintheta = np.sin(theta)\n\n        temp = (u + polemass_length * theta_dot**2 * sintheta) / total_mass\n        theta_acc = (gravity * sintheta - costheta * temp) / (\n            length * (4.0 / 3.0 - masspole * costheta**2 / total_mass)\n        )\n        x_acc = temp - polemass_length * theta_acc * costheta / total_mass\n\n        # Euler integration\n        x = x + tau * x_dot\n        x_dot = x_dot + tau * x_acc\n        theta = theta + tau * theta_dot\n        theta_dot = theta_dot + tau * theta_acc\n\n        env.state = (x, x_dot, theta, theta_dot)\n\n        # ---- 4) Termination: success/failure ----\n        # \"Upright\" region: near upright and reasonably calm\n        upright = (\n            abs(theta) < self.upright_theta_thresh\n            and abs(theta_dot) < self.upright_theta_dot_thresh\n            and abs(x) < self.upright_x_thresh\n        )\n\n        if upright:\n            self.steps_upright += 1\n        else:\n            self.steps_upright = 0\n\n        # Success if we've stayed upright long enough\n        success = self.steps_upright >= self.success_steps\n\n        # Failure: cart leaves the track\n        failure = abs(x) > self.x_fail_threshold\n\n        terminated = success or failure\n        truncated = False  # TimeLimit wrapper outside can handle max steps\n\n        # ---- 5) Swing-up reward ----\n        # Main term: cos(theta) is highest at upright (1) and lowest at downward (-1)\n        r_angle = np.cos(theta)\n\n        # Penalize cart displacement\n        r_x = -0.01 * (x**2)\n\n        # Stronger penalty on angular velocity\n        w_xdot = 0.01\n        w_thetadot = 0.03\n        r_vel = -(w_xdot * x_dot**2 + w_thetadot * theta_dot**2)\n\n        # Penalize control effort\n        r_u = -0.001 * (u**2)\n\n        # Penalize approaching boundaries\n        r_boundary = -5.0 * (abs(x) / self.env.unwrapped.x_threshold) ** 4\n\n        reward = float(r_angle + r_x + r_vel + r_u + r_boundary)\n\n        # Terminal bonuses/penalties\n        if success:\n            reward += self.success_bonus\n        if failure:\n            reward -= self.failure_penalty\n\n        obs = np.array([x, x_dot, theta, theta_dot], dtype=np.float32)\n        info = {\n            \"u\": u,\n            \"upright\": upright,\n            \"success\": success,\n            \"failure\": failure,\n            \"steps_upright\": self.steps_upright,\n        }\n\n        return obs, reward, terminated, truncated, info\n\n    def render(self):\n        return self.env.render()\n\n\n\nclass SwingUpInitWrapper(gym.Wrapper):\n    \"\"\"\n    Override reset() so the pole starts near the downward position (theta ~ pi).\n    \"\"\"\n\n    # define the angle noise in degrees, default is 10 degrees\n    def __init__(self, env, angle_noise_deg: float = 10.0):\n        super().__init__(env)\n        self.angle_noise = np.deg2rad(angle_noise_deg)\n\n    def reset(self, **kwargs):\n        # Call base reset to set up RNG etc.\n        obs, info = self.env.reset(**kwargs)\n\n        rng = self.env.unwrapped.np_random\n\n        x = 0.0\n        x_dot = 0.0\n        # Set theta near pi (downward) with some noise\n        theta = np.pi + rng.uniform(-self.angle_noise, self.angle_noise)\n        theta_dot = 0.0\n\n        self.env.unwrapped.state = (x, x_dot, theta, theta_dot)\n\n        obs = np.array([x, x_dot, theta, theta_dot], dtype=np.float32)\n        return obs, info\n\n\n\n\nimport gymnasium as gym\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.monitor import Monitor\nimport matplotlib.pyplot as plt\n\n\n# 1) Build vectorized environment\ndef make_env():\n    base_env = gym.make(\"CartPole-v1\")\n    base_env = Monitor(base_env, log_dir)   # <-- log episodic stats\n    env = ContinuousSwingUpCartPoleWrapper(base_env, u_max=20.0)\n    env = SwingUpInitWrapper(env, angle_noise_deg=15.0)\n    return env\n\nenv = make_vec_env(make_env, n_envs=4)  # multiple envs helps PPO\n\n# 2) Train PPO\nmodel = PPO(\n    \"MlpPolicy\",\n    env,\n    verbose=1,\n    learning_rate=3e-4,\n    gamma=0.99,\n    n_steps=1024,\n    batch_size=64,\n)\n\nmodel.learn(total_timesteps=1_000_000)\nmodel.save(\"ppo_cartpole_swingup\")\nenv.close()\n\n\n\n\n\nimport imageio\nfrom stable_baselines3 import PPO\nfrom IPython.display import Video\n\n# 1) Rebuild render env with same wrappers\nrender_env = gym.make(\n    \"CartPole-v1\",\n    render_mode=\"rgb_array\",\n    max_episode_steps=5000,   # allow enough time for swing-up\n)\n\nrender_env = ContinuousSwingUpCartPoleWrapper(render_env, u_max=20.0)\nrender_env = SwingUpInitWrapper(render_env, angle_noise_deg=15.0)\n\n# 2) Load trained model\nmodel = PPO.load(\"ppo_cartpole_swingup\")\n\n# 3) Record video\nwriter = imageio.get_writer(\"CartPole_swingup.mp4\", fps=30)\n\nobs, info = render_env.reset()\n\nfor _ in range(5000):\n    action, _ = model.predict(obs, deterministic=True)\n    obs, reward, terminated, truncated, info = render_env.step(action)\n\n    frame = render_env.render()\n    writer.append_data(frame)\n\n    if terminated or truncated:\n        break\n\nwriter.close()\nrender_env.close()\n\nVideo(\"CartPole_swingup.mp4\", embed=True, width=600)\n\n\n\n\n\n\n","type":"content","url":"/lab10-rl#task-4-learning-swing-up-for-the-cart-pole-system","position":17},{"hierarchy":{"lvl1":"Lab 10: RL: Policy Gradient Methods","lvl2":"Task 5: Hybrid Policy to Swing Up and then Stay"},"type":"lvl2","url":"/lab10-rl#task-5-hybrid-policy-to-swing-up-and-then-stay","position":18},{"hierarchy":{"lvl1":"Lab 10: RL: Policy Gradient Methods","lvl2":"Task 5: Hybrid Policy to Swing Up and then Stay"},"content":"In this task, we combine two trained PPO policies: a swing-up policy and a stabilizer policy. We run the environment with ContinuousSwingUpCartPoleWrapper (continuous force, swing-up reward/termination) and initialize near the downward position using SwingUpInitWrapper. During the episode, we start in “swingup” mode and switch to “stabilize” when the state meets the upright criteria. After switching, actions come from stabilizer_model; before switching, actions come from swingup_model. We record the full run to a video (combined_cartpole.mp4) to visualize the handoff and final stabilization.\n\nimport numpy as np\nimport gymnasium as gym\nimport imageio\nfrom stable_baselines3 import PPO\nfrom IPython.display import Video\n\n\n# ---------- 1. Upright condition used for switching ----------\n\n# Tunable thresholds for the *handoff* to stabilizer\nTHETA_THRESH = np.deg2rad(3.0)   # ~3 degrees\nTHETA_DOT_THRESH = 1            # rad/s\nX_THRESH = 0.7                    # meters\n\n\ndef is_upright(obs) -> bool:\n    \"\"\"\n    Check if the current state is 'upright enough' to hand off\n    from the swing-up policy to the stabilizer policy.\n\n    obs = [x, x_dot, theta, theta_dot]\n    \"\"\"\n    x, x_dot, theta, theta_dot = obs\n    return (\n        abs(theta) < THETA_THRESH\n        and abs(theta_dot) < THETA_DOT_THRESH\n        and abs(x) < X_THRESH\n    )\n\n\n# ---------- 2. Load trained models ----------\nprint(\"Loading models...\")\nswingup_model = PPO.load(\"ppo_cartpole_swingup\")\nstabilizer_model = PPO.load(\"ppo_continuous_cartpole\")\nprint(\"Models loaded successfully\")\n\n\n# ---------- 3. Create evaluation environment ----------\nrender_env = gym.make(\n    \"CartPole-v1\",\n    render_mode=\"rgb_array\",\n    max_episode_steps=1000,\n)\n\n# Wrap with swing-up wrapper for physics\nrender_env = ContinuousSwingUpCartPoleWrapper(\n    render_env,\n    u_max=20.0,\n    x_fail_threshold=2.4,\n    success_steps=10**9,   # disable automatic success termination\n)\n\n# Start from downward position\nrender_env = SwingUpInitWrapper(render_env, angle_noise_deg=5.0)\n\n\n# ---------- 4. Run combined episode and record video ----------\nwriter = imageio.get_writer(\"combined_cartpole.mp4\", fps=30)\n\nobs, info = render_env.reset()\nmode = \"swingup\"\nprint(f\"Starting episode in mode: {mode}\")\nprint(f\"Initial state: x={obs[0]:.3f}, theta={obs[2]:.3f} rad ({np.rad2deg(obs[2]):.1f} deg)\")\n\nfor t in range(1000):\n    # Check if we should switch to stabilizer\n    if mode == \"swingup\" and is_upright(obs):\n        mode = \"stabilize\"\n        print(f\"[step {t}] Switching to STABILIZER policy\")\n        print(f\"  State: x={obs[0]:.3f}, theta={obs[2]:.3f} rad ({np.rad2deg(obs[2]):.1f} deg)\")\n\n    # Choose action based on current mode\n    if mode == \"swingup\":\n        action, _ = swingup_model.predict(obs, deterministic=True)\n    else:\n        action, _ = stabilizer_model.predict(obs, deterministic=True)\n\n    obs, reward, terminated, truncated, info = render_env.step(action)\n\n    # Record frame\n    frame = render_env.render()\n    writer.append_data(frame)\n\n    if terminated or truncated:\n        print(f\"Episode ended at step {t}\")\n        print(f\"  terminated={terminated}, truncated={truncated}\")\n        print(f\"  Final state: x={obs[0]:.3f}, theta={obs[2]:.3f} rad\")\n        break\n\nwriter.close()\nrender_env.close()\n\nprint(\"Video saved to combined_cartpole.mp4\")\nVideo(\"combined_cartpole.mp4\", embed=True, width=600)\n\n\n\n\n\n","type":"content","url":"/lab10-rl#task-5-hybrid-policy-to-swing-up-and-then-stay","position":19},{"hierarchy":{"lvl1":"Lab2: Ordinary Differential Equations"},"type":"lvl1","url":"/lab2-odes","position":0},{"hierarchy":{"lvl1":"Lab2: Ordinary Differential Equations"},"content":"","type":"content","url":"/lab2-odes","position":1},{"hierarchy":{"lvl1":"Lab2: Ordinary Differential Equations","lvl3":"Introduction"},"type":"lvl3","url":"/lab2-odes#introduction","position":2},{"hierarchy":{"lvl1":"Lab2: Ordinary Differential Equations","lvl3":"Introduction"},"content":"Dynamic systems are mostly described by ordinary differential equations (ODEs). To predict their behaviors, we must solve ODEs. However, most ODEs do not have analytical solutions, so numerical methods are required. Typical numerical methods for ODEs include:\n\nEuler’s Method: A simple, first-order technique for approximating solutions (\n\nWikipedia).\n\nRunge-Kutta Methods: More accurate, higher-order methods widely used in practice (\n\nWikipedia).\n\nMultistep Methods: Such as Adams-Bashforth and Adams-Moulton, which use information from previous steps (\n\nSciPy Docs).\n\nPython provides robust, modern tools for solving ODEs through \n\nSciPy, an open-source scientific computing library. SciPy’s ODE solvers implement these and other advanced algorithms, making it a reliable and easy-to-use choice for engineering, physics, and data science applications.\n\nIn this lab, we’ll solve ODE initial value problems using:\n\nSciPy’s built-in ODE solvers\n\nA custom ODE function for the pendulum system\n\nAnimation tools to visualize pendulum motion","type":"content","url":"/lab2-odes#introduction","position":3},{"hierarchy":{"lvl1":"Lab2: Ordinary Differential Equations","lvl3":"Learning Objectives"},"type":"lvl3","url":"/lab2-odes#learning-objectives","position":4},{"hierarchy":{"lvl1":"Lab2: Ordinary Differential Equations","lvl3":"Learning Objectives"},"content":"Introduction to SciPy’s ODE solving capabilities\n\nLearn to solve both simple and complex ODE systems\n\nCreate interactive visualizations of dynamic systems\n\nLet’s first import some necessary libararies\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import solve_ivp # For solving differential equations\nfrom matplotlib.animation import FuncAnimation # For creating animations\nfrom IPython.display import HTML    # For displaying animations in Jupyter\nimport matplotlib\nmatplotlib.rcParams['animation.embed_limit'] = 50  # Increase animation size limit\n\n# For consistent plots in Jupyter\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (8, 6) # Set default figure size\nplt.rcParams['axes.grid'] = True # Enable grid by default\n\n\n\n","type":"content","url":"/lab2-odes#learning-objectives","position":5},{"hierarchy":{"lvl1":"Lab2: Ordinary Differential Equations","lvl3":"Task 1 – Simple First-Order ODE Example"},"type":"lvl3","url":"/lab2-odes#task-1-simple-first-order-ode-example","position":6},{"hierarchy":{"lvl1":"Lab2: Ordinary Differential Equations","lvl3":"Task 1 – Simple First-Order ODE Example"},"content":"Let’s start with a simple first-order ODE to demonstrate SciPy’s \n\nsolve_ivp:\\frac{dx}{dt} = -2x + \\sin(t)\n\nwith initial condition x(0) = 1. This represents a damped oscillator with sinusoidal forcing.\n\nFor this ODE, we can write analytical solution as:x(t) = \\frac{1}{5}(2\\sin(t) - \\cos(t) + 6e^{-2t})\n\nWe will compare the numerical solution with the analytical solution. We will also use two different methods RK45 and RK23 to numerically solve the ODEs. RK45 is higher order and usually preferred for most ODEs due to better accuracy, while RK23 can be faster for less demanding problems. Both methods automatically adjust their step size to control error.\n\ndef simple_ode(t, x):\n    \"\"\"Simple first-order ODE: dx/dt = -2*x + sin(t)\"\"\"\n    return -2*x + np.sin(t)\n\ndef analytical_solution(t):\n    \"\"\"Analytical solution for comparison\"\"\"\n    return (2*np.sin(t) - np.cos(t) + 6*np.exp(-2*t)) / 5\n\n# Solve the ODE\nt_span = (0, 10) # time span\nx0 = [1.0]  # initial condition\nt_eval = np.linspace(0, 10, 100) # time points where solution is evaluated\n\n# Using different methods\nsol_rk45 = solve_ivp(simple_ode, t_span, x0, method='RK45', t_eval=t_eval, rtol=1e-8) #rtol: relative tolerance: default is 1e-3\nsol_rk23 = solve_ivp(simple_ode, t_span, x0, method='RK23', t_eval=t_eval, rtol=1e-8)\n\n# Analytical solution for comparison\nx_analytical = analytical_solution(t_eval)\n\n# Plot results\nplt.figure(figsize=(10, 6))\nplt.plot(t_eval, x_analytical, 'k-', linewidth=2, label='Analytical')\nplt.plot(sol_rk45.t, sol_rk45.y[0], 'r--', label='RK45')\nplt.plot(sol_rk23.t, sol_rk23.y[0], 'b:', label='RK23')\nplt.xlabel('Time (s)')\nplt.ylabel('x(t)')\nplt.title('Simple First-Order ODE: dx/dt = -2x + sin(t)')\nplt.legend()\nplt.grid(True)\n\n# plot the error with respect to the analytical solution\nplt.figure(figsize=(10, 6))\nplt.plot(t_eval, sol_rk45.y[0]-x_analytical, 'r-', linewidth=2, label='error (RK45)')\nplt.plot(t_eval, sol_rk23.y[0]-x_analytical, 'b--', label='error (RK23)')\nplt.xlabel('Time (s)')\nplt.ylabel('Error')\nplt.title('Error Comparison')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Check accuracy\nerror_rk45 = np.abs(sol_rk45.y[0] - analytical_solution(sol_rk45.t))\nprint(f\"Maximum error (RK45): {np.max(error_rk45):.2e}\")\nerror_rk23 = np.abs(sol_rk23.y[0] - analytical_solution(sol_rk23.t))\nprint(f\"Maximum error (RK23): {np.max(error_rk23):.2e}\")\n\n\n\n\n\n\n\n","type":"content","url":"/lab2-odes#task-1-simple-first-order-ode-example","position":7},{"hierarchy":{"lvl1":"Lab2: Ordinary Differential Equations","lvl3":"Task 2 – Pendulum System Using SciPy ODE Solvers"},"type":"lvl3","url":"/lab2-odes#task-2-pendulum-system-using-scipy-ode-solvers","position":8},{"hierarchy":{"lvl1":"Lab2: Ordinary Differential Equations","lvl3":"Task 2 – Pendulum System Using SciPy ODE Solvers"},"content":"Now let’s solve the nonlinear pendulum equations of motion with friction. The equations become:\n$$\\begin{align}\n\\dot{x}_1 &= x_2 \\\\\n\\dot{x}_2 &= -\\frac{g}{L}\\sin(x_1) - \\frac{b}{mL^2}\\,x_2\n\\end{align}\n\n$$\n\nwhere:\n\nx_1 = \\theta (angle)\n\nx_2 = \\dot{\\theta} (angular velocity)\n\ng = 9.81\\,\\text{m/s}^2\n\nm = 1 \\,\\text{kg}\n\nL = 1\\,\\text{m}\n\nb = 0.2 (N.m.s/rad, damping coefficient)\n\nWe’ll compare different initial conditions and solver methods.\n\ndef pendulum_rhs(t, x, g=9.81, L=1.0, m=1, b=0.2):\n    \"\"\"Right-hand side of the pendulum ODE.\n    state: x = [theta, omega]\n    Returns [theta_dot, omega_dot].\n    \"\"\"\n    theta, omega = x\n    theta_dot = omega\n    omega_dot = -(g/L)*np.sin(theta) - b*omega/(m*L**2)\n    return np.array([theta_dot, omega_dot])\n\n# Different initial conditions\ninitial_conditions = [\n    [0.1, 0.0],   # Small angle\n    [1.0, 0.0],   # Medium angle\n    [2.5, 0.0],   # Large angle\n    [np.pi-0.1, 0.0]  # Near inverted\n]\n\nt_span = (0, 20) # time span\nt_eval = np.linspace(0, 20, 1000) # time points where solution is evaluated\n\nplt.figure(figsize=(12, 8))\n\nfor i, x0 in enumerate(initial_conditions):\n    # Solve using RK45\n    sol = solve_ivp(pendulum_rhs, t_span, x0, method='RK45', t_eval=t_eval, rtol=1e-8)\n\n    plt.subplot(2, 2, i+1)\n    plt.plot(sol.t, sol.y[0], 'b-', label='θ(t)') # y[0] is the angle theta\n    plt.plot(sol.t, sol.y[1], 'r-', label='ω(t)') # y[1] is the anglular velocity\n    plt.xlabel('Time (s)')\n    plt.ylabel('Angle (rad) / Angular velocity (rad/s)')\n    plt.title(f'Initial: θ₀={x0[0]:.2f}, ω₀={x0[1]:.2f}')\n    plt.legend()\n    plt.grid(True)\n\nplt.tight_layout()\n# Add a super-title to the figure for all subplots\n#y=1.02 means the title is positioned at 102% of the figure height\nplt.suptitle('Pendulum Motion for Different Initial Conditions', y=1.02)\nplt.show()\n\n\n# Phase portrait\nplt.figure(figsize=(10, 6))\nfor i, x0 in enumerate(initial_conditions):\n    sol = solve_ivp(pendulum_rhs, t_span, x0, method='RK45', t_eval=t_eval, rtol=1e-8)\n    plt.plot(sol.y[0], sol.y[1], label=f'θ₀={x0[0]:.2f}')\n\nplt.xlabel('θ (rad)')\nplt.ylabel('ω (rad/s)')\nplt.title('Pendulum Phase Portrait')\nplt.legend()\nplt.grid(True)\n\n\n\n\n\n","type":"content","url":"/lab2-odes#task-2-pendulum-system-using-scipy-ode-solvers","position":9},{"hierarchy":{"lvl1":"Lab2: Ordinary Differential Equations","lvl3":"Task 3 – Animated Pendulum Visualization"},"type":"lvl3","url":"/lab2-odes#task-3-animated-pendulum-visualization","position":10},{"hierarchy":{"lvl1":"Lab2: Ordinary Differential Equations","lvl3":"Task 3 – Animated Pendulum Visualization"},"content":"In this task, we will create an interactive animation of the pendulum motion. You can modify the initial conditions to see how the pendulum behaves differently. To do this, we will use FuncAnimation, a powerful tool from Matplotlib for creating animations in Python.\n\nFuncAnimation repeatedly calls a user-defined function to update the plot for each frame, making it ideal for visualizing dynamic systems like the pendulum. For more details, see the \n\nMatplotlib FuncAnimation documentation.\n\nTo display the animation directly in a Jupyter notebook, we use IPython.display.HTML. After creating the animation object (e.g., anim), you can convert it to HTML using anim.to_jshtml() and then display it with display(HTML(anim.to_jshtml())). This approach embeds the animation in the notebook, allowing for interactive visualization without saving to an external file.\n\nFor more details, see the \n\nMatplotlib animation documentation.\n\ndef solve_pendulum_ode(theta0=1.0, omega0=0.0, L=1.0, g=9.81, m=1.0, b=0.2, duration=10.0):\n    \"\"\"\n    Solve the pendulum ODE for given initial conditions.\n\n    Parameters:\n    theta0: Initial angle (radians)\n    omega0: Initial angular velocity (rad/s)\n    L: Pendulum length (m)\n    g: Gravitational acceleration (m/s²)\n    m: Pendulum mass (kg)\n    b: Damping coefficient\n    duration: Simulation duration (s)\n\n    Returns:\n    sol: Solution object from scipy.integrate.solve_ivp\n    \"\"\"\n    # Set up initial conditions and time span\n    t_span = (0, duration)\n    x0 = [theta0, omega0]\n    t_eval = np.linspace(0, duration, int(duration*30))  # 30 fps for smooth animation\n\n    # Solve the ODE system\n    # Lambda function used to pass extra parameters (g, L, m, b) to pendulum_rhs.\n    # This allows solve_ivp to call pendulum_rhs(t, x, g, L, m, b) while only varying t and x,\n    # keeping the physical parameters fixed for the integration.\n    sol = solve_ivp(lambda t, x: pendulum_rhs(t, x, g, L, m, b), t_span, x0,\n                   method='RK45', t_eval=t_eval, rtol=1e-8)\n\n    return sol\n\ndef animate_pendulum_solution(sol, theta0, omega0, L=1.0, duration=10.0):\n    \"\"\"\n    Create animation from a solved pendulum ODE solution.\n\n    Parameters:\n    sol: Solution object from solve_pendulum_ode\n    theta0: Initial angle (for display purposes)\n    omega0: Initial angular velocity (for display purposes)\n    L: Pendulum length (m)\n    duration: Animation duration (s)\n\n    Returns:\n    HTML animation object\n    \"\"\"\n\n    # Set up the figure and axis for animation: ax1 for pendulum, ax2 for time series\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Pendulum animation subplot\n    ax1.set_xlim(-1.5*L, 1.5*L) # Set x-limits\n    ax1.set_ylim(-1.5*L, 0.5*L) # Set y-limits\n    ax1.set_aspect('equal') # Ensure equal scaling\n    ax1.set_title(f'Pendulum Animation (θ₀={theta0:.2f}, ω₀={omega0:.2f})') # Set title\n    ax1.grid(True) # Enable grid\n\n    # Create pendulum elements\n    line, = ax1.plot([], [], 'b-', linewidth=3) # Line for rod, blue color\n    bob, = ax1.plot([], [], 'ro', markersize=10) # Bob at the end, red color\n    trace, = ax1.plot([], [], 'r--', alpha=0.3, linewidth=1) # Trace of the bob\n\n    # Time series subplot\n    ax2.set_xlim(0, duration) # Set x-limits\n    ax2.set_ylim(-max(abs(theta0), 3), max(abs(theta0), 3)) # Set y-limits\n    ax2.set_xlabel('Time (s)') # x-label\n    ax2.set_ylabel('Angle (rad)') # y-label\n    ax2.set_title('Angle vs Time')  # Set title\n    ax2.grid(True)  # Enable grid\n\n    theta_line, = ax2.plot([], [], 'b-', label='θ(t)') # Line for angle over time\n    time_marker, = ax2.plot([], [], 'ks', markersize=6) # Current time marker\n    ax2.legend()\n\n    # Store trace points\n    x_trace, y_trace = [], []\n\n    # Animation function: generate each frame at each time step\n    def animate(frame):\n        if frame < len(sol.t):\n            # Current angle\n            theta = sol.y[0][frame]\n            t_current = sol.t[frame]\n\n            # Pendulum position\n            x_bob = L * np.sin(theta)\n            y_bob = -L * np.cos(theta)\n\n            # Update pendulum\n            line.set_data([0, x_bob], [0, y_bob])\n            bob.set_data([x_bob], [y_bob])\n\n            # Update trace (last 50 points for better performance)\n            x_trace.append(x_bob)\n            y_trace.append(y_bob)\n            if len(x_trace) > 50:\n                x_trace.pop(0)\n                y_trace.pop(0)\n            trace.set_data(x_trace, y_trace)\n\n            # Update time series\n            theta_line.set_data(sol.t[:frame+1], sol.y[0][:frame+1])\n            time_marker.set_data([t_current], [theta])\n\n        return line, bob, trace, theta_line, time_marker\n\n    # Create and display animation\n    anim = FuncAnimation(fig, animate, frames=len(sol.t), interval=100, blit=True, repeat=True)\n    plt.tight_layout()\n    plt.show()\n\n    # Return HTML version for display\n    return HTML(anim.to_jshtml())\n\n# Example usage - modify these initial conditions!\ntheta_initial = 2.0  # Initial angle in radians (try values like 0.5, 1.5, 3.0, etc.)\nomega_initial = 0.0  # Initial angular velocity (try 2.0, -1.0, etc.)\nduration = 8.0       # Simulation duration in seconds\n\nprint(f\"Solving pendulum ODE with θ₀ = {theta_initial:.2f} rad, ω₀ = {omega_initial:.2f} rad/s\")\n\n# Step 1: Solve the ODE\nsolution = solve_pendulum_ode(theta_initial, omega_initial, duration=duration)\n\nprint(f\"ODE solved successfully! Solution contains {len(solution.t)} time points.\")\nprint(f\"Maximum angle reached: {np.max(np.abs(solution.y[0])):.2f} rad ({np.max(np.abs(solution.y[0]))*180/np.pi:.1f}°)\")\n\n# Step 2: Create and display animation\nprint(\"Creating animation...\")\nanimation_html = animate_pendulum_solution(solution, theta_initial, omega_initial, duration=duration)\ndisplay(animation_html)\n\n\n\n\n\n\n\nQuestions to explore based on the simulation:\n\nHow does the period change with initial amplitude?\n\nWhat happens when θ₀ approaches π (inverted pendulum)?\n\nHow does initial angular velocity affect the motion?\n\nCan you find initial conditions that make the pendulum rotate continuously?\n\n","type":"content","url":"/lab2-odes#task-3-animated-pendulum-visualization","position":11},{"hierarchy":{"lvl1":"Lab2: Ordinary Differential Equations","lvl2":"HW problems"},"type":"lvl2","url":"/lab2-odes#hw-problems","position":12},{"hierarchy":{"lvl1":"Lab2: Ordinary Differential Equations","lvl2":"HW problems"},"content":"","type":"content","url":"/lab2-odes#hw-problems","position":13},{"hierarchy":{"lvl1":"Lab2: Ordinary Differential Equations","lvl3":"Problem 1: Coupled Mass-Spring System","lvl2":"HW problems"},"type":"lvl3","url":"/lab2-odes#problem-1-coupled-mass-spring-system","position":14},{"hierarchy":{"lvl1":"Lab2: Ordinary Differential Equations","lvl3":"Problem 1: Coupled Mass-Spring System","lvl2":"HW problems"},"content":"For the coupled mass-spring system shown below:\n\n\n\nThe dynamics of the system are:\\begin{aligned}\n    m \\ddot{q}_1 &= -2 k q_1 - c \\dot{q}_1 + k q_2, \\\\\n    m \\ddot{q}_2 &= k q_1 - 2 k q_2 - c \\dot{q}_2 + ku\n\\end{aligned}\n\nAnswer the following questions (questions 1–5 require manual derivation; you may attach a separate PDF):\n\nDefine the state vector x = [q_1, \\dot{q}_1, q_2, \\dot{q}_2]^T, with output y = q_1. Write the dynamics in state-space form: \\dot{x} = f(x, u), y = g(x, u).\n\nSolve the equilibrium point x_e for \\dot{x} = f(x, u) when u = 0.\n\nSolve the operating point x_d, u_d, y_d for a given y_d.\n\nIs this system linear or nonlinear? Is it time-invariant? Justify.\n\nIf the system is linear, rewrite in the standard form:\\dot{x} = Ax + Bu \\\\ y = Cx + Du\n\nProgramming problem: numerically solve the ODEs for the system with zero control input and the following initial conditions:\n\n[0.1,\\, 0,\\, -0.1,\\, 0]^T\n\n[0.2,\\, 0,\\, 0,\\, 0]^T\n\n[0,\\, 0.1,\\, 0,\\, -0.1]^T\n\n[-0.1,\\, 0,\\, 0.1,\\, 0]^T\n\nPlot all four state variables versus time for each initial condition. Explain your simulation results.\n\n","type":"content","url":"/lab2-odes#problem-1-coupled-mass-spring-system","position":15},{"hierarchy":{"lvl1":"Lab2: Ordinary Differential Equations","lvl3":"Problem 2: Servo Mechanism","lvl2":"HW problems"},"type":"lvl3","url":"/lab2-odes#problem-2-servo-mechanism","position":16},{"hierarchy":{"lvl1":"Lab2: Ordinary Differential Equations","lvl3":"Problem 2: Servo Mechanism","lvl2":"HW problems"},"content":"Consider a simple mechanism consisting of a spring loaded arm that is driven by a motor, as shown below. Such a mechanism can be considered as a simplified model for disk drives, robotic actuators, camera gimbals, or any application where precise angular positioning is required and the restoring force is provided by a spring.\n\n\n\nThe motor applies a torque that twists the arm against a linear spring and moves the end of the arm across a rotating platter. The input to the system is the motor torque \\tau_\\text{m}. The force exerted by the spring is a nonlinear function of the head position due to the way it is attached.\n\nThe equations of motion for the system are given byJ \\ddot \\theta = -b \\dot\\theta - k r\\sin\\theta + \\tau_\\text{m},\n\nwhere\n\nJ is the rotational inertia of the motor (kg·m²)\n\nb is the viscous damping coefficient (N·m·s/rad)\n\nk is the linear spring constant (N/m)\n\nr is the location of spring contact on arm (m)\n\nAnswer the following questions (questions 1–4 require manual derivation; you may attach a separate PDF):\n\nDefine the state vector x = [\\theta,\\, \\dot{\\theta}]^T and output y = \\theta. Write the dynamics in state-space form: \\dot{x} = f(x, u), y = g(x, u) with u=\\tau_\\text{m}.\n\nSolve the equilibrium point x_e for \\dot{x} = f(x, u) when u = 0.\n\nSolve the operating point x_d,\\, u_d,\\, y_d for a given y_d.\n\nIs this system linear or nonlinear? Is it time-invariant? Justify.\n\nProgramming problem: Assume the values for the parameters:k = 1,\\quad J = 100,\\quad b = 10,\n \\quad r = 1\n\nNumerically solve the ODEs for the system with zero motor torque and the following initial conditions:\n\n[0.1,\\, 0]^T\n\n[0.5,\\, 0]^T\n\n[0,\\, 0.2]^T\n\n[-0.2,\\, 0]^T\n\nPlot both \\theta and \\dot{\\theta} versus time for each initial condition. Explain your simulation results.","type":"content","url":"/lab2-odes#problem-2-servo-mechanism","position":17},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library"},"type":"lvl1","url":"/lab3-intro-to-pythoncontrol","position":0},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library"},"content":"This lab introduces the basics of Python Control Systems Library, or python-control, a powerful open-source package for modeling, analyzing, and simulating control systems. The library supports both linear and nonlinear systems, providing tools for state-space and transfer function representations, system linearization, time and frequency response analysis, and visualization (such as phase portraits and step responses). The official documentation for this can be found at: \n\nhttps://​python​-control​.readthedocs​.io​/en​/0​.10​.2/\n\nWhat you’ll practice in this lab\n\nBuild and simulate state-space models.\n\nAnalyze time responses using python-control tools.\n\nExplore nonlinear system modeling.\n\nPerform system linearization and compare linear vs. nonlinear responses.\n\nVisualize and interpret step, impulse, ramp, and sinusoidal responses.\n\n","type":"content","url":"/lab3-intro-to-pythoncontrol","position":1},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl3":"Setup (run once per environment)"},"type":"lvl3","url":"/lab3-intro-to-pythoncontrol#setup-run-once-per-environment","position":2},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl3":"Setup (run once per environment)"},"content":"We first need to install python-control (and import required libraries) every time you open this notebook, to ensure all dependencies are available in your current environment.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Import the python-control package\ntry:\n    import control as ctl # ctl is the alias for control\n    print(\"python-control\", ctl.__version__)\nexcept ImportError:\n    %pip install control\n    import control as ctl\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (7, 4)\nplt.rcParams['axes.grid'] = True\n\n\n\n","type":"content","url":"/lab3-intro-to-pythoncontrol#setup-run-once-per-environment","position":3},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl3":"Task 1: Coupled mass spring system"},"type":"lvl3","url":"/lab3-intro-to-pythoncontrol#task-1-coupled-mass-spring-system","position":4},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl3":"Task 1: Coupled mass spring system"},"content":"In this task, we will model, simulate, and analyze the dynamics of the following coupled mass-spring system using the Python Control Systems Library. We will construct the state-space representation, explore the system’s time-domain responses (such as initial and step responses), and investigate how the system behaves under different initial conditions and external inputs. This will provide insight into the fundamental properties of multi-degree-of-freedom mechanical systems and demonstrate key analysis tools for linear systems.\n\n","type":"content","url":"/lab3-intro-to-pythoncontrol#task-1-coupled-mass-spring-system","position":5},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl4":"System dynamics","lvl3":"Task 1: Coupled mass spring system"},"type":"lvl4","url":"/lab3-intro-to-pythoncontrol#system-dynamics","position":6},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl4":"System dynamics","lvl3":"Task 1: Coupled mass spring system"},"content":"The dynamics of the system can be written as\\begin{aligned}\n  m \\ddot{q}_1 &= -2 k q_1 - c \\dot{q}_1 + k q_2, \\\\\n  m \\ddot{q}_2 &= k q_1 - 2 k q_2 - c \\dot{q}_2 + ku\n\\end{aligned}\n\nor in state space form, with state vector x = [x_1,\\, x_2,\\, x_3,\\, x_4]^T where\n\nx_1 = q_1 (position of mass 1)\n\nx_2 = q_2 (position of mass 2)\n\nx_3 = \\dot{q}_1 (velocity of mass 1)\n\nx_4 = \\dot{q}_2 (velocity of mass 2)\n\nand output of y= [q_1, q_2]^T\\begin{aligned}\n    \\dfrac{dx}{dt} &= \\begin{bmatrix}\n        0 & 0 & 1 & 0 \\\\\n        0 & 0 & 0 & 1 \\\\[0.5ex]\n        -\\dfrac{2k}{m} & \\dfrac{k}{m} & -\\dfrac{c}{m} & 0 \\\\[0.5ex]\n        \\dfrac{k}{m} & -\\dfrac{2k}{m} & 0 & -\\dfrac{c}{m}\n    \\end{bmatrix} x\n    + \\begin{bmatrix}\n        0 \\\\ 0 \\\\[0.5ex] 0 \\\\[1ex] \\dfrac{k}{m}\n    \\end{bmatrix} u, \\\\[2ex]\n    y &= \\begin{bmatrix}\n        1 & 0 & 0 & 0 \\\\\n        0 & 1 & 0 & 0\n    \\end{bmatrix} x\n\\end{aligned}\n\n# Define the parameters for the system\nm, c, k = 1, 0.1, 2\n# Create a linear system\nA = np.array([\n    [0, 0, 1, 0],\n    [0, 0, 0, 1],\n    [-2*k/m, k/m, -c/m, 0],\n    [k/m, -2*k/m, 0, -c/m]\n])\nB = np.array([[0], [0], [0], [k/m]])\nC = np.array([[1, 0, 0, 0], [0, 1, 0, 0]])\nD = 0\n\n# Create a state-space system using A, B, C, D\n# https://python-control.readthedocs.io/en/0.10.2/generated/control.ss.html\nsys = ctl.ss(A, B, C, D, outputs=['q1', 'q2'], name=\"coupled spring mass\")\nprint(sys)\n\n\n\n","type":"content","url":"/lab3-intro-to-pythoncontrol#system-dynamics","position":7},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl4":"Initial response","lvl3":"Task 1: Coupled mass spring system"},"type":"lvl4","url":"/lab3-intro-to-pythoncontrol#initial-response","position":8},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl4":"Initial response","lvl3":"Task 1: Coupled mass spring system"},"content":"The initial_response function can be used to compute the response of the system with no input, but starting from a given initial condition.  This function returns a response object, which can be used for plotting.\n\nNote: “No input” here means the input u(t) = 0 for all t. So, the initial response is the system’s behavior due solely to its initial state.\n\n# Initial condition response\nresponse = ctl.initial_response(sys, X0=[1, 0, 0, 0])\ncplt = response.plot()\n# The response only includes q1 and q2 only the first two states (q1 and q2) are outputs of the system.\n# q3 and q4 (the velocities) are not included in the outputs, so they are not shown in the response plot.\n\n\n\nIf you want to change the default way the data are plotted, you can also use the response object to get direct access to the states and outputs.\n\n# Plot the outputs of the system on the same graph, in different colors\nt = response.time\nx = response.states\nplt.plot(t, x[0], 'b', t, x[1], 'r')\nplt.legend(['$x_1$', '$x_2$'])\nplt.xlim(0, 50)\nplt.ylabel('States')\nplt.xlabel('Time [s]')\nplt.title(\"Initial response from $x_1 = 1$, $x_2 = 0$\");\n\n\n\nThere are also lots of options available in initial_response and .plot() for tuning the plots that you get.\n\nfor X0 in [[1, 0, 0, 0], [0, 2, 0, 0], [1, 2, 0, 0], [0, 0, 1, 0], [0, 0, 2, 0]]:\n  response = ctl.initial_response(sys, T=20, X0=X0)\n  response.plot(label=f\"{X0=}\")\n\n\n\n","type":"content","url":"/lab3-intro-to-pythoncontrol#initial-response","position":9},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl4":"Step response","lvl3":"Task 1: Coupled mass spring system"},"type":"lvl4","url":"/lab3-intro-to-pythoncontrol#step-response","position":10},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl4":"Step response","lvl3":"Task 1: Coupled mass spring system"},"content":"Similar to initial_response, you can also generate a step response for a linear system using the step_response function, which returns a time  response object:\n\ncplt = ctl.step_response(sys).plot()\n\n\n\nWe can analyze the properties of the step response using the stepinfo command:\n\nstep_info = ctl.step_info(sys)\nprint(\"rise time = \",\n      step_info[0][0]['RiseTime'], \"seconds\\n\") #[0][0] to access the first output's rise time\nstep_info # A dictionary of all step response characteristics\n\n\n\n\n\nNote that by default the inputs are not included in the step response plot (since they are a bit boring), but you can change that:\n\nstepresp = ctl.step_response(sys)\ncplt = stepresp.plot(plot_inputs=True)\n\n# Plot the inputs on top of the outputs: only overlay on the first output\ncplt = stepresp.plot(plot_inputs='overlay')\n\n# Look at the \"shape\" of the step response\nprint(f\"{stepresp.time.shape=}\")\nprint(f\"{stepresp.inputs.shape=}\")\nprint(f\"{stepresp.states.shape=}\")\nprint(f\"{stepresp.outputs.shape=}\")\n\n\n\n\n\n\n\n","type":"content","url":"/lab3-intro-to-pythoncontrol#step-response","position":11},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl4":"Forced response","lvl3":"Task 1: Coupled mass spring system"},"type":"lvl4","url":"/lab3-intro-to-pythoncontrol#forced-response","position":12},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl4":"Forced response","lvl3":"Task 1: Coupled mass spring system"},"content":"To compute the response to a given input, we can use the forced_response function:\n\nT = np.linspace(0, 50, 500)\nU1 = np.cos(T) # input 1\nU2 = np.sin(3 * T) # input 2\n\nresp1 = ctl.forced_response(sys, T, U1)\nresp2 = ctl.forced_response(sys, T, U2)\nresp3 = ctl.forced_response(sys, T, U1 + U2)\n\n# Plot the individual responses\nresp1.sysname = 'U1'; resp1.plot(color='b')\nresp2.sysname = 'U2'; resp2.plot(color='g')\nresp3.sysname = 'U1 + U2'; resp3.plot(color='r');\n\n\n\n# Show that the system response is linear\ncplt = resp3.plot()\ncplt.axes[0, 0].plot(resp1.time, resp1.outputs[0] + resp2.outputs[0], 'k--')\ncplt.axes[1, 0].plot(resp1.time, resp1.outputs[1] + resp2.outputs[1], 'k--')\ncplt.axes[2, 0].plot(resp1.time, resp1.inputs[0] + resp2.inputs[0], 'k--');\n\n\n\nMore details about time response for LTI system can be found at\n\n\nhttps://​python​-control​.readthedocs​.io​/en​/0​.10​.2​/response​.html​#time​-response​-data\n\n","type":"content","url":"/lab3-intro-to-pythoncontrol#forced-response","position":13},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl3":"Task 2 — Pendulum System"},"type":"lvl3","url":"/lab3-intro-to-pythoncontrol#task-2-pendulum-system","position":14},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl3":"Task 2 — Pendulum System"},"content":"In this task, we will use the Python control Library’s \n\nnlsys that can create a nonlinear input/output system. We will use the pendulum example with control input\\begin{align}\n\\dot{x}_1 &= x_2 \\\\\n\\dot{x}_2 &= -\\frac{g}{L}\\sin x_1 - \\frac{b}{mL^2}\\,x_2 + \\frac{u}{mL^2}\n\\end{align}\n\nwhere:\n\nx_1 = \\theta (angle)\n\nx_2 = \\dot{\\theta} (angular velocity)\n\ng = 9.81\\,\\text{m/s}^2\n\nm = 1 \\,\\text{kg}\n\nL = 1\\,\\text{m}\n\nb = 1.5 (N.m.s/rad, damping coefficient)\n\nAssume the output y=x_1, and control input u, we can create the system using \n\nnlsys function, which constructs a nonlinear system model. Its key parameters are:\n\nupdate: function defining the state update law (system dynamics or state equation)\n\noutput: function defining the output equation\n\nparams: dictionary of system parameters\n\nstates: list of state variable names\n\ninputs: list of input variable names\n\noutputs: list of output variable names\n\nname: (optional) system name","type":"content","url":"/lab3-intro-to-pythoncontrol#task-2-pendulum-system","position":15},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl4":"Create the system","lvl3":"Task 2 — Pendulum System"},"type":"lvl4","url":"/lab3-intro-to-pythoncontrol#create-the-system","position":16},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl4":"Create the system","lvl3":"Task 2 — Pendulum System"},"content":"We can follow three steps to create the system\n\nStep 1: Define the system parameters in a dictionary (e.g., gravity, mass, length, damping).\n\nStep 2: Implement the state update function and output function using these parameters.\n\nStep 3: Create the nonlinear system object using ctl.nlsys, passing the functions, parameters, and variable names.\n\n# Parameter values for pendulum\npendulum_params = {\n    'g': 9.81,            # Gravitational acceleration (m/s^2)\n    'L': 1.0,             # Pendulum length (m)\n    'b': 1.5,             # Damping coefficient (N.m.s/rad)\n    'm': 1.0,             # Pendulum mass (kg)\n}\n\n# State derivative for pendulum\ndef pendulum_update(t, x, u, params):\n    # Extract the configuration and velocity variables from the state vector\n    theta = x[0]                # Angular position of the pendulum\n    thetadot = x[1]             # Angular velocity of the pendulum\n    tau = u[0]                  # Torque applied at the pivot\n\n    # Get the parameter values\n    g, L, b, m = map(params.get, ['g', 'L', 'b', 'm'])\n\n    # Compute the angular acceleration using pendulum dynamics\n    # x_dot_1 = x_2\n    # x_dot_2 = -g/L * sin(x_1) - b/(m*L^2) * x_2 + u/(m*L^2)\n    dthetadot = -g/L * np.sin(theta) - b/(m*L**2) * thetadot + tau/(m*L**2)\n\n    # Return the state update law\n    return np.array([thetadot, dthetadot])\n\n# System output (angle only, as specified y = x_1)\ndef pendulum_output(t, x, u, params):\n    return np.array([x[0]])  # Output y = theta (x_1)\n\n# create the nonlinear system using nlsys\npendulum = ctl.nlsys(\n    pendulum_update, pendulum_output, name='pendulum',\n    params=pendulum_params, states=['theta_', 'thdot_'],\n    outputs=['y'], inputs=['tau'])\n\nprint(pendulum)\nprint(\"\\nParams:\", pendulum.params)\n\n\n\nAfter the system is created, we can use several existing functions in the library to invesitgate its dynamic behaviror, perform linearization, etc.","type":"content","url":"/lab3-intro-to-pythoncontrol#create-the-system","position":17},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl4":"Phase Portraits","lvl3":"Task 2 — Pendulum System"},"type":"lvl4","url":"/lab3-intro-to-pythoncontrol#phase-portraits","position":18},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl4":"Phase Portraits","lvl3":"Task 2 — Pendulum System"},"content":"We will first see how to create the phase portrait for the pendulum system. The portrait shows how the angle (\\theta) and angular velocity (\\dot{\\theta}) evolve over time for different initial conditions. This visualization helps us understand the system’s stability, equilibrium points, and overall behavior.\n\nBelow is sample code to plot the phase portrait for the pendulum using the python-control library:\n\n# Create phase portrait using python-control's built-in function\n# Plot the phase plane with specified limits and grid points\n# phase_plane_plot(system, xlim, ylim, grid_points)\naxis_limits = [-2*np.pi - 1, 2*np.pi + 1, -3, 3]\nctl.phase_plane_plot(\n    pendulum, axis_limits)\n\n# Draw lines at the downward equilibrium angles (unstable equilibria)\nplt.plot([-np.pi, -np.pi], [-2, 2], 'k--', linewidth=2, label='Unstable equilibria')\nplt.plot([np.pi, np.pi], [-2, 2], 'k--', linewidth=2)\n\n\n\n\n","type":"content","url":"/lab3-intro-to-pythoncontrol#phase-portraits","position":19},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl4":"Linearization","lvl3":"Task 2 — Pendulum System"},"type":"lvl4","url":"/lab3-intro-to-pythoncontrol#linearization","position":20},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl4":"Linearization","lvl3":"Task 2 — Pendulum System"},"content":"To study the open loop dynamics of the system, we compute the linearization of the dynamics about the operating point corresponding to \\theta_\\text{d}=x_{1d} = y_d=15^\\circ.\n\n# Convert the equilibrium angle to radians\ntheta_d = (85 / 180) * np.pi\n\n# For the pendulum, the equilibrium torque needed to hold position theta_d is:\n# tau_d = m*g*L*sin(theta_d)\ng, L, m = map(pendulum.params.get, ['g', 'L', 'm'])\nu_d = m * g * L * np.sin(theta_d)\nprint(\"Equilibrium torque = %g N.m\" % u_d)\n\n# Linearize the system about the operating point\n# https://python-control.readthedocs.io/en/0.10.2/generated/control.linearize.html\nP = pendulum.linearize([theta_d, 0], u_d)\nprint(\"Linearized dynamics:\\n\", P)\n\n\n\n","type":"content","url":"/lab3-intro-to-pythoncontrol#linearization","position":21},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl4":"Open loop step response","lvl3":"Task 2 — Pendulum System"},"type":"lvl4","url":"/lab3-intro-to-pythoncontrol#open-loop-step-response","position":22},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl4":"Open loop step response","lvl3":"Task 2 — Pendulum System"},"content":"We use the step_response function to plot the step response of the linearized, open-loop system and compute the “rise time” and “settling time”\n\n# Compute the step response\nlin_response = ctl.step_response(P)\ntimepts, output = lin_response.time, lin_response.outputs # obtain time and output arrays\n\n# Plot step response (input 0 to output 0)\nplt.plot(timepts, output)\nplt.xlabel(\"Time $t$ [s]\")\nplt.ylabel(\"Position $y$ [cm]\")\nplt.title(\"Step response for the linearized, open-loop system\")\n\n# Compute and print selected properties of the step response\nresults = ctl.step_info(P)\nprint(\"Rise time:\", results['RiseTime'])              # 10-90% rise time\nprint(\"Settling time:\", results['SettlingTime'])      # 2% error\n\n# Print all response characteristics\nprint(\"Step Response Characteristics:\")\nprint(\"=\" * 35) # print a line of '=' for emphasis\nfor key, value in results.items():\n    if value is not None: # some values may be None\n        print(f\"{key}: {value:.4f}\") # formatted to 4 decimal places\n\n# Calculate the rise time start time by hand\nrise_time_start = timepts[np.where(output > 0.1 * output[-1])[0][0]]\nrise_time_stop = rise_time_start + results['RiseTime']\n\n# Add lines for the step response features\nplt.plot([timepts[0], timepts[-1]], [output[-1], output[-1]], 'k--')\n\nplt.plot([rise_time_start, rise_time_start], [0, 0.2], 'k:')# 10% line\nplt.plot([rise_time_stop, rise_time_stop], [0, 0.2], 'k:') # 90% line\nplt.arrow(rise_time_start, 0.14, rise_time_stop - rise_time_start, 0) # rise time arrow\nplt.text((rise_time_start + rise_time_stop)/2-0.1, 0.15, '$T_r$') # rise time text\n\nplt.plot([0, 0], [0, 0.2], 'k:') # 0% line\nplt.plot([results['SettlingTime'], results['SettlingTime']], [0, 0.2], 'k:') # settling time line\nplt.arrow(0, 0.19, results['SettlingTime'], 0)\nplt.text(results['SettlingTime']/2, 0.2, '$T_s$');\n\n\n\n\n\n\nWe see that the open loop step response (for the linearized system) is stable, and that the final value is less than 1 (this value just depends on the parameters in the system).\n\nWe can also compare the response of the linearized system to the full nonlinear system:\n\n# Simulate nonlinear system with same time points and input\n# Need to specify initial condition and proper input format\nnl_response = ctl.input_output_response(pendulum, timepts, U=1)\n\n# Plot step response comparison\nplt.figure(figsize=(8, 5))\nplt.plot(timepts, output, label=\"linearized\", linewidth=2) # linearized response\nplt.plot(timepts, nl_response.outputs.squeeze(), label=\"nonlinear\", linewidth=2, linestyle='--') # nonlinear response\n\nplt.xlabel(\"Time $t$ [s]\")\nplt.ylabel(\"Position $y$ [rad]\")\nplt.title(\"Step response comparison: linearized vs nonlinear system\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Print final values for comparison\nprint(f\"Linearized final value: {output[-1]:.4f} rad\")\nprint(f\"Nonlinear final value: {nl_response.outputs[-1]:.4f} rad\")\nprint(f\"Difference: {abs(output[-1] - nl_response.outputs[-1]):.4f} rad\")\n\n\n\n\n\n#!!! Updated version for the comparison !!!\n# For linearized system - step response (this is already a perturbation)\nlin_response = ctl.step_response(P)\ntimepts, output = lin_response.time, lin_response.outputs\n\n# For nonlinear system - apply the SAME perturbation\n# The nonlinear system needs: equilibrium input + step perturbation\nstep_magnitude = 0.01  # Same as linearized system\nu_nonlinear = np.ones_like(timepts) * (u_d + step_magnitude)  # u_d + step\n\n# Simulate nonlinear system starting from equilibrium with perturbed input\nnl_response = ctl.input_output_response(\n    pendulum, timepts,\n    U = u_nonlinear,  # equilibrium + perturbation\n    X0 = [theta_d, 0], # start from equilibrium\n)\n\n# Plot step response comparison\nplt.figure(figsize=(8, 5))\n# linearized response: note we start from the equilibrium position and scale the output by the step magnitude: step_magnitude*output+theta_d\nplt.plot(timepts, step_magnitude*output+theta_d, label=\"linearized\", linewidth=2)\n# nonlinear response\nplt.plot(timepts, nl_response.outputs.squeeze(), label=\"nonlinear\", linewidth=2, linestyle='--')\n\nplt.xlabel(\"Time $t$ [s]\")\nplt.ylabel(\"Position $y$ [rad]\")\nplt.title(\"Step response comparison: linearized vs nonlinear system\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Print final values for comparison\nprint(f\"Linearized final value: {output[-1]:.4f} rad\")\nprint(f\"Nonlinear final value: {nl_response.outputs[-1]:.4f} rad\")\nprint(f\"Difference: {abs(output[-1] - nl_response.outputs[-1]):.4f} rad\")\n\n\n\n\n\nExplore additional time domain analysis capabilities.\n\n# Different input responses\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# 1. Step response (already computed)\naxes[0,0].plot(timepts, output)\naxes[0,0].set_title('Step Response')\naxes[0,0].set_xlabel('Time [s]')\naxes[0,0].set_ylabel('Output')\naxes[0,0].grid(True, alpha=0.3)\n\n# 2. Impulse response\nt_imp, y_imp = ctl.impulse_response(P)\naxes[0,1].plot(t_imp, y_imp)\naxes[0,1].set_title('Impulse Response')\naxes[0,1].set_xlabel('Time [s]')\naxes[0,1].set_ylabel('Output')\naxes[0,1].grid(True, alpha=0.3)\n\n# 3. Ramp response (using forced_response)\nt_ramp = np.linspace(0, 10, 1000)\nu_ramp = t_ramp  # Ramp input\nt_ramp_resp, y_ramp = ctl.forced_response(P, t_ramp, u_ramp)\naxes[1,0].plot(t_ramp_resp, y_ramp, label='Output')\naxes[1,0].plot(t_ramp, u_ramp, '--', alpha=0.7, label='Ramp Input')\naxes[1,0].set_title('Ramp Response')\naxes[1,0].set_xlabel('Time [s]')\naxes[1,0].set_ylabel('Amplitude')\naxes[1,0].legend()\naxes[1,0].grid(True, alpha=0.3)\n\n# 4. Sinusoidal response\nt_sin = np.linspace(0, 10, 1000)\nfreq = 1.0  # 1 rad/s\nu_sin = np.sin(freq * t_sin)\nt_sin_resp, y_sin = ctl.forced_response(P, t_sin, u_sin)\naxes[1,1].plot(t_sin_resp, y_sin, label='Output')\naxes[1,1].plot(t_sin, u_sin, '--', alpha=0.7, label='Sine Input')\naxes[1,1].set_title(f'Sinusoidal Response ({freq} rad/s)')\naxes[1,1].set_xlabel('Time [s]')\naxes[1,1].set_ylabel('Amplitude')\naxes[1,1].legend()\naxes[1,1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n","type":"content","url":"/lab3-intro-to-pythoncontrol#open-loop-step-response","position":23},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl2":"HW problems"},"type":"lvl2","url":"/lab3-intro-to-pythoncontrol#hw-problems","position":24},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl2":"HW problems"},"content":"","type":"content","url":"/lab3-intro-to-pythoncontrol#hw-problems","position":25},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl3":"Problem 1: Servo Mechanism Linearization and Time Response","lvl2":"HW problems"},"type":"lvl3","url":"/lab3-intro-to-pythoncontrol#problem-1-servo-mechanism-linearization-and-time-response","position":26},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl3":"Problem 1: Servo Mechanism Linearization and Time Response","lvl2":"HW problems"},"content":"Consider the same servo mechanism consisting of a spring loaded arm that is driven by a motor, as shown below.\n\n\n\nThe equations of motion for the system are given byJ \\ddot \\theta = -b \\dot\\theta - k r\\sin\\theta + \\tau_\\text{m},\n\nwhere\n\nJ is the rotational inertia of the arm (kg·m²)\n\nb is the viscous damping coefficient (N·m·s/rad)\n\nk is the linear spring constant (N/m)\n\nr is the effective radius or lever arm where the spring force acts (m)\n\nDefine the state vector x = [\\theta,\\, \\dot{\\theta}]^T and output y = \\theta. The dynamics can be written in state-space form: \\dot{x} = f(x, u), y = g(x, u) with u=\\tau_\\text{m} as\\begin{align*}\n\\dot{x} &= \\begin{bmatrix} \\dot{x}_1 \\\\ \\dot{x}_2 \\end{bmatrix}  =\n  \\begin{bmatrix} x_2 \\\\ -k r \\sin x_1 / J - bx_2 / J + u/J \\end{bmatrix} \\\\\n  y &= x_1\n\\end{align*}k = 1,\\quad J = 100,\\quad b = 10,\n    \\quad r = 1\n\nAnswer the following questions (questions 1 and 2 require manual derivation; you may attach a separate PDF):\n\nLinearize the system around an operating point x_d,\\, u_d,\\, y_d. You need to manual derive this usingA = \\frac{\\partial f}{\\partial x},\\quad\nB = \\frac{\\partial f}{\\partial u},\\quad\nC = \\frac{\\partial g}{\\partial x},\\quad\nD = \\frac{\\partial g}{\\partial u}\n\nBased on the A, B, C, D matrices you have obtained, get the numerical values for two cases y_d=10^\\circ, y_d=75^\\circ. Note that you first need to compute the corresponding x_d and u_d for each case. You may use Python to compute the specific matrices.\n\nProgramming problem: First create the system using nlsys and then linearize the system for two cases y_d=10^\\circ, y_d=75^\\circ using the linearization function. Compare the A, B, C, D matrices with the one you obtain in question 2. They should match.\n\nProgramming problem: Generate the step reponse for the linearized system when y_d=10^\\circ and label the rising time and settling time, similar to the pendulum example. You may need to revise the code to show the texts at proper locations in the figure.\n\nProgramming problem: Plot the step response for the orignal nonlinear system and the two linearized system for both y_d=10^\\circ and y_d=75^\\circ. Compare the results.\n\n","type":"content","url":"/lab3-intro-to-pythoncontrol#problem-1-servo-mechanism-linearization-and-time-response","position":27},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl3":"Problem 2: Cart–Pole Linearization and Time Response","lvl2":"HW problems"},"type":"lvl3","url":"/lab3-intro-to-pythoncontrol#problem-2-cart-pole-linearization-and-time-response","position":28},{"hierarchy":{"lvl1":"Lab3: Python Control Systems Library","lvl3":"Problem 2: Cart–Pole Linearization and Time Response","lvl2":"HW problems"},"content":"For this problem, we will consider the cart–pole system, which is widely used for demonstrating control concepts. You can see a video for the system in action \n\nhere. It consist of a cart of mass m_c and a pole of mass m_p and length l pivoted on the cart. A horizontal force f_x is applied to the cart.\n\nThe equations of motion for the system are given by\\ddot{x} = \\frac{1}{m_c + m_p \\sin^2\\theta}\n\\left[f_x + m_p \\sin\\theta\\big(l\\dot{\\theta}^2 + g\\cos\\theta\\big)\\right]\\ddot{\\theta} = \\frac{1}{l(m_c + m_p \\sin^2\\theta)}\n\\left[-f_x\\cos\\theta - m_p l\\dot{\\theta}^2\\cos\\theta\\sin\\theta\n- (m_c+m_p)g\\sin\\theta\\right]\n\nwhere\n\nx: cart position\n\n\\theta: pole angle measured from vertical (counter-clockwise positive)\n\ng: gravitational constant\n\nf_x: horizontal force input to the cart\n\nDefine the state vector, output, and control input as\\mathbf{x} = [\\,x,\\, \\dot{x},\\, \\theta,\\, \\dot{\\theta}\\,]^T,\n\\quad y = \\theta,\n\\quad u = f_x\n\nso the dynamics can be written as \\dot{x} = f(x, u),\\; y = g(x, u).\n\nUse the following parameters:m_c = 1.0,\\quad m_p = 0.1,\\quad l = 0.5,\\quad g = 9.81\n\nState Space form: Rewrite the sytem into state space form \\dot{x} = f(x, u), y = g(x, u) with the state, output, and control input.\n\nManual Linearization:  Linearize the system around an operating point (x_d, \\dot{x}_d, \\theta_d, \\dot{\\theta}_d, u_d).Derive the state-space matricesA = \\frac{\\partial f}{\\partial x},\\quad\nB = \\frac{\\partial f}{\\partial u},\\quad\nC = \\frac{\\partial g}{\\partial x},\\quad\nD = \\frac{\\partial g}{\\partial u}\n\nevaluated at the operating point. Show your derivation clearly (attach a PDF if needed).\n\nEquilibria at Different Operating Points:  Compute the equilibrium input u_d for both:\n\n\\theta_d = 0 (downward equilibrium, stable)\n\n\\theta_d = \\pi (upright equilibrium, unstable)\n\nFor each case, compute the numerical values of A, B, C, D.\n\nProgramming Problem:  Use the python-control library’s nonlinear system tools (such as nlsys and linearize) to construct the cart–pole system and compute the A, B, C, D matrices at the two equilibrium points (\\theta_d = 0 and \\theta_d = \\pi). Verify that your results match the manual derivation in Question 3.\n\nProgramming Problem: Time Responses & Nonlinear vs Linear Comparison\n\n(a) Generate the step response in y = \\theta for the downward linearized model.  Simulate the nonlinear model under the same step input force and compare its output against the linearized model.\n\n(b) Generate the step response in y = \\theta for the upward linearized model.  Simulate the nonlinear model under the same step input force and compare its output against the linearized model.\n\n(c) Discuss the differences you observe. In particular, explain why the linear model only provides a good approximation near the operating point.","type":"content","url":"/lab3-intro-to-pythoncontrol#problem-2-cart-pole-linearization-and-time-response","position":29},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control"},"type":"lvl1","url":"/lab4-closedloopcontrol","position":0},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control"},"content":"All the previous labs are preparatory for this lab: controller design. Given a dynamic system, how can we design a controller to make it stable (such as make a drone hover in air), track a desired trajectory (make a robotic arm follow a line for welding), etc.? In this lab, we will practice how to design controllers based on state-space models for mechanical systems. You will learn how to analyze their controllability, connect the controller and the dynamics of the systems, and design controllers that achieve desired closed-loop performance. Through hands-on exercises, you will use the Python Control Systems Library to:\n\nModel both linear and nonlinear systems\n\nTest system controllability\n\nDesign state-feedback controllers using eigenvalue assignment techniques\n\nCompute feedforward gains to enable accurate reference tracking\n\nSimulate and visualize the time response of both open-loop and closed-loop systems\n\nBy the end of this lab, you will gain practical experience in applying modern control design methods to real-world mechanical systems using Python.\n\n","type":"content","url":"/lab4-closedloopcontrol","position":1},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl3":"Setup (run once per environment)"},"type":"lvl3","url":"/lab4-closedloopcontrol#setup-run-once-per-environment","position":2},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl3":"Setup (run once per environment)"},"content":"We first need to install python-control (and import required libraries) every time you open this notebook, to ensure all dependencies are available in your current environment.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Import the python-control package\ntry:\n    import control as ctl # ctl is the alias for control\n    print(\"python-control\", ctl.__version__)\nexcept ImportError:\n    %pip install control\n    import control as ctl\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (7, 4)\nplt.rcParams['axes.grid'] = True\n\n\n\n","type":"content","url":"/lab4-closedloopcontrol#setup-run-once-per-environment","position":3},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl3":"Task 1 — Conroller Design for Coupled Mass Spring System"},"type":"lvl3","url":"/lab4-closedloopcontrol#task-1-conroller-design-for-coupled-mass-spring-system","position":4},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl3":"Task 1 — Conroller Design for Coupled Mass Spring System"},"content":"In this task, we will design a controller for the coupled mass-spring system using the Python Control Systems Library.\n\n","type":"content","url":"/lab4-closedloopcontrol#task-1-conroller-design-for-coupled-mass-spring-system","position":5},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl4":"System dynamics","lvl3":"Task 1 — Conroller Design for Coupled Mass Spring System"},"type":"lvl4","url":"/lab4-closedloopcontrol#system-dynamics","position":6},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl4":"System dynamics","lvl3":"Task 1 — Conroller Design for Coupled Mass Spring System"},"content":"For completeless, we include the dynamics of the system again here\\begin{aligned}\n  m \\ddot{q}_1 &= -2 k q_1 - c \\dot{q}_1 + k q_2, \\\\\n  m \\ddot{q}_2 &= k q_1 - 2 k q_2 - c \\dot{q}_2 + ku\n\\end{aligned}\n\nor in state space form, with state vector x = [x_1,\\, x_2,\\, x_3,\\, x_4]^T where\n\nx_1 = q_1 (position of mass 1)\n\nx_2 = q_2 (position of mass 2)\n\nx_3 = \\dot{q}_1 (velocity of mass 1)\n\nx_4 = \\dot{q}_2 (velocity of mass 2)\n\nFor output, we consider we are only interested in the position of mass one for now. In this case, we have output of y= q_1\\begin{aligned}\n    \\dfrac{dx}{dt} &= \\begin{bmatrix}\n        0 & 0 & 1 & 0 \\\\\n        0 & 0 & 0 & 1 \\\\[0.5ex]\n        -\\dfrac{2k}{m} & \\dfrac{k}{m} & -\\dfrac{c}{m} & 0 \\\\[0.5ex]\n        \\dfrac{k}{m} & -\\dfrac{2k}{m} & 0 & -\\dfrac{c}{m}\n    \\end{bmatrix} x\n    + \\begin{bmatrix}\n        0 \\\\ 0 \\\\[0.5ex] 0 \\\\[1ex] \\dfrac{k}{m}\n    \\end{bmatrix} u, \\\\[2ex]\n    y &= \\begin{bmatrix}\n        1 & 0 & 0 & 0\n    \\end{bmatrix} x\n\\end{aligned}\n\n# Define the parameters for the system\nm, c, k = 1, 0.1, 2\n# Create a linear system\nA = np.array([\n    [0, 0, 1, 0],\n    [0, 0, 0, 1],\n    [-2*k/m, k/m, -c/m, 0],\n    [k/m, -2*k/m, 0, -c/m]\n])\nB = np.array([[0], [0], [0], [k/m]])\nC = np.array([1, 0, 0, 0])\nD = 0\n\n# Create a state-space system using A, B, C, D\n# https://python-control.readthedocs.io/en/0.10.2/generated/control.ss.html\nsys = ctl.ss(A, B, C, D, outputs=['q1'], name=\"coupled spring mass\")\nprint(sys)\n\n\n\n","type":"content","url":"/lab4-closedloopcontrol#system-dynamics","position":7},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl4":"Controllability Testing","lvl3":"Task 1 — Conroller Design for Coupled Mass Spring System"},"type":"lvl4","url":"/lab4-closedloopcontrol#controllability-testing","position":8},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl4":"Controllability Testing","lvl3":"Task 1 — Conroller Design for Coupled Mass Spring System"},"content":"\n\n# Compute the controllability matrix and its rank and determinant\nctrb = ctl.ctrb(A, B)\nrank = np.linalg.matrix_rank(ctrb)\ndeterminant = np.linalg.det(ctrb)\n\nprint(f\"Controllability matrix:\\n{ctrb}\")\nprint(f\"Determinant: {determinant} (nonzero means full rank)\")\nprint(f\"Rank: {rank} (should be {A.shape[0]} for controllable system)\")\n\n\n\n","type":"content","url":"/lab4-closedloopcontrol#controllability-testing","position":9},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl4":"Controller Design","lvl3":"Task 1 — Conroller Design for Coupled Mass Spring System"},"type":"lvl4","url":"/lab4-closedloopcontrol#controller-design","position":10},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl4":"Controller Design","lvl3":"Task 1 — Conroller Design for Coupled Mass Spring System"},"content":"Now we can design a state-feedback controller to make the output y track a desired value r.u=-Kx+K_fr\n\n# We assume the following desired closed-loop poles (eigenvalues)\ndesired_eigenvalues = [-2, -2.5, -3, -3.5]  # original choice\n# Alternative choices:\n# desired_eigenvalues = [-2 + 1j, -2 - 1j, -3 + 2j, -3 - 2j]\n\n# Place the poles using state feedback (assuming full state feedback)\nK = ctl.place(A, B, desired_eigenvalues)\nprint(\"State feedback gain K:\", K)\n\n# Compute feedforward gain for reference tracking (for output y = q1)\nA_cl = A - B @ K\nKf =  (-1 / (C @ np.linalg.inv(A_cl) @ B))\nprint('Kf: '+str(Kf))\n\n\n\n\n","type":"content","url":"/lab4-closedloopcontrol#controller-design","position":11},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl4":"Simulate the closed-loop system","lvl3":"Task 1 — Conroller Design for Coupled Mass Spring System"},"type":"lvl4","url":"/lab4-closedloopcontrol#simulate-the-closed-loop-system","position":12},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl4":"Simulate the closed-loop system","lvl3":"Task 1 — Conroller Design for Coupled Mass Spring System"},"content":"Now that we have our gains designed, we can simulate the closed loop system:\n$$\\begin{align}\n\\frac{dx}{dt} &= Ax + Bu \\\\\n              & = Ax + B(-Kx+k_fr)\\\\\n              & =(A-BK)x+Bk_fr \\\\\n              & =  A_{cl}x + B_{cl} r,\n\\end{align}\n\n$\nwhere A_{cl} = A-BK, B_{cl} = Bk_f. Notice that, with a state feedback controller, the new (closed loop) dynamics matrix A_{cl} absorbs the old (open loop) \"input\" u, and the new (closed loop) input is our reference signal r$. This is shown in the following figure.\n\n# Create a closed loop system\nA_cl = A - B @ K\nB_cl =  B * Kf\nclsys = ctl.ss(A_cl, B_cl, C, 0)\nprint(clsys)\n\n\n\n# Plot the step response with and without the controller\ntvec = np.linspace(0, 10, 100)\nU = 1  # Step input magnitude\nr = 1  # Reference value\n\n# Closed-loop response (with controller)\ntime_op, output_op = ctl.input_output_response(clsys, tvec, U)\n\n# Open-loop response (no controller)\ntime_ol, output_ol = ctl.input_output_response(sys, tvec, U)\n\nplt.plot(time_op, output_op, label='Closed-loop (with controller)')\nplt.plot(time_ol, output_ol, label='Open-loop (no controller)')\nplt.plot([time_op[0], time_op[-1]], [r, r], '--', label='Reference $r$')\nplt.legend()\nplt.ylabel(\"Output\")\nplt.xlabel(\"Time $t$ [sec]\")\nplt.title(\"Step Response: Closed-loop vs Open-loop\")\nplt.show()\n\n\n\n","type":"content","url":"/lab4-closedloopcontrol#simulate-the-closed-loop-system","position":13},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl3":"Task 2 — Conroller Design for Pendulum System"},"type":"lvl3","url":"/lab4-closedloopcontrol#task-2-conroller-design-for-pendulum-system","position":14},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl3":"Task 2 — Conroller Design for Pendulum System"},"content":"In this task, we will design controllers for the pendulum system. For completeness, we include the dynamics equation here again:\\begin{align}\n\\dot{x}_1 &= x_2 \\\\\n\\dot{x}_2 &= -\\frac{g}{L}\\sin x_1 - \\frac{b}{mL^2}\\,x_2 + \\frac{u}{mL^2}\n\\end{align}\n\nwhere:\n\nx_1 = \\theta (angle)\n\nx_2 = \\dot{\\theta} (angular velocity)\n\ng = 9.81\\,\\text{m/s}^2\n\nm = 1 \\,\\text{kg}\n\nL = 1\\,\\text{m}\n\nb = 1.5 (N.m.s/rad, damping coefficient)\n\nWe begin by creating a nonlinear model of the system:\n\n# Parameter values for pendulum\npendulum_params = {\n    'g': 9.81,            # Gravitational acceleration (m/s^2)\n    'L': 1.0,             # Pendulum length (m)\n    'b': 0.5,             # Damping coefficient (N.m.s/rad)\n    'm': 1.0,             # Pendulum mass (kg)\n}\n\n# State derivative for pendulum\ndef pendulum_update(t, x, u, params):\n    # Extract the configuration and velocity variables from the state vector\n    theta = x[0]                # Angular position of the pendulum\n    thetadot = x[1]             # Angular velocity of the pendulum\n    tau = u[0]                  # Torque applied at the pivot\n\n    # Get the parameter values\n    g, L, b, m = map(params.get, ['g', 'L', 'b', 'm'])\n\n    # Compute the angular acceleration using pendulum dynamics\n    # x_dot_1 = x_2\n    # x_dot_2 = -g/L * sin(x_1) - b/(m*L^2) * x_2 + u/(m*L^2)\n    dthetadot = -g/L * np.sin(theta) - b/(m*L**2) * thetadot + tau/(m*L**2)\n\n    # Return the state update law\n    return np.array([thetadot, dthetadot])\n\n# System output (angle only, as specified y = x_1)\ndef pendulum_output(t, x, u, params):\n    return np.array([x[0]])  # Output y = theta (x_1)\n\n# create the nonlinear system using nlsys,\n# note that the outputs is theta so that we can successfully connect the controller to it\npendulum = ctl.nlsys(\n    pendulum_update, pendulum_output, name='pendulum',\n    params=pendulum_params, states=['theta', 'thetadot'],\n    outputs=['theta'], inputs=['tau'])\n\nprint(pendulum)\nprint(\"\\nParams:\", pendulum.params)\n\n\n\n","type":"content","url":"/lab4-closedloopcontrol#task-2-conroller-design-for-pendulum-system","position":15},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl4":"Proportional feedback controller","lvl3":"Task 2 — Conroller Design for Pendulum System"},"type":"lvl4","url":"/lab4-closedloopcontrol#proportional-feedback-controller","position":16},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl4":"Proportional feedback controller","lvl3":"Task 2 — Conroller Design for Pendulum System"},"content":"We first try to stabilize the system at \\theta_d=\\pi using a simple proportional feedback controller:u = -k_\\text{p} (\\theta-\\pi).\n\nThis controller can be designed as an input/output system that has no state dynamics, just a mapping from the inputs to the outputs.\n\nInputs and outputs for each block that we will create:\n\nController\n\nInputs: \\theta (measured angle from pendulum), r (reference/desired angle)\n\nOutput: \\tau (control torque to be applied to the pendulum)\n\nPendulum dynamics\n\nInput: \\tau (control torque from controller)\n\nOutputs: \\theta (pendulum angle), \\dot{\\theta} (angular velocity)\n\nClosed-loop system\n\nInput: r (reference/desired angle)\n\nOutput: \\theta (pendulum angle)\n\nThis separation of roles allows us to connect the controller and the plant (pendulum) in a feedback loop, where the controller processes the measured output and reference to generate the appropriate input for the plant.\n\n# Set up the controller\ndef propctrl_output(t, x, u, params):\n  kp = params.get('kp', 1)\n  return -kp * (u[0] - u[1]) # u[0] is theta, u[1] is r, which is the desired angle\n\n# Create the proportional controller as a nonlinear system\npropctrl = ctl.nlsys(\n    None, propctrl_output, name=\"p_ctrl\",\n    inputs=['theta', 'r'], outputs='tau'\n)\nprint(propctrl)\n\n\n\nTo connect the controller to the system, we use the \n\ninterconnect function, which will connect all signals that have the same names:\n\n# Create the closed loop system\nclsys = ctl.interconnect(\n    [pendulum, propctrl], name='pendulum w/ proportional feedback',\n    inputs=['r'], outputs=['theta'], params={'kp'})\nprint(clsys)\n\n\n\nWe can now linearize the closed loop system at different values of k_p and compute the eigenvalues to check for stability:\n\nfor kp in [0, 1, 10]:\n    lin_sys = clsys.linearize([np.pi, 0], [0], params={'kp': kp})\n    A = lin_sys.A\n    eigvals = np.linalg.eigvals(A)\n    print(f\"kp = {kp}; A =\\n{A}\\neigenvalues = {eigvals}\\n\")\n\n\n\nNow we can simulate the closed-loop system (with proportional control) and open-loop (no control)\n\ntimepts = np.linspace(0, 20, 500)\nx0 = [np.pi + 0.3, 0]  # initial condition: slightly perturbed from upright\n\n# Closed-loop: kp = 10\n# U=np.full_like(timepts, np.pi) creates the reference input signal for the simulation.\nCl_response = ctl.input_output_response(\n    clsys, timepts, U=np.full_like(timepts, np.pi), X0=x0, params={'kp': 10}, return_x=True\n)\n\n# Open-loop: kp = 0\nOl_response = ctl.input_output_response(\n    clsys, timepts, U=np.full_like(timepts, np.pi), X0=x0, params={'kp': 0}, return_x=True\n)\n\nplt.plot(timepts, Cl_response.outputs, label='Closed-loop (kp=10)')\nplt.plot(timepts, Ol_response.outputs, label='Open-loop (kp=0)')\nplt.axhline(np.pi, color='gray', linestyle='--', label='Reference ($\\pi$)')\nplt.xlabel('Time [s]')\nplt.ylabel('Theta [rad]')\nplt.title('Closed-loop vs Open-loop Response')\nplt.legend()\nplt.show()\n\n\n\n\n\n","type":"content","url":"/lab4-closedloopcontrol#proportional-feedback-controller","position":17},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl4":"State space controller","lvl3":"Task 2 — Conroller Design for Pendulum System"},"type":"lvl4","url":"/lab4-closedloopcontrol#state-space-controller","position":18},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl4":"State space controller","lvl3":"Task 2 — Conroller Design for Pendulum System"},"content":"For the proportional controller, we have limited control over the dynamics of the closed loop system.  An alternative is to use “full state feedback” for reference tracking, in which we set\\tilde{u} = u-u_d= -K\\tilde{x} + K_f \\tilde{r} = -k_1 (\\theta -\\theta_d) - k_2 (\\dot\\theta -\\dot\\theta_d) + K_f (r - \\theta_d)\n\nwhere K = [k_1, k_2] is the feedback gain and K_f is the feedforward gain for tracking the reference r = \\theta_d. Note that we use \\tilde{u} and \\tilde{x} here to indicate we linearize the system at an operating point.\n\nWe have verified in class the linearized system around \\theta_d=\\pi is controllable.\n\n# Linearize the original pendulum system around theta = pi, thetadot = 0, tau = 0\nP = pendulum.linearize([np.pi, 0], [0])\nctrb_matrix = ctl.ctrb(P.A, P.B) # Controllability matrix\nrank = np.linalg.matrix_rank(ctrb_matrix)\ndeterminant = np.linalg.det(ctrb_matrix)\n\nprint(f\"Determinant: {determinant} (nonzero means full rank)\")\nprint(f\"Controllability matrix:\\n{ctrb_matrix}\")\nprint(f\"Rank: {rank} (should be {P.A.shape[0]} for controllable system)\")\n\n\n\nTo compute the gains K, we make use of the place command, applied to the linearized system:\n\n# Place the closed loop eigenvalues (poles) at desired locations\n# Assume the desired poles are at -1 +/- 0.1j\nK = ctl.place(P.A, P.B, [-1 + 0.1j, -1 - 0.1j])\n#K = ctl.place(P.A, P.B, [-2, -1.0])\nprint(f\"Feedback gain K = {K}\")\n\n# Compute feedforward gain for reference tracking\n# For pendulum, C = [1, 0] to extract theta from state\nC = np.array([[1, 0]])  # Output matrix for theta\nA_cl = P.A - P.B @ K    # Closed-loop A matrix\nKf = -1 / (C @ np.linalg.inv(A_cl) @ P.B)[0, 0]  # [0, 0] extracts the single element from this 1×1 matrix\n\nprint(f\"Feedforward gain Kf = {Kf}\")\n\n\n\nNow we can obtain the controller\n\ndef statefbk_output(t, x, u, params):\n  K = params.get('K', np.array([0, 0]))\n  Kf = params.get('Kf', 0)\n\n  # Current state from pendulum\n  x_current = u[0:2]  # [theta, thetadot]\n  r = u[2]            # Reference input (desired theta)\n\n  # operating point where we linearized\n  x_d = np.array([np.pi, 0])  # [theta_eq, thetadot_eq]\n\n\n  # Controller: \\tilde{u} = -K*\\tilde{x} + Kf * \\tilde{r}\n  # we need to switch to the original\n  return -K @ (x_current - x_d) + Kf * (r - np.pi)\n\n# Create the state feedback controller as a nonlinear system\nstatefbk = ctl.nlsys(\n  None, statefbk_output, name=\"k_ctrl\",\n  inputs=['theta', 'thetadot', 'r'], outputs='tau'\n)\nprint(statefbk)\n\n\n\nAfter defining the state-feedback controller, we can connect it to the pendulum system to create a closed-loop system.\n\nclsys_sf = ctl.interconnect(\n    [pendulum, statefbk], name='pendulum w/ state feedback',\n    inputs=['r'], outputs=['theta'], params={'K': K, 'Kf': Kf})\nprint(clsys_sf)\n\n\n\nNow we can see how the designed state-feedback controller will influence the system. We can visulize this through the same response.\n\n# Compare the performance of state-space controller vs proportional controller\n\n# Simulate state-space controller (closed-loop)\nsf_response = ctl.input_output_response(\n    clsys_sf, timepts, U=np.full_like(timepts, np.pi), X0=x0, return_x=True\n)\n\n# Simulate proportional controller (closed-loop)\np_response = ctl.input_output_response(\n    clsys, timepts, U=np.full_like(timepts, np.pi), X0=x0, params={'kp': 10}, return_x=True\n)\n\nplt.plot(timepts, p_response.outputs, label='Proportional Controller (kp=10)')\nplt.plot(timepts, sf_response.outputs, label='State-Space Controller')\nplt.axhline(np.pi, color='gray', linestyle='--', label='Reference ($\\pi$)')\nplt.xlabel('Time [s]')\nplt.ylabel('Theta [rad]')\nplt.title('Closed-loop Response: State-Space vs Proportional Controller')\nplt.legend()\nplt.show()\n\n\n\n\n\n","type":"content","url":"/lab4-closedloopcontrol#state-space-controller","position":19},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl4":"Things to try","lvl3":"Task 2 — Conroller Design for Pendulum System"},"type":"lvl4","url":"/lab4-closedloopcontrol#things-to-try","position":20},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl4":"Things to try","lvl3":"Task 2 — Conroller Design for Pendulum System"},"content":"Here are some things to try with the above code:\n\nTry changing the locations of the closed loop eigenvalues in the place command to make the repose better than the proportional controller.\n\nTry leaving the state space controller fixed but changing the parameters of the system dynamics (m, l, b).  Does the controller still stabilize the system?\n\n","type":"content","url":"/lab4-closedloopcontrol#things-to-try","position":21},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl2":"HW problems"},"type":"lvl2","url":"/lab4-closedloopcontrol#hw-problems","position":22},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl2":"HW problems"},"content":"","type":"content","url":"/lab4-closedloopcontrol#hw-problems","position":23},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl3":"Problem 1: Servo Mechanism State-Feedback Control","lvl2":"HW problems"},"type":"lvl3","url":"/lab4-closedloopcontrol#problem-1-servo-mechanism-state-feedback-control","position":24},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl3":"Problem 1: Servo Mechanism State-Feedback Control","lvl2":"HW problems"},"content":"Consider the same servo mechanism again. The equations of motion for the system are given byJ \\ddot \\theta = -b \\dot\\theta - k r\\sin\\theta + \\tau_\\text{m},\n\nwhere\n\nJ is the rotational inertia of the arm (kg·m²)\n\nb is the viscous damping coefficient (N·m·s/rad)\n\nk is the linear spring constant (N/m)\n\nr is the effective radius or lever arm where the spring force acts (m)\n\nDefine the state vector x = [\\theta,\\, \\dot{\\theta}]^T and output y = \\theta. The dynamics can be written in state-space form: \\dot{x} = f(x, u), y = g(x, u) with u=\\tau_\\text{m} as\\begin{align*}\n\\dot{x} &= \\begin{bmatrix} \\dot{x}_1 \\\\ \\dot{x}_2 \\end{bmatrix}  =\n  \\begin{bmatrix} x_2 \\\\ -k r \\sin x_1 / J - bx_2 / J + u/J \\end{bmatrix} \\\\\n  y &= x_1\n\\end{align*}k = 1,\\quad J = 100,\\quad b = 10,\n    \\quad r = 1\n\nIn the previous lab, we have linearized the system around y_d=75^\\circ to obtain A, B, C, D matrices. Now we will practice controller design and analysis for the linearized system, similar to the tasks in this lab.\n\nProgramming problem: Check the controllability of the linearized system using the controllability matrix and its rank.\n\nProgramming problem: Design a state-feedback controller to track the desired position y_\\text{d}=75^\\circ by placing the closed-loop eigenvalues at \\lambda_{1,2} = -10 \\pm 10i. Compute the state feedback gain K and the feedforward gain K_f for reference tracking.\n\nProgramming problem: Simulate and plot the response of the closed-loop system when \\theta starts at two different initial angles 70^\\circ and 50^\\circ. Compare the closed-loop response to the open-loop response (i.e., no control is applied).\n\n","type":"content","url":"/lab4-closedloopcontrol#problem-1-servo-mechanism-state-feedback-control","position":25},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl3":"Problem 2: Cart–Pole State Feedback Control","lvl2":"HW problems"},"type":"lvl3","url":"/lab4-closedloopcontrol#problem-2-cart-pole-state-feedback-control","position":26},{"hierarchy":{"lvl1":"Lab4: Closed-loop Control","lvl3":"Problem 2: Cart–Pole State Feedback Control","lvl2":"HW problems"},"content":"For this problem, we will consider the cart–pole system. The equations of motion for the system are given by\\ddot{x} = \\frac{1}{m_c + m_p \\sin^2\\theta}\n\\left[f_x + m_p \\sin\\theta\\big(l\\dot{\\theta}^2 + g\\cos\\theta\\big)\\right]\\ddot{\\theta} = \\frac{1}{l(m_c + m_p \\sin^2\\theta)}\n\\left[-f_x\\cos\\theta - m_p l\\dot{\\theta}^2\\cos\\theta\\sin\\theta\n- (m_c+m_p)g\\sin\\theta\\right]\n\nwhere\n\nx: cart position\n\n\\theta: pole angle measured from vertical (counter-clockwise positive)\n\ng: gravitational constant\n\nf_x: horizontal force input to the cart\n\nDefine the state vector, output, and control input as\\mathbf{x} = [\\,x,\\, \\dot{x},\\, \\theta,\\, \\dot{\\theta}\\,]^T,\n\\quad y = \\theta,\n\\quad u = f_x\n\nso the dynamics can be written as \\dot{x} = f(x, u),\\; y = g(x, u).\n\nUse the following parameters:m_c = 1.0,\\quad m_p = 0.1,\\quad l = 0.5,\\quad g = 9.81\n\nLast Lab we have linearized the system around the upward equilibrium \\theta_d = \\pi to get the A, B, C, D matrices.\n\nProgramming Problem: Check the controllability of the linearized system using the controllability matrix and its rank.\n\nProgramming Problem: Design a state-feedback controller to stabilize the pendulum at \\theta_d = \\pi by placing the closed-loop eigenvalues at the following locations:\n\n\\lambda_{1,2} = -1 \\pm 2i, \\lambda_{3,4} = -0.35 \\pm 0.35i\n\n\\lambda_{1,2} = -0.33 \\pm 0.66i, \\lambda_{3,4} = -0.18 \\pm 0.18i\n\nCompute the state feedback gain K and the feedforward gain K_f for reference tracking.\n\nProgramming Problem: Simulate and plot the response of the closed-loop system when the pendulum starts at \\theta = 0.9\\pi. Compare the closed-loop response to the open-loop response (i.e., no control is applied).","type":"content","url":"/lab4-closedloopcontrol#problem-2-cart-pole-state-feedback-control","position":27},{"hierarchy":{"lvl1":"Lab5: Second-Order Systems and Linear Quadratic Regulator (LQR)"},"type":"lvl1","url":"/lab5-lqr","position":0},{"hierarchy":{"lvl1":"Lab5: Second-Order Systems and Linear Quadratic Regulator (LQR)"},"content":"This lab will practice how to decide where to place the eigenvalues to desired performances. We will use two approaches, one rely on the standard second order system, another one based on the linear quadratic regulators (LQR). Specifically, we will\n\nAnalyze the effects of damping ratio and natural frequency on system response for a 2nd order system\n\nDesign state-feedback controllers to meet specific performance criteria (settling time, overshoot)\n\nImplement and tune LQR controllers, exploring the impact of weighting matrices\n\nSimulate and visualize open-loop and closed-loop system responses using the Python Control Systems Library\n\nBy completing this lab, you will develop practical skills in modern control design, including eigenvalue assignment, reference tracking, and performance verification for mechanical systems.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\ntry:\n  import control as ctl\n  print(\"python-control\", ctl.__version__)\nexcept ImportError:\n  !pip install control\n  import control as ctl\n\n\n\n","type":"content","url":"/lab5-lqr","position":1},{"hierarchy":{"lvl1":"Lab5: Second-Order Systems and Linear Quadratic Regulator (LQR)","lvl3":"Task 1: Second order system"},"type":"lvl3","url":"/lab5-lqr#task-1-second-order-system","position":2},{"hierarchy":{"lvl1":"Lab5: Second-Order Systems and Linear Quadratic Regulator (LQR)","lvl3":"Task 1: Second order system"},"content":"Consider the standard form of a second-order ordinary differential equation (ODE):\\frac{d^2q}{dt^2} + 2\\zeta\\omega_0 \\frac{dq}{dt} + \\omega_0^2 q = k \\omega_0^2 u(t)\n\nwhere:\n\nq is the output,\n\nu(t) is the input,\n\n\\omega_0 is the natural frequency,\n\n\\zeta is the damping ratio,\n\nk is the DC gain.\n\nTo convert this ODE into state-space form, define the state variables:\n\nx_1 = q\n\nx_2 = \\frac{dq}{dt}\n\nThen, the state-space representation is:\\begin{aligned}\n\\frac{d}{dt}\n\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 & 1 \\\\\n-\\omega_0^2 & -2\\zeta\\omega_0\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n0 \\\\\nk\\omega_0^2\n\\end{bmatrix}\nu(t) \\\\\ny &= \\begin{bmatrix} 1 & 0 \\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix}\n\\end{aligned}\n\nThis form is used for simulation and analysis of second-order systems in control engineering.\n\nWe first try to see how the damping ratio \\zeta will influecen the shape of the step response by fixing the natural frequency.\n\n# Parameters\nk = 1\nomega_0 = 2 * np.pi  # 1 Hz natural frequency to standard unit rad/s\n\n# Damping ratios for each case\nzeta_cases = {\n    \"Undamped\": 0.0,\n    \"Underdamped\": 0.2,\n    \"Critically damped\": 1.0,\n    \"Overdamped\": 2.0\n}\n\nt = np.linspace(0, 5, 500)\nplt.figure(figsize=(8, 5))\n\nfor label, zeta in zeta_cases.items():\n    A = np.array([[0, 1], [-omega_0**2, -2*zeta*omega_0]])\n    B = np.array([[0], [k * omega_0**2]])\n    C = np.array([[1, 0]])\n    D = np.array([[0]])\n    sys = ctl.ss(A, B, C, D)\n    t_out, y_out = ctl.step_response(sys, t)\n    plt.plot(t_out, y_out, label=f\"{label} ($\\\\zeta={zeta}$)\")\n\nplt.xlabel(\"Time [s]\")\nplt.ylabel(\"Step Response\")\nplt.title(\"Second Order System Step Response for Different Damping Ratios (State Space)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\nWe will then see how the natural frequency \\omega_0 influences the speed of response by fixing zeta=0.707\n\n# Illustrate how omega_0 influences the response by fixing zeta=0.707\nzeta = 0.707\nomega_0_values = [2 * np.pi * f for f in [0.5, 1, 2, 4]]  # 0.5Hz, 1Hz, 2Hz, 4Hz\n\nplt.figure(figsize=(8, 5))\nfor omega_0 in omega_0_values:\n    A = np.array([[0, 1], [-omega_0**2, -2*zeta*omega_0]])\n    B = np.array([[0], [k * omega_0**2]])\n    C = np.array([[1, 0]])\n    D = np.array([[0]])\n    sys = ctl.ss(A, B, C, D)\n    t_out, y_out = ctl.step_response(sys, t)\n    plt.plot(t_out, y_out, label=f\"$\\\\omega_0={omega_0:.2f}$ rad/s\")\n\nplt.xlabel(\"Time [s]\")\nplt.ylabel(\"Step Response\")\nplt.title(\"Effect of $\\omega_0$ on Second Order System Response ($\\\\zeta=0.707$)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n","type":"content","url":"/lab5-lqr#task-1-second-order-system","position":3},{"hierarchy":{"lvl1":"Lab5: Second-Order Systems and Linear Quadratic Regulator (LQR)","lvl3":"Task 2: Design state-feedback controller to achieve desired performance"},"type":"lvl3","url":"/lab5-lqr#task-2-design-state-feedback-controller-to-achieve-desired-performance","position":4},{"hierarchy":{"lvl1":"Lab5: Second-Order Systems and Linear Quadratic Regulator (LQR)","lvl3":"Task 2: Design state-feedback controller to achieve desired performance"},"content":"Now let’s use the mass-spring-damper system to illustrate how to design a state-feedback controller to achieve desired performance metrics. The mass-spring-damper system is governed by the second-order ODE:m\\ddot{q} + b\\dot{q} + kq = u(t)\n\nwhere m is the mass, b is the damping coefficient, k is the spring constant, and u(t) is the input force.\n\nAssuming m = 1, b = 1, and k = 1, the equation simplifies to:\\ddot{q} + \\dot{q} + q = u(t)\n\nThis can be written in state-space form as:\\dot{x} = Ax + Bu\n\nwhereA = \\begin{bmatrix} 0 & 1 \\\\ -1 & -1 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n\nIf we design a state feedback controller:u = -Kx = -\\begin{bmatrix} k_1 & k_2 \\end{bmatrix} x\n\nthen the closed-loop system becomes:A_{cl} = A-BK= \\begin{bmatrix} 0 & 1 \\\\ -1-k_1 & -1-k_2 \\end{bmatrix}\n\nWe want to achieve:\n\nSettling time T_s < 1 s\n\nOvershoot M_p < 5\\%\n\nThe standard second-order system relationships are:T_s \\approx \\frac{4}{\\zeta \\omega_0}\n\nM_p = e^{-\\frac{\\pi \\zeta}{\\sqrt{1-\\zeta^2}}}\n\nBelow is code to solve for the required \\zeta and \\omega_0 to meet these specifications:\n\nfrom scipy.optimize import fsolve\n\n# Desired specifications\nTs_max = 1      # Settling time < 1 s\nMp_max = 0.05   # Overshoot < 5%\n\n# Solve for minimum zeta from overshoot specification\ndef overshoot_eq(zeta):\n    return np.exp(-np.pi * zeta / np.sqrt(1 - zeta**2)) - Mp_max\n\nzeta_guess = 0.7\nzeta_req = fsolve(overshoot_eq, zeta_guess)[0]\n\n# Solve for minimum omega_0 from settling time specification\nomega_0_req = 4 / (zeta_req * Ts_max)\n\nprint(f\"Required zeta: {zeta_req:.3f}\")\nprint(f\"Required omega_0: {omega_0_req:.3f} rad/s\")\n\n# Verify the design\n# Compute state feedback gains k1 and k2 for desired omega_0 and zeta\n# k1 and k2 are chosen to place the closed-loop poles at the desired location\n# For a second-order system, the characteristic equation is:\n# s^2 + 2*zeta*omega_0*s + omega_0^2 = 0\n# State feedback places the eigenvalues at s^2 + (1 + k2)*s + (1 + k1) = 0\n# Matching coefficients gives:\nk1 = omega_0_req**2 - 1      # ensures desired natural frequency\nk2 = 2 * zeta_req * omega_0_req - 1  # ensures desired damping ratio\n\n# Form the closed-loop A matrix using state feedback\nA = np.array([[0, 1], [-1 - k1, -1 - k2]])\nB = np.array([[0], [1]])\nC = np.array([[1, 0]])\nD = np.array([[0]])\nsys = ctl.ss(A, B, C, D)\nt_out, y_out = ctl.step_response(sys, t)\nplt.figure(figsize=(8, 5))\nplt.plot(t_out, y_out, label=\"Designed System\")\nplt.xlabel(\"Time [s]\")\nplt.ylabel(\"Step Response\")\nplt.title(\"Step Response of Designed System\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nstep_info = ctl.step_info(sys)\nprint(\"Step Response Info:\")\nstep_info\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/lab5-lqr#task-2-design-state-feedback-controller-to-achieve-desired-performance","position":5},{"hierarchy":{"lvl1":"Lab5: Second-Order Systems and Linear Quadratic Regulator (LQR)","lvl3":"Task 3: Linear quadratic regulator (LQR) design"},"type":"lvl3","url":"/lab5-lqr#task-3-linear-quadratic-regulator-lqr-design","position":6},{"hierarchy":{"lvl1":"Lab5: Second-Order Systems and Linear Quadratic Regulator (LQR)","lvl3":"Task 3: Linear quadratic regulator (LQR) design"},"content":"Now we’ll design a controller based on LQR. The controller will have the formu=-Kx+k_rr\n\nFor the feedback control gain K, we’ll use linear quadratic regulator theory.  We seek to find the control law that minimizes the cost function:J(x(\\cdot), u(\\cdot)) = \\int_0^\\infty x^T(\\tau) Q_x x(\\tau) + u^T(\\tau) Q_u u(\\tau)\\, d\\tau\n\nThe weighting matrices Q_x\\succeq 0 \\in \\mathbb{R}^{n \\times n} and Q_u \\succ 0\\in \\mathbb{R}^{m \\times m} should be chosen based on the desired performance of the system (tradeoffs in state errors and input magnitudes).  See Example 3.5 in \n\nOptimization Based Control (OBC) for a discussion of how to choose these weights.  For now, we just choose identity weights for all states and inputs.\n\nFor the feedforward control gain k_r, we derive the feedforward gain from an equilibrium point analysis:y_e = C(A-BK)^{-1}Bk_rr\n\\qquad\\implies\\qquad k_r = \\frac{-1}{C(A-BK)^{-1}B}\n\n# Construct the mass-spring-damper system\nA = np.array([[0, 1], [-1, -1]])\nB = np.array([[0], [1]])\nC = np.array([[1, 0]])\nD = np.array([[0]])\nsys = ctl.ss(A, B, C, D)\n\n# Construct an LQR controller for the system\nrho = 1 # constant to balance state and input weights\nQx = np.eye(sys.nstates) # create an identity matrix with size equal to number of states\nQu = np.eye(sys.ninputs) # create an identity matrix with size equal to number of inputs\nQu = rho * Qu # scale the input weight matrix\nK, _, _ = ctl.lqr(sys, Qx, Qu) # compute the LQR gain matrix\nprint('K: '+str(K)) #str is to convert K to string for printing\n\n# Set the feedforward gain to track the reference\nkr = (-1 / (C @ np.linalg.inv(A - B @ K) @ B))\nprint('k_r: '+str(kr))\n\n\n\nNow that we have our gains designed, we can simulate the closed loop system:\\frac{dx}{dt} = A_{cl}x + B_{cl} r,\n\\quad A_{cl} = A-BK,\n\\quad B_{cl} = Bk_r\n\nNotice that, with a state feedback controller, the new (closed loop) dynamics matrix absorbs the old (open loop) “input” u, and the new (closed loop) input is our reference signal r.\n\n# Create a closed loop system\nA_cl = A - B @ K\nB_cl =  B * kr\nclsys = ctl.ss(A_cl, B_cl, C, 0)\nprint(clsys)\n\n\n\nWith the designed controller, we can now simulate the system to see its performance. We ask it to track a constant reference r :\n\n# Plot the step response with respect to the reference input\nr = 5\nTf = 8\ntvec = np.linspace(0, Tf, 100)\n\nU = r * np.ones_like(tvec)\ntime, output = ctl.forced_response(clsys, tvec, U)\n\n# Get step response info\ninfo = ctl.step_info(clsys)\nprint(f\"Settling time: {info['SettlingTime']:.3f} s\")\nprint(f\"Overshoot: {info['Overshoot']:.2f} %\")\nplt.plot(time, output)\nplt.plot([time[0], time[-1]], [r, r], '--');\nplt.legend(['y', 'r']);\nplt.ylabel(\"Output\")\nplt.xlabel(\"Time $t$ [sec]\")\nplt.title(\"Baseline controller step response\")\n\n\n\n\n\n\n\nThings to try:\n\nset k_r=0\n\nset k_r \\neq \\frac{-1}{C(A-BK)^{-1}B}\n\ntry different LQR weightings\n\n","type":"content","url":"/lab5-lqr#task-3-linear-quadratic-regulator-lqr-design","position":7},{"hierarchy":{"lvl1":"Lab5: Second-Order Systems and Linear Quadratic Regulator (LQR)","lvl2":"HW problems"},"type":"lvl2","url":"/lab5-lqr#hw-problems","position":8},{"hierarchy":{"lvl1":"Lab5: Second-Order Systems and Linear Quadratic Regulator (LQR)","lvl2":"HW problems"},"content":"","type":"content","url":"/lab5-lqr#hw-problems","position":9},{"hierarchy":{"lvl1":"Lab5: Second-Order Systems and Linear Quadratic Regulator (LQR)","lvl3":"Problem 1","lvl2":"HW problems"},"type":"lvl3","url":"/lab5-lqr#problem-1","position":10},{"hierarchy":{"lvl1":"Lab5: Second-Order Systems and Linear Quadratic Regulator (LQR)","lvl3":"Problem 1","lvl2":"HW problems"},"content":"For this problem, consider the following second-order system with the state-space representation:\\dot{x} = Ax + Bu, \\quad y = Cx\n\nwhereA = \\begin{bmatrix} 2 & 1 \\\\ 10 & -1 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}, \\quad C = \\begin{bmatrix} 1 & 0 \\end{bmatrix}, \\quad D = 0\n\nTasks:\n\nManual Calculation:      Design a state-feedback controller u = -Kx to achieve a closed-loop system with a damping ratio \\zeta = 1/\\sqrt{2} and natural frequency \\omega_0 = 2\\pi rad/s (1 Hz).   Derive the feedback gain K = [k_1, k_2] manually using the desired eigenvalue locations based on \\zeta and \\omega_0.\n\nProgramming problem:      Write a program to compute the feedback gain K for the above specifications and compare with your manual result.\n\nProgramming problem:    Design a state-feedback controller u = -Kx to achieve:\n\nSettling time T_s < 1 s\n\nOvershoot M_p < 5\\%\n\nSteps:\n\nCompute the required damping ratio \\zeta and natural frequency \\omega_0 using the standard second-order system formulas:\nT_s \\approx \\frac{4}{\\zeta \\omega_0}, \\qquad\nM_p = e^{-\\frac{\\pi \\zeta}{\\sqrt{1-\\zeta^2}}}\n\nDetermine the feedback gain K.\n\nSimulate the step response of the closed-loop system and verify that the specifications are met.\n\nProgramming problem:  Design a LQR controller for the system:\n\nChoose Q_x and Q_u as identity matrices, but use a ratio \\rho to balance Q_x and Q_u (i.e., Q_u = \\rho I).\n\nExplore how different values of \\rho influence the controller performance and explain your observations.\n\n","type":"content","url":"/lab5-lqr#problem-1","position":11},{"hierarchy":{"lvl1":"Lab5: Second-Order Systems and Linear Quadratic Regulator (LQR)","lvl3":"Problem 2","lvl2":"HW problems"},"type":"lvl3","url":"/lab5-lqr#problem-2","position":12},{"hierarchy":{"lvl1":"Lab5: Second-Order Systems and Linear Quadratic Regulator (LQR)","lvl3":"Problem 2","lvl2":"HW problems"},"content":"This problem requires manual derivation. You may attach a PDF.\n\nAssume a system has the following state–space representation\n$$\n\\dot{x} =\\begin{bmatrix}\n0 & 1\\\\\n0 & -a\n\\end{bmatrix}\\begin{bmatrix}\n0\\\\\n1\n\\end{bmatrix}\n\n\\qquad\ny = \\begin{bmatrix} b & 1 \\end{bmatrix} xand we want to minimize the following objective\n\nJ=\\int_{0}^{\\infty} \\big(x_1^2 + u^2\\big), d\\tau = \\int_0^\\infty x^T(\\tau) Q_x x(\\tau) + u^T(\\tau) Q_u u(\\tau), d\\tau\n$$\n\nWhat is the numerical values of Q_x and Q_u?\n\nLetP=\\begin{bmatrix}\n p_{11} & p_{12}\\\\\n p_{21} & p_{22}\n \\end{bmatrix},\n\nwith p_{12}=p_{21} and P>0 (positive definite). Write the steady–state Riccati equation PA + A^TP - PBR^{-1}B^TP + Q = 0 as a system of four explicit equations in terms of the elements of P and the constants a and b. Solve p_{11}, p_{12}, p_{22} as a function of a and b.\n\nFind the gains for the optimal controller u=-Kx assuming the full state is available for feedback.","type":"content","url":"/lab5-lqr#problem-2","position":13},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking"},"type":"lvl1","url":"/lab6-trajectorytracking","position":0},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking"},"content":"In this lab, we will explore trajectory tracking control using a nonlinear vehicle steering system. The objective is to design controllers that enable the vehicle to follow a desired trajectory, focusing on both constant gain and gain-scheduled state feedback approaches.\n\nThe lab is organized as follows:\n\nTask 1: Design a state feedback controller with a constant gain, computed around a nominal forward speed. Analyze the closed-loop performance for different speeds and observe the limitations of using a fixed gain for trajectory tracking.\n\nTask 2: Develop a gain-scheduled controller by linearizing the system at multiple operating points (different speeds and headings). Interpolate the feedback gains based on the current operating condition to improve tracking performance across a range of speeds.\n\nWe will use the following controller structure:u = u_\\text{d} - K(\\mu) (x - x_\\text{d}),\n\nwhere x_\\text{d}, u_\\text{d} are the desired state and input trajectories, and \\mu represents the scheduling variables (such as speed and heading).\n\nBy the end of this lab, you will understand the benefits and challenges of both constant gain and gain-scheduled controllers for nonlinear trajectory tracking.\n\n# Import the packages needed for the examples included in this notebook\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\nfrom cmath import sqrt\nfrom math import pi\ntry:\n  import control as ctl\n  print(\"python-control\", ctl.__version__)\nexcept ImportError:\n  !pip install control\n  import control as ctl\nimport control.optimal as opt\n\n\n\n\n","type":"content","url":"/lab6-trajectorytracking","position":1},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking","lvl2":"Vehicle Steering Dynamics"},"type":"lvl2","url":"/lab6-trajectorytracking#vehicle-steering-dynamics","position":2},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking","lvl2":"Vehicle Steering Dynamics"},"content":"We consider a simplified vehicle as follows:\n\nThe vehicle dynamics can modeled as:\\begin{aligned}\n  \\dot x &= \\cos\\theta\\, v \\\\\n  \\dot y &= \\sin\\theta\\, v \\\\\n  \\dot\\theta &= \\frac{v}{l} \\tan \\delta\n\\end{aligned}\n\nWe take the state of the system as (x, y, \\theta) where (x, y) is the position of the vehicle in the plane and \\theta is the angle of the vehicle with respect to horizontal.  The vehicle input is given by (v, \\delta) where v is the forward velocity of the vehicle and \\delta is the angle of the steering wheel.  The model includes saturation of the vehicle steering angle.\n\nWe first create the dynamics model for vehicle steering:\n\n\n# Function to compute the RHS of the system dynamics\ndef kincar_update(t, x, u, params):\n    # Get the parameters for the model\n    l = params['wheelbase']             # vehicle wheelbase\n    deltamax = params['maxsteer']       # max steering angle (rad)\n\n    # Saturate the steering input,\n    # the steering angle will be between -deltamax and deltamax\n    delta = np.clip(u[1], -deltamax, deltamax)\n\n    # Return the derivative of the state\n    return np.array([\n        np.cos(x[2]) * u[0],            # xdot = cos(theta) v\n        np.sin(x[2]) * u[0],            # ydot = sin(theta) v\n        (u[0] / l) * np.tan(delta)      # thdot = v/l tan(delta)\n    ])\n\nkincar_params={'wheelbase': 3, 'maxsteer': 0.5}\n\n# Create a nonlinear input/output system\nkincar = ctl.nlsys(\n    kincar_update, None, name=\"kincar\", params=kincar_params,\n    inputs=('v', 'delta'), outputs=('x', 'y', 'theta'),\n    states=('x', 'y', 'theta'))\n\n\n\nWe then create a function that will be used later for plotting the lane change manuever.\n\n# Utility function to plot lane change manuever\n# t: time vector\n# y: state trajectory\ndef plot_lanechange(t, y, u, figure=None, yf=None, label=None):\n    # Plot the xy trajectory\n    plt.subplot(3, 1, 1, label='xy')\n    plt.plot(y[0], y[1], label=label)\n    plt.xlabel(\"x [m]\")\n    plt.ylabel(\"y [m]\")\n    if yf is not None: # if final point is given, plot it\n        plt.plot(yf[0], yf[1], 'ro')\n\n    # Plot x and y as functions of time\n    plt.subplot(3, 2, 3, label='x')\n    plt.plot(t, y[0])\n    plt.ylabel(\"$x$ [m]\")\n\n    plt.subplot(3, 2, 4, label='y')\n    plt.plot(t, y[1])\n    plt.ylabel(\"$y$ [m]\")\n\n    # Plot the inputs as a function of time\n    plt.subplot(3, 2, 5, label='v')\n    plt.plot(t, u[0])\n    plt.xlabel(\"Time $t$ [sec]\")\n    plt.ylabel(\"$v$ [m/s]\")\n\n    plt.subplot(3, 2, 6, label='delta')\n    plt.plot(t, u[1])\n    plt.xlabel(\"Time $t$ [sec]\")\n    plt.ylabel(\"$\\\\delta$ [rad]\")\n\n    plt.subplot(3, 1, 1)\n    plt.title(\"Lane change manuever\")\n    if label:\n        plt.legend()\n    plt.tight_layout()\n\n\n\n","type":"content","url":"/lab6-trajectorytracking#vehicle-steering-dynamics","position":3},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking","lvl2":"Task 1: State feedback controller with a constant gain"},"type":"lvl2","url":"/lab6-trajectorytracking#task-1-state-feedback-controller-with-a-constant-gain","position":4},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking","lvl2":"Task 1: State feedback controller with a constant gain"},"content":"We start by designing a state feedback controller that can be used to stabilize the system.  We design the controller around a nominal forward speed of 10 m/s, but we apply this to the vehicle at different speeds.\n\n# Compute the linearization of the dynamics at a nominal point\n\n# nominal position and heading\nx_nom = np.array([0, 0, 0])\n# nominal velocity and 0 steering angle, v can be any constant as long as delta=0, why?\nu_nom = np.array([10, 0])\n# Linearize around this operating point\nP = ctl.linearize(kincar, x_nom, u_nom)\nprint(P)\n\nQx = np.diag([1, 10, 0.1]) # we care about y position more than x position, which is more than theta\nQu = np.diag([1, 1]) # we care equally about v and delta inputs\nK, _, _ = ctl.lqr(P.A, P.B, Qx, Qu)\nprint(K)\n\n\n\nWe will then create the closed loop system using the control.create_statefbk_iosystem function, which constructs a closed-loop nonlinear input/output system using state feedback. This function allows you to specify feedback gains and scheduling variables for gain-scheduled controllers. For more details, see the \n\npython-control documentation.\n\n# Create the closed loop system using create_statefbk_iosystem\n# ctrl is the controller system, clsys is the closed loop system\nctrl, clsys = ctl.create_statefbk_iosystem(\n    kincar, K,\n    xd_labels=['xd', 'yd', 'thetad'],\n    ud_labels=['vd', 'deltad'])\nprint(clsys)\n\n\n\n","type":"content","url":"/lab6-trajectorytracking#task-1-state-feedback-controller-with-a-constant-gain","position":5},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking","lvl3":"Reference trajectory subsystem","lvl2":"Task 1: State feedback controller with a constant gain"},"type":"lvl3","url":"/lab6-trajectorytracking#reference-trajectory-subsystem","position":6},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking","lvl3":"Reference trajectory subsystem","lvl2":"Task 1: State feedback controller with a constant gain"},"content":"We can create x_\\text{d}(t) and u_\\text{d}(t) corresponding to desired speed (v_{ref}) and lateral position (y_{ref}).\nThe reference trajectory block below generates a simple trajectory for the system given v_{ref} and y_{ref}.  The trajectory consists of a straight line of the formx_\\text{d}(t)=[v_{ref} t, \\quad y_{ref}, \\quad0]^T\n\nwith nominal inputu_\\text{d}(t)\n=[v_{ref},\\quad 0]^T\n\n# System state: none\n# System input: vref, yref\n# System output: xd, yd, thetad, vd, deltad\n# System parameters: none\n\ndef trajgen_output(t, x, u, params):\n    vref, yref = u\n    return np.array([vref * t, yref, 0, vref, 0])\n\n# Define the trajectory generator as an input/output system with no parameters or states\ntrajgen = ctl.nlsys(\n    None, trajgen_output, name='trajgen',\n    inputs=('vref', 'yref'),\n    outputs=('xd', 'yd', 'thetad', 'vd', 'deltad'))\n\nprint(trajgen)\n\n\n\n","type":"content","url":"/lab6-trajectorytracking#reference-trajectory-subsystem","position":7},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking","lvl3":"Step responses","lvl2":"Task 1: State feedback controller with a constant gain"},"type":"lvl3","url":"/lab6-trajectorytracking#step-responses","position":8},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking","lvl3":"Step responses","lvl2":"Task 1: State feedback controller with a constant gain"},"content":"To explore the dynamics of the system, we create a set of lane changes at different forward speeds.  Since the linearization depends on the speed, this means that the closed loop performance of the system will vary.\n\nsteering_fixed = ctl.interconnect(\n    [kincar, ctrl, trajgen],\n    inputs=['vref', 'yref'],\n    #specify the outputs of the interconnected system to be the kincar outputs and inputs\n    outputs=kincar.output_labels + kincar.input_labels\n)\nprint(steering_fixed)\n\n\n\n# Set up the simulation conditions\nyref = 15\nT = np.linspace(0, 5, 100)\n\n# Do an iteration through different speeds\nfor vref in [2, 5, 20]:\n    # Simulate the closed loop controller response\n    tout, yout = ctl.input_output_response(\n        steering_fixed, T, [vref * np.ones(len(T)), yref * np.ones(len(T))],\n        X0=[0, 0, 0], params={'maxsteer': 0.5})\n\n    # Plot the results\n    plot_lanechange(tout, yout, yout[3:])\n\n# Label the different curves\nplt.subplot(3, 1, 1)\nplt.legend([\"$v_d$ = \" + f\"{vref}\" for vref in [2, 10, 20]])\nplt.tight_layout()\n\n\n\nTry to change the values of y_{ref}, you will see that the constant gain values will not be able to track the reference. Why is this?\n\n","type":"content","url":"/lab6-trajectorytracking#step-responses","position":9},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking","lvl2":"Task 2: Gain scheduled controller"},"type":"lvl2","url":"/lab6-trajectorytracking#task-2-gain-scheduled-controller","position":10},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking","lvl2":"Task 2: Gain scheduled controller"},"content":"Now let’s design a gain scheduled controller for the system. We first linearize the system about the desired trajectory to obtain\\begin{aligned}\n    A(x_\\text{d}) &= \\left. \\frac{\\partial f}{\\partial x} \\right|_{(x_\\text{d}, u_\\text{d})}\n      = \\begin{bmatrix}\n          0 & 0 & -\\sin\\theta_\\text{d}\\, v_\\text{d} \\\\ 0 & 0 & \\cos\\theta_\\text{d}\\, v_\\text{d} \\\\ 0 & 0 & 0\n        \\end{bmatrix}. \\\\\n    B(x_\\text{d}) &= \\left. \\frac{\\partial f}{\\partial u} \\right|_{(x_\\text{d}, u_\\text{d})}\n     = \\begin{bmatrix}\n       1 & 0 \\\\ 0 & 0 \\\\ 0 & v_\\text{d}/l\n       \\end{bmatrix}.\n  \\end{aligned}\n\nwherex_\\text{d}(t) = \\begin{bmatrix} x_\\text{d} \\\\ y_\\text{d} \\\\ \\theta_\\text{d} \\end{bmatrix}, \\qquad\nu_\\text{d}(t) = \\begin{bmatrix} v_\\text{d} \\\\ \\delta_\\text{d} \\end{bmatrix}\n\nWe see that these matrices depend only on \\theta_\\text{d} and v_\\text{d}, so we choose these as the scheduling variables and design a controller of the formu = u_\\text{d} - K(\\mu) (x - x_\\text{d})\n\nwhere \\mu = (\\theta_\\text{d}, v_\\text{d}) and we interpolate the gains based on LQR controllers computed at a fixed set of points \\mu_i.\n\n# Define the points for the scheduling variables\ngs_speeds = [2, 10, 20]\ngs_angles = np.linspace(-pi, pi, 4) # generate 4 angles from -pi to pi for theta\n\n# generate all combinations of speed and angle, how many combinations?\npoints = [np.array([speed, angle])\n          for speed in gs_speeds\n          for angle in gs_angles]\n# Create controllers at each scheduling point\n# Since the linearization depends on only speed and angle, we assume other states are zero\ngains = [np.array(ctl.lqr(kincar.linearize([0, 0, angle], [speed, 0]), Qx, Qu)[0])\n    for speed in gs_speeds\n    for angle in gs_angles]\nprint(f\"{points=}\")\nprint(f\"{gains=}\")\n\n\n# Create the gain scheduled system\n# gainsched_method='linear' creates a linear interpolation of the gains between the points\nctrl_gs, _ = ctl.create_statefbk_iosystem(\n    kincar, (gains, points), name='controller',\n    xd_labels=['xd', 'yd', 'thetad'], ud_labels=['vd', 'deltad'],\n    gainsched_indices=['vd', 'theta'], gainsched_method='linear')\nprint(ctrl_gs)\n\n\n\n","type":"content","url":"/lab6-trajectorytracking#task-2-gain-scheduled-controller","position":11},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking","lvl3":"System construction","lvl2":"Task 2: Gain scheduled controller"},"type":"lvl3","url":"/lab6-trajectorytracking#system-construction","position":12},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking","lvl3":"System construction","lvl2":"Task 2: Gain scheduled controller"},"content":"The input to the full closed loop system is the desired lateral position and the desired forward velocity.  The output for the system is taken as the full vehicle state plus the velocity of the vehicle.\n\nWe construct the system using the ct.interconnect function and use signal labels to keep track of everything.\n\nsteering_gainsched = ctl.interconnect(\n    [trajgen, ctrl_gs, kincar], name='steering',\n    inputs=['vref', 'yref'],\n    outputs=kincar.output_labels + kincar.input_labels\n)\nprint(steering_gainsched)\n\n\n\n","type":"content","url":"/lab6-trajectorytracking#system-construction","position":13},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking","lvl3":"System simulation","lvl2":"Task 2: Gain scheduled controller"},"type":"lvl3","url":"/lab6-trajectorytracking#system-simulation","position":14},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking","lvl3":"System simulation","lvl2":"Task 2: Gain scheduled controller"},"content":"We now simulate the gain scheduled controller for a step input in the y position, using a range of vehicle speeds v_\\text{d}:\n\n# Plot the reference trajectory for the y position\n# plt.plot([0, 5], [yref, yref], 'k-', linewidth=0.6)\n\n# Find the signals we want to plot\ny_index = steering_gainsched.find_output('y')\nv_index = steering_gainsched.find_output('v')\n\n# Do an iteration through different speeds\nfor vref in [2, 5, 20]:\n    # Simulate the closed loop controller response\n    tout, yout = ctl.input_output_response(\n        steering_gainsched, T, [vref * np.ones(len(T)), yref * np.ones(len(T))],\n        X0=[0, 0, 0], params={'maxsteer': 1}\n    )\n\n    # Plot the results\n    plot_lanechange(tout, yout, yout[3:])\n\n# Label the different curves\nplt.subplot(3, 1, 1)\nplt.legend([\"$v_d$ = \" + f\"{vref}\" for vref in [2, 10, 20]])\nplt.tight_layout()\n\n\n\n","type":"content","url":"/lab6-trajectorytracking#system-simulation","position":15},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking","lvl2":"HW problem"},"type":"lvl2","url":"/lab6-trajectorytracking#hw-problem","position":16},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking","lvl2":"HW problem"},"content":"","type":"content","url":"/lab6-trajectorytracking#hw-problem","position":17},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking","lvl3":"Problem 1: Constant Gain Controller Variation Study","lvl2":"HW problem"},"type":"lvl3","url":"/lab6-trajectorytracking#problem-1-constant-gain-controller-variation-study","position":18},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking","lvl3":"Problem 1: Constant Gain Controller Variation Study","lvl2":"HW problem"},"content":"In this problem, we will try to vary the vref, yref , and linearization point to investigate the performance of the constant gain controller.\n\nFix the forward speed vref (e.g., vref = 10) and vary the lateral reference yref (e.g., yref = 5, yref = 40, yref = -10).  Plot the vehicle trajectory and compare the tracking performance using the constant gain controller.\n\nFix the lateral reference yref (e.g., yref = 10) and vary the forward speed vref (e.g., vref = 8, vref = 15).  Plot the vehicle trajectory and compare the tracking performance using the constant gain controller.\n\nTry changing the linearization point for the controller gain computation:\n\nx_nom = np.array([0, 0, 0]), u_nom = np.array([5, 0]), simulate the system for (1) yref=5 and vref=5; (2) yref=5 and vref=20\n\nx_nom = np.array([0, 0, -np.pi/4]), u_nom = np.array([20, 0]), simulate the system for (1) yref=5 and vref=5; (2) yref=5 and vref=20\n\nExplain the results you have obtained in the previous three cases, especially why the linearization point matters for nonlinear systems.","type":"content","url":"/lab6-trajectorytracking#problem-1-constant-gain-controller-variation-study","position":19},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking","lvl3":"Problem 2. Gain-Scheduled Controller Comparison:","lvl2":"HW problem"},"type":"lvl3","url":"/lab6-trajectorytracking#problem-2-gain-scheduled-controller-comparison","position":20},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking","lvl3":"Problem 2. Gain-Scheduled Controller Comparison:","lvl2":"HW problem"},"content":"Repeat the above experiments using the gain-scheduled controller (see cell 21).\n\nCompare the tracking results for different yref and vref values.\n\nExplain why the gain-scheduled controller performs better across a wider range of operating conditions.","type":"content","url":"/lab6-trajectorytracking#problem-2-gain-scheduled-controller-comparison","position":21},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking","lvl3":"Problem 3: Implement an analytical gain controller","lvl2":"HW problem"},"type":"lvl3","url":"/lab6-trajectorytracking#problem-3-implement-an-analytical-gain-controller","position":22},{"hierarchy":{"lvl1":"Lab6: Trajectory Tracking","lvl3":"Problem 3: Implement an analytical gain controller","lvl2":"HW problem"},"content":"In lectures, we derived an analytical gain-scheduled controller, where the feedback gain K is a continuous function of the reference speed v_r. This approach is different from the gain-scheduled controller implemented in this lab, which uses interpolation between discrete LQR gains.\n\nThe analytical gain matrix is given by:K(x_d, u_d) =\n    \\begin{bmatrix}\n    \\lambda_1 & 0 & 0 \\\\\n    0 & \\dfrac{a_2 l}{v_r^2} & \\dfrac{a_1 l}{v_r}\n    \\end{bmatrix}\n\nTasks:\n\nImplement this analytical gain-scheduled controller in your simulation, using v_r as the reference speed (you may use l = 3 as in the lab).\n\nChoose and justify reasonable values for \\lambda_1, a_1, and a_2.\n\nSimulate the closed-loop system using this controller for several values of v_{ref} and y_{ref}.\n\nCompare the tracking performance of this controller with the gain-scheduled controller from Task 2 (which uses interpolated LQR gains).","type":"content","url":"/lab6-trajectorytracking#problem-3-implement-an-analytical-gain-controller","position":23},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation"},"type":"lvl1","url":"/lab7-trajectory-generation","position":0},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation"},"content":"In this lab, we will learn how to generate optimal trajectories for dynamic systems using numerical optimization. The lab is organized into several tasks:\n\nTask 1: Generate optimal trajectories for a double integrator system using both a convex optimization library (CVXPY) and the Python control library. Compare the results and explore how changing parameters affects the solution.\n\nTask 2: Formulate and solve optimal control problems for a kinematic car model. Investigate different cost functions, input and terminal constraints, and analyze how these choices influence the resulting trajectories.\n\nThroughout the lab, you will experiment with problem setup, solver options, and initial guesses to understand their impact on optimal control solutions.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\ntry:\n  import cvxpy as cp\n  print(\"cvxpy\", cp.__version__)\nexcept ImportError:\n  !pip install cvxpy\ntry:\n  import control as ctl\n  print(\"python-control\", ctl.__version__)\nexcept ImportError:\n  !pip install control\n  import control as ctl\nimport control.optimal as opt\n\n\n\n","type":"content","url":"/lab7-trajectory-generation","position":1},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl2":"Task 1: Double Integrator Optimal Trajectory Generation"},"type":"lvl2","url":"/lab7-trajectory-generation#task-1-double-integrator-optimal-trajectory-generation","position":2},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl2":"Task 1: Double Integrator Optimal Trajectory Generation"},"content":"In this task, we try to generate optimal trajectories by reformulating the problme into a numerical optimization problem. We will use two approaches. The first approach will directly use an optimization libarary, while the second will use the Python control libarary.\n\n","type":"content","url":"/lab7-trajectory-generation#task-1-double-integrator-optimal-trajectory-generation","position":3},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl3":"Approach 1: Using Convex Optimization Libarary","lvl2":"Task 1: Double Integrator Optimal Trajectory Generation"},"type":"lvl3","url":"/lab7-trajectory-generation#approach-1-using-convex-optimization-libarary","position":4},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl3":"Approach 1: Using Convex Optimization Libarary","lvl2":"Task 1: Double Integrator Optimal Trajectory Generation"},"content":"We first use the CVXPY library to solve the optimization problem. CVXPY is an open-source Python-embedded modeling language for convex optimization problems, allowing users to specify and solve a wide range of convex programs easily. For more information, see the \n\nCVXPY documentation.\n\nTf = 5  # final time\ndt = 0.1  # time step\ntimepts = np.linspace(0, Tf, int(Tf/dt), endpoint=True)\nN = len(timepts)  # number of time steps\nA = np.eye(2) + dt * np.array([[0, 1], [0, 0]])\nB = dt * np.array([[0], [1]])\n\nx = cp.Variable((2, N))  # decision variable: states [position; velocity]\nu = cp.Variable((1, N-1))  # decision variable: control input [acceleration]\n\nconstraints = [x[:, 0] == [2, 1]]  # initial condition, x[:, 0] is the first column of x, i.e., x at time 0\n# dynamics and input constraints: \"+=\" means append to the list\nfor n in range(N-1):\n    constraints += [x[:, n+1] == A @ x[:, n] + B @ u[:, n]]  # dynamics constraint\n    constraints += [cp.abs(u[:, n]) <= 1]  # input constraint\n# The final state is at index N-1 because of 0-based indexing (indices are 0, 1, ..., N-1)\nconstraints += [x[:, N-1] == [0, 0]]  # final condition\n\n# objective: minimize control effort\n#cost = cp.sum_squares(u)\ncost = 100*cp.sum_squares(u) + 1*cp.sum_squares(x) # alternative cost with state penalty\n# formulate and solve the problem\nprob = cp.Problem(cp.Minimize(cost), constraints)\nprob.solve()\n\nx_sol = x.value  # optimal state trajectory\nu_sol = u.value  # optimal control inputs\n\n# Plot the results\nplt.figure(figsize=(8,4))\nplt.subplot(2,1,1)\nplt.plot(timepts, x_sol[0, :], label='Position')\nplt.plot(timepts, x_sol[1, :], label='Velocity')\nplt.xlabel('Time [s]')\nplt.ylabel('State')\nplt.legend()\n\nplt.subplot(2,1,2)\nplt.plot(timepts[:-1], u_sol[0, :], label='Control input')\nplt.xlabel('Time [s]')\nplt.ylabel('u')\nplt.tight_layout()\nplt.show()\n\n\n\nTry to play with the following to see what will happen:\n\nchange the value of Tf\n\nchange the time step dt\n\nchange the initial condition x[:, 0]\n\n","type":"content","url":"/lab7-trajectory-generation#approach-1-using-convex-optimization-libarary","position":5},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl3":"Approach 2: Using Python Control Libarary","lvl2":"Task 1: Double Integrator Optimal Trajectory Generation"},"type":"lvl3","url":"/lab7-trajectory-generation#approach-2-using-python-control-libarary","position":6},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl3":"Approach 2: Using Python Control Libarary","lvl2":"Task 1: Double Integrator Optimal Trajectory Generation"},"content":"We will then generate the trajectory directly using Python control Libarary. The main function used in this approach is solve_ocp, which solves the optimal control problem for a given system. See the \n\npython-control optimal control documentation for a detailed description of this function.\n\nThe required and optional input parameters are:\n\nRequired:\n\nsystem: The system to optimize (e.g., state-space or nonlinear system).\n\ntimepts: Array of time points for discretization.\n\nx0: Initial state.\n\ncost: Cost function to minimize (e.g., quadratic cost).\n\nOptional:\n\nconstraints: List of trajectory/input constraints (e.g., input limits).\n\nterminal_constraints: List of constraints on the final state.\n\nterminal_cost: Additional cost on the final state.\n\ninitial_guess: Initial guess for state and input trajectories.\n\nminimize_method: Optimization solver to use (e.g., 'SLSQP', 'trust-constr').\n\nminimize_options: Dictionary of options for the solver.\n\ntrajectory_method, solve_ivp_method, solve_ivp_kwargs: Advanced options for trajectory integration and solver configuration.\n\nThis function returns an object containing the optimal state and input trajectories, which can be plotted and analyzed.\n\n# Set up the cost functions for the double integrator\nQx = np.diag([0, 0])  # penalize position and velocity error\nQu = np.array([[1]])  # penalize control effort\n\n# Define initial and final states\nx0_di = np.array([2, 1])\nxf_di = np.array([0, 0])\nuf_di = np.array([0])   # desired final input\n\n# Create a state-space system for the double integrator\nA_di = np.array([[0, 1], [0, 0]])\nB_di = np.array([[0], [1]])\nC_di = np.eye(2)  # output both position and velocity\nD_di = np.zeros((2,1)) # no direct feedthrough\nsys_di = ctl.ss(A_di, B_di, C_di, D_di)\n\n# Quadratic cost function\nquad_cost_di = opt.quadratic_cost(sys_di, Qx, Qu, x0=xf_di, u0=uf_di)\n\n#add constraints for the control input\nu_min = np.array([-1])\nu_max = np.array([1])\nconstraints = [ opt.input_range_constraint(sys_di, u_min, u_max) ] # input limits\n# Define lower and upper bounds for the final state\nxf_lb = np.array([0, 0])  # lower bound for final state\nxf_up = np.array([0, 0])  # upper bound for final state\n# The terminal constraint enforces that the final state must be exactly [0, 0]\nterminal = [ opt.state_range_constraint(sys_di, xf_lb, xf_up) ]\n\n# Solve the optimal control problem\nresult_di = opt.solve_ocp(\n    sys_di, timepts, x0_di, quad_cost_di,\n    constraints=constraints,\n    terminal_constraints=terminal,\n    # initial_guess=initial_guess_di,\n    # minimize_method='SLSQP',\n    # minimize_options={'ftol': 1e-4, 'eps': 0.01}\n)\n\n# Plot the results\nplt.figure(figsize=(8,4))\nplt.subplot(2,1,1)\nplt.plot(result_di.time, result_di.states[0], label='Position')\nplt.plot(result_di.time, result_di.states[1], label='Velocity')\nplt.xlabel('Time [s]')\nplt.ylabel('State')\nplt.legend()\n\nplt.subplot(2,1,2)\nplt.plot(result_di.time, result_di.inputs[0], label='Control input')\nplt.xlabel('Time [s]')\nplt.ylabel('u')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nCompare the results generate by the two approaches. Are they the same?\n\n","type":"content","url":"/lab7-trajectory-generation#approach-2-using-python-control-libarary","position":7},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl2":"Task 2: Kinematic Car: Optimal trajectory generation"},"type":"lvl2","url":"/lab7-trajectory-generation#task-2-kinematic-car-optimal-trajectory-generation","position":8},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl2":"Task 2: Kinematic Car: Optimal trajectory generation"},"content":"","type":"content","url":"/lab7-trajectory-generation#task-2-kinematic-car-optimal-trajectory-generation","position":9},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl3":"Vehicle steering dynamics","lvl2":"Task 2: Kinematic Car: Optimal trajectory generation"},"type":"lvl3","url":"/lab7-trajectory-generation#vehicle-steering-dynamics","position":10},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl3":"Vehicle steering dynamics","lvl2":"Task 2: Kinematic Car: Optimal trajectory generation"},"content":"\n\nThe vehicle dynamics are given by a simple bicycle model.  We take the state of the system as (x, y, \\theta) where (x, y) is the position of the vehicle in the plane and \\theta is the angle of the vehicle with respect to horizontal.  The vehicle input is given by (v, \\delta) where v is the forward velocity of the vehicle and \\delta is the angle of the steering wheel.  The model includes saturation of the vehicle steering angle.\\begin{aligned}\n  \\dot x &= \\cos\\theta\\, v \\\\\n  \\dot y &= \\sin\\theta\\, v \\\\\n  \\dot\\theta &= \\frac{v}{l} \\tan \\delta\n\\end{aligned}\n\n# Code to model vehicle steering dynamics\n\n# Function to compute the RHS of the system dynamics\ndef kincar_update(t, x, u, params):\n    # Get the parameters for the model\n    l = params['wheelbase']             # vehicle wheelbase\n    deltamax = params['maxsteer']         # max steering angle (rad)\n\n    # Saturate the steering input\n    delta = np.clip(u[1], -deltamax, deltamax)\n\n    # Return the derivative of the state\n    return np.array([\n        np.cos(x[2]) * u[0],            # xdot = cos(theta) v\n        np.sin(x[2]) * u[0],            # ydot = sin(theta) v\n        (u[0] / l) * np.tan(delta)      # thdot = v/l tan(delta)\n    ])\n\nkincar_params={'wheelbase': 3, 'maxsteer': 0.5}\n\n# Create nonlinear input/output system\nkincar = ctl.nlsys(\n    kincar_update, None, name=\"kincar\", params=kincar_params,\n    inputs=('v', 'delta'), outputs=('x', 'y', 'theta'),\n    states=('x', 'y', 'theta'))\n\n\n\n# Utility function to plot lane change manuever\ndef plot_lanechange(t, y, u, figure=None, yf=None, label=None):\n    # Plot the xy trajectory\n    plt.subplot(3, 1, 1, label='xy')\n    plt.plot(y[0], y[1], label=label)\n    plt.xlabel(\"x [m]\")\n    plt.ylabel(\"y [m]\")\n    if yf is not None:\n        plt.plot(yf[0], yf[1], 'ro')\n\n    # Plot x and y as functions of time\n    plt.subplot(3, 2, 3, label='x')\n    plt.plot(t, y[0])\n    plt.ylabel(\"$x$ [m]\")\n\n    plt.subplot(3, 2, 4, label='y')\n    plt.plot(t, y[1])\n    plt.ylabel(\"$y$ [m]\")\n\n    # Plot the inputs as a function of time\n    plt.subplot(3, 2, 5, label='v')\n    plt.plot(t, u[0])\n    plt.xlabel(\"Time $t$ [sec]\")\n    plt.ylabel(\"$v$ [m/s]\")\n\n    plt.subplot(3, 2, 6, label='delta')\n    plt.plot(t, u[1])\n    plt.xlabel(\"Time $t$ [sec]\")\n    plt.ylabel(\"$\\\\delta$ [rad]\")\n\n    plt.subplot(3, 1, 1)\n    plt.title(\"Lane change manuever\")\n    if label:\n        plt.legend()\n    plt.tight_layout()\n\n\n\nThe general problem we are solving is of the form:\\min_{u(\\cdot)}\n  \\int_0^T L(x,u)\\, dt + V \\bigl( x(T) \\bigr)\n\nsubject to\\dot x = f(x, u), \\qquad x\\in \\mathcal{X} \\subset \\mathbb{R}^n,\\, u\\in \\mathcal{U} \\subset \\mathbb{R}^m\n\nWe consider the problem of changing from one lane to another over a perod of 10 seconds while driving at a forward speed of 10 m/s.\n\n# Initial and final conditions\nx0 = np.array([  0., -2., 0.]); u0 = np.array([10., 0.])\nxf = np.array([100.,  2., 0.]); uf = np.array([10., 0.])\nTf = 10.0  # total maneuver time\n\n\n\nAn important part of the optimization procedure is to give a good initial guess so that the algorithm can start from this guess to find the solution.  A good initial guess increases the likelihood that the optimizer will converge to a desirable solution and helps avoid getting stuck in a local minimum. Here are some possibilities:\n\n# Define the time horizon (and spacing) for the optimization\n# timepts = np.linspace(0, Tf, 5, endpoint=True)    # Try using this and see what happens\n# timepts = np.linspace(0, Tf, 10, endpoint=True)   # Try using this and see what happens\ntimepts = np.linspace(0, Tf, 20, endpoint=True)\n\n# Compute some initial guesses to use\n# Note: bend_left only specifies the control inputs as an initial guess (not the states).\n# The solver will have to compute the corresponding states that result from those inputs.\nbend_left = [10, 0.01]          # slight left veer (will extend over all timepts)\nstraight_line = (               # straight line from start to end with nominal input\n    np.array([x0 + (xf - x0) * t/Tf for t in timepts]).transpose(),\n    u0\n)\n\n\n\n","type":"content","url":"/lab7-trajectory-generation#vehicle-steering-dynamics","position":11},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl3":"Approach 1: standard quadratic cost","lvl2":"Task 2: Kinematic Car: Optimal trajectory generation"},"type":"lvl3","url":"/lab7-trajectory-generation#approach-1-standard-quadratic-cost","position":12},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl3":"Approach 1: standard quadratic cost","lvl2":"Task 2: Kinematic Car: Optimal trajectory generation"},"content":"We can set up the optimal control problem as trying to minimize the distance from the desired final point while at the same time as not exerting too much control effort to achieve our goal.\\min_{u(\\cdot)}\n  \\int_0^T \\left[(x(\\tau) - x_\\text{f})^T Q_x (x(\\tau) - x_\\text{f}) + (u(\\tau) - u_\\text{f})^T Q_u (u(\\tau) - u_\\text{f})\\right] \\, d\\tau\n\nsubject to\\dot x = f(x, u), \\qquad x \\in \\mathbb{R}^n,\\, u \\in \\mathbb{R}^m\n\nThe optimization module in default mode is using direct shooting method to solve optimal control problems by choosing the values of the input at each point in the time horizon to try to minimize the cost:u_k(t_j) = \\alpha_{k, j}, \\qquad\nu_k(t) = \\frac{t_{j+1} - t}{h} \\alpha_{k, j} + \\frac{t - t_j}{h} \\alpha_{{k},{j+1}}\n\nwhere k is the index for control input (we have two inputs), j is the index for disctetized time, h is the time step. This equation is used because, in direct shooting methods for optimal control, the optimizer searches for the best input values at each discretized time point. The control input u_k(t) is parameterized by a set of variables \\alpha_{k, j} at each time t_j, and between time points, the input is typically interpolated (here, linearly) to ensure a smooth trajectory. This approach converts the infinite-dimensional control problem into a finite-dimensional optimization over the parameters \\alpha_{k, j}, making it tractable for numerical solvers. The finer the time discretization, the more parameters the optimizer has to adjust, allowing for more accurate and flexible control profiles.\n\n# Set up the cost functions\nQx = np.diag([.1, 10, .1])       # keep lateral error low\nQu = np.diag([.1, 1])            # minimize applied inputs\n# Set desired final state and input for cost function (penalize deviation from these)\n# Note: x0=xf and u0=uf here specify the \"target\" state and input for the cost function,\n# not the initial condition. This means the optimizer penalizes deviation from the desired final state/input\n# throughout the trajectory, even though the actual initial state is x0 (defined elsewhere).\nquad_cost = opt.quadratic_cost(kincar, Qx, Qu, x0=xf, u0=uf)\n\n# Compute the optimal control, setting step size for gradient calculation (eps)\nstart_time = time.process_time() # time how long the optimization takes\nresult1 = opt.solve_ocp(\n    kincar, timepts, x0, quad_cost,\n     initial_guess=straight_line,\n    # initial_guess= bend_left,\n    # initial_guess=u0,\n    # minimize_method='trust-constr',\n    # minimize_options={'finite_diff_rel_step': 0.01},\n    # trajectory_method='shooting'\n    # solve_ivp_method='LSODA'\n)\nprint(\"* Total time = %5g seconds\\n\" % (time.process_time() - start_time))\n\n# Plot the results from the optimization\nplot_lanechange(timepts, result1.states, result1.inputs, xf)\nprint(\"Final computed state: \", result1.states[:,-1])\n\n# Simulate the system using the optimal control inputs and see what happens\n# Think about why doing this\nt1, u1 = result1.time, result1.inputs\nt1, y1 = ctl.input_output_response(kincar, timepts, u1, x0)\nplot_lanechange(t1, y1, u1, yf=xf[0:2])\nprint(\"Final simulated state:\", y1[:,-1])\n\n# Label the different lines\nplt.subplot(3, 1, 1)\nplt.legend(['optimized', 'simulated', 'endpoint'])\nplt.tight_layout()\n\n\n\n\n\nNote the amount of time required to solve the problem and also any warning messages about to being able to solve the optimization (mainly in earlier versions of python-control).  You can try to adjust a number of factors to try to get a better solution:\n\nTry changing the number of points in the time horizon\n\nTry using a different initial guess\n\nTry changing the optimization method (see commented out code)\n\n","type":"content","url":"/lab7-trajectory-generation#approach-1-standard-quadratic-cost","position":13},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl3":"Approach 2: input cost, input constraints, terminal cost","lvl2":"Task 2: Kinematic Car: Optimal trajectory generation"},"type":"lvl3","url":"/lab7-trajectory-generation#approach-2-input-cost-input-constraints-terminal-cost","position":14},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl3":"Approach 2: input cost, input constraints, terminal cost","lvl2":"Task 2: Kinematic Car: Optimal trajectory generation"},"content":"The previous solution integrates the position error for the entire horizon, and so the car changes lanes very quickly (at the cost of larger inputs).  Instead, we can penalize the final state and impose a higher cost on the inputs, resulting in a more gradual lane change. We can also limit the control input to be in a range u_{\\min} \\le u(\\tau) \\le u_{\\max}, for any \\tau \\in [0, T].\\min_{u(\\cdot)}\n  \\int_0^T \\underbrace{\\left[(u(\\tau) - u_\\text{f})^T Q_u (u(\\tau) - u_\\text{f})\\right]}_{L(x, u)} \\, d\\tau + \\underbrace{(x(T) - x_\\text{f})^T Q_\\text{f} (x(T) - x_\\text{f})}_{V\\left(x(T)\\right)}\n\nsubject to\\dot x = f(x, u), \\qquad u_\\text{lb} \\leq u(t) \\leq u_\\text{ub} \\qquad x \\in \\mathbb{R}^n,\\, u \\in \\mathbb{R}^m\n\nWe can also try using a different solver for this example.  You can pass the solver using the minimize_method keyword and send options to the solver using the minimize_options keyword (which should be set to a dictionary of options).\n\n# Add input constraint, input cost, terminal cost\nu_lb = np.array([8, -0.1])\nu_ub = np.array([12, 0.1])\nconstraints = [ opt.input_range_constraint(kincar, u_lb, u_ub) ] # input limits\ntraj_cost = opt.quadratic_cost(kincar, None, np.diag([0.1, 1]), u0=uf)\nterm_cost = opt.quadratic_cost(kincar, np.diag([1, 10, 100]), None, x0=xf)\n\n# Compute the optimal control\nstart_time = time.process_time()\nresult2 = opt.solve_ocp(\n    kincar, timepts, x0, traj_cost, constraints, terminal_cost=term_cost,\n    initial_guess=straight_line,\n    # minimize_method='trust-constr',\n    # minimize_options={'finite_diff_rel_step': 0.01},\n    # minimize_method='SLSQP', minimize_options={'eps': 0.01},\n    # log=True,\n)\nprint(\"* Total time = %5g seconds\\n\" % (time.process_time() - start_time))\n\n# Plot the results from the optimization\nplot_lanechange(timepts, result2.states, result2.inputs, xf)\nprint(\"Final computed state: \", result2.states[:,-1])\n\n# Simulate the system and see what happens\nt2, u2 = result2.time, result2.inputs\nt2, y2 = ctl.input_output_response(kincar, timepts, u2, x0)\nplot_lanechange(t2, y2, u2, yf=xf[0:2])\nprint(\"Final simulated state:\", y2[:,-1])\n\n# Label the different lines\nplt.subplot(3, 1, 1)\nplt.legend(['desired', 'simulated', 'endpoint'], loc='upper left')\nplt.tight_layout()\n\n\n\n\n\n","type":"content","url":"/lab7-trajectory-generation#approach-2-input-cost-input-constraints-terminal-cost","position":15},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl3":"Approach 3: Input cost, terminal constraints","lvl2":"Task 2: Kinematic Car: Optimal trajectory generation"},"type":"lvl3","url":"/lab7-trajectory-generation#approach-3-input-cost-terminal-constraints","position":16},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl3":"Approach 3: Input cost, terminal constraints","lvl2":"Task 2: Kinematic Car: Optimal trajectory generation"},"content":"We can also remove the cost function on the state and replace it with a terminal constraint on the state as well as bounds on the inputs.  If a solution is found, it guarantees we get to exactly the final state:\\min_{u(\\cdot)}\n  \\int_0^T \\underbrace{(u(\\tau) - u_\\text{f})^T Q_u (u(\\tau) - u_\\text{f})}_{L(x, u)} \\, d\\tau\n\nsubject to\\begin{aligned}\n  \\dot x &= f(x, u), & \\qquad &x \\in \\mathbb{R}^n,\\, u \\in \\mathbb{R}^m \\\\\n  x(T) &= x_\\text{f} & &u_\\text{lb} \\leq u(t) \\leq u_\\text{ub},\\, \\text{for all $t$}\n  \\end{aligned}\n\nNote that trajectory and terminal constraints can be very difficult to satisfy for a general optimization.\n\n# Input cost and terminal constraints\nR = np.diag([1, 1])                 # minimize applied inputs\ncost3 = opt.quadratic_cost(kincar, np.zeros((3,3)), R, u0=uf)\nconstraints = [\n    opt.input_range_constraint(kincar, [8, -0.1], [12, 0.1]) ]\nterminal = [ opt.state_range_constraint(kincar, xf, xf) ]\n\n# Compute the optimal control\nstart_time = time.process_time()\nresult3 = opt.solve_ocp(\n    kincar, timepts, x0, cost3, constraints,\n    terminal_constraints=terminal, initial_guess=straight_line,\n#    solve_ivp_kwargs={'atol': 1e-3, 'rtol': 1e-2},\n#    minimize_method='trust-constr',\n#    minimize_options={'finite_diff_rel_step': 0.01},\n)\nprint(\"* Total time = %5g seconds\\n\" % (time.process_time() - start_time))\n\n# Plot the results from the optimization\nplot_lanechange(timepts, result3.states, result3.inputs, xf)\nprint(\"Final computed state: \", result3.states[:,-1])\n\n# Simulate the system and see what happens\nt3, u3 = result3.time, result3.inputs\nt3, y3 = ctl.input_output_response(kincar, timepts, u3, x0)\nplot_lanechange(t3, y3, u3, yf=xf[0:2])\nprint(\"Final simulated state:\", y3[:,-1])\n\n# Label the different lines\nplt.subplot(3, 1, 1)\nplt.legend(['desired', 'simulated', 'endpoint'], loc='upper left')\nplt.tight_layout()\n\n\n\n\n\n","type":"content","url":"/lab7-trajectory-generation#approach-3-input-cost-terminal-constraints","position":17},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl3":"Additional things to try","lvl2":"Task 2: Kinematic Car: Optimal trajectory generation"},"type":"lvl3","url":"/lab7-trajectory-generation#additional-things-to-try","position":18},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl3":"Additional things to try","lvl2":"Task 2: Kinematic Car: Optimal trajectory generation"},"content":"Try using different weights, solvers, initial guess and other properties and see how things change.\n\nTry using different values for initial_guess to get faster convergence and/or different classes of solutions.\n\n","type":"content","url":"/lab7-trajectory-generation#additional-things-to-try","position":19},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl2":"HW Problem"},"type":"lvl2","url":"/lab7-trajectory-generation#hw-problem","position":20},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl2":"HW Problem"},"content":"","type":"content","url":"/lab7-trajectory-generation#hw-problem","position":21},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl3":"Problem 1: Trajectory Generation for Torque Limited Pendulum","lvl2":"HW Problem"},"type":"lvl3","url":"/lab7-trajectory-generation#problem-1-trajectory-generation-for-torque-limited-pendulum","position":22},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl3":"Problem 1: Trajectory Generation for Torque Limited Pendulum","lvl2":"HW Problem"},"content":"In this problem, we will generate optimal trajectories for the pendulum with limited torque. For completeness, we include the dynamics equation here again:\\begin{align}\n\\dot{x}_1 &= x_2 \\\\\n\\dot{x}_2 &= -\\frac{g}{L}\\sin x_1 - \\frac{b}{mL^2}\\,x_2 + \\frac{u}{mL^2}\n\\end{align}\n\nwhere:\n\nx_1 = \\theta (pendulum angle)\n\nx_2 = \\dot{\\theta} (angular velocity)\n\nu = input torque (Nm)\n\ng = 9.81\\,\\text{m/s}^2 (gravity)\n\nm = 1\\,\\text{kg} (mass)\n\nL = 1\\,\\text{m} (length)\n\nb = 1.5 (N·m·s/rad, damping coefficient)\n\nInitial state: x_0 = [0,\\, 0] (pendulum hanging down at rest)Final state: x_f = [\\pi,\\, 0] (pendulum upright at rest)","type":"content","url":"/lab7-trajectory-generation#problem-1-trajectory-generation-for-torque-limited-pendulum","position":23},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl4":"Tasks","lvl3":"Problem 1: Trajectory Generation for Torque Limited Pendulum","lvl2":"HW Problem"},"type":"lvl4","url":"/lab7-trajectory-generation#tasks","position":24},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl4":"Tasks","lvl3":"Problem 1: Trajectory Generation for Torque Limited Pendulum","lvl2":"HW Problem"},"content":"1. Large Torque Limit (|u| \\leq 5):\n\nGenerate an optimal trajectory for the pendulum with input constraint |u| \\leq 5 (corresponding to a strong motor). Use a time horizon of 5 seconds to start; increase it if the optimizer cannot reach the final state.\n\nFor each of the following cost functions, generate and plot the optimal trajectory:\n\n(a) Input cost + terminal cost:\n\\min_{u(\\cdot)} \\int_0^T (u(\\tau) - u_f)^T Q_u (u(\\tau) - u_f)\\, d\\tau + (x(T) - x_f)^T Q_f (x(T) - x_f)\n\n(b) Input cost with terminal constraint:\n\\min_{u(\\cdot)} \\int_0^T (u(\\tau) - u_f)^T Q_u (u(\\tau) - u_f)\\, d\\tau\n\n\nsubject to x(T) = x_f\n\nFor each case, discuss:\n\nAre the results different? Why?\n\nDid you need to increase the time horizon to reach the final state?\n\n2. Small Torque Limit (|u| \\leq 0.5):\n\nRepeat Task 1 with a smaller input constraint |u| \\leq 0.5 (weaker motor).\n\nGenerate and plot the optimal trajectory for both cost functions above.\n\nDiscuss how the reduced torque limit affects the ability to reach the final state and the required maneuver time.\n\n3. Velocity Constraint (|x_2| \\leq 0.3 rad/s):\n\nNow, add a constraint on the angular velocity: |x_2| \\leq 0.3 rad/s (e.g., for safety or actuator limits), with |u| \\leq 1.\n\nGenerate and plot the optimal trajectory for both cost functions above.\n\nIf all other parameters (horizon, Q_u, Q_f, etc.) are the same as in Task 1, how do the results change compared to Task 1? Discuss the effect of the velocity constraint.\n\n# Parameter values for pendulum\npendulum_params = {\n    'g': 9.81,            # Gravitational acceleration (m/s^2)\n    'L': 1.0,             # Pendulum length (m)\n    'b': 0.5,             # Damping coefficient (N.m.s/rad)\n    'm': 1.0,             # Pendulum mass (kg)\n}\n\n# State derivative for pendulum\ndef pendulum_update(t, x, u, params):\n    # Extract the configuration and velocity variables from the state vector\n    theta = x[0]                # Angular position of the pendulum\n    thetadot = x[1]             # Angular velocity of the pendulum\n    tau = u[0]                  # Torque applied at the pivot\n\n    # Get the parameter values\n    g, L, b, m = map(params.get, ['g', 'L', 'b', 'm'])\n\n    # Compute the angular acceleration using pendulum dynamics\n    # x_dot_1 = x_2\n    # x_dot_2 = -g/L * sin(x_1) - b/(m*L^2) * x_2 + u/(m*L^2)\n    dthetadot = -g/L * np.sin(theta) - b/(m*L**2) * thetadot + tau/(m*L**2)\n\n    # Return the state update law\n    return np.array([thetadot, dthetadot])\n\n# System output (angle only, as specified y = x_1)\ndef pendulum_output(t, x, u, params):\n    return x  # Output y = [theta, thetadot]\n\n# create the nonlinear system using nlsys,\n# note that the outputs is theta so that we can successfully connect the controller to it\npendulum = ctl.nlsys(\n    pendulum_update, pendulum_output, name='pendulum',\n    params=pendulum_params, states=['theta', 'thetadot'],\n    outputs=['theta', 'thetadot'], inputs=['tau'])\n\nprint(pendulum)\nprint(\"\\nParams:\", pendulum.params)\n\n\n\n# Utility function to plot pendulum results\ndef plot_pendulum(t, x, u, figure=None, label=None):\n    plt.subplot(2, 1, 1)\n    plt.plot(t, x[0], label=f'$\\\\theta$ {label or \"\"}')\n    plt.plot(t, x[1], label=f'$\\\\dot{{\\\\theta}}$ {label or \"\"}')\n    plt.ylabel(\"State\")\n    plt.legend()\n    plt.title(\"Pendulum Trajectory\")\n\n    plt.subplot(2, 1, 2)\n    plt.plot(t, u[0], label=f'$\\\\tau$ {label or \"\"}')\n    plt.xlabel(\"Time $t$ [sec]\")\n    plt.ylabel(\"Input Torque [Nm]\")\n    plt.legend()\n    plt.tight_layout()\n\n# Problem setup\nx0 = np.array([0., 0.])\nxf = np.array([np.pi, 0.])\nuf = np.array([0.])\nTf = 10.0  # Maneuver time\ntimepts = np.linspace(0, Tf, 50, endpoint=True)\n\n# Initial guess: straight line from x0 to xf, zero input\ninitial_guess = (\n    np.array([x0 + (xf - x0) * t/Tf for t in timepts]).T,\n    np.zeros((1, len(timepts)))\n)\n\n\n\n\n","type":"content","url":"/lab7-trajectory-generation#tasks","position":25},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl3":"Problem 2: Static Optimization with Lagrange Multipliers","lvl2":"HW Problem"},"type":"lvl3","url":"/lab7-trajectory-generation#problem-2-static-optimization-with-lagrange-multipliers","position":26},{"hierarchy":{"lvl1":"Lab7: Trajectory Generation","lvl3":"Problem 2: Static Optimization with Lagrange Multipliers","lvl2":"HW Problem"},"content":"This problem is a manual derivation exercise. Please submit your detailed solution as a separate PDF.\n\nWe want to minimize the following cost function:J(u_0, u_1, u_2) = u_0^2 + u_1^2 + u_2^2\n\nSubject to the constraints:\\begin{cases}\n2u_0 + u_1 = 5 \\\\\\\\\nu_0 + u_1 + u_2 = 1\n\\end{cases}\n\nTask:Use the method of Lagrange multipliers to find the values of u_0, u_1, and u_2 that minimize J while satisfying the constraints above. Show all steps in your derivation.","type":"content","url":"/lab7-trajectory-generation#problem-2-static-optimization-with-lagrange-multipliers","position":27},{"hierarchy":{"lvl1":"Lab8: Model Predictive Control"},"type":"lvl1","url":"/lab8-model-predictive-control","position":0},{"hierarchy":{"lvl1":"Lab8: Model Predictive Control"},"content":"This lab is modified from \n\nnotebook by Prof. Richard Murray\n\nThis lab introduces Model Predictive Control (MPC). We will start by implementing a continuous-time MPC for a double integrator system and compare its performance against a standard LQR controller. We will then explore a discrete-time MPC implementation for the same system.\n\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport time\ntry:\n  import control as ctl\n  print(\"python-control\", ctl.__version__)\nexcept ImportError:\n  !pip install control\n  import control as ctl\nimport control.optimal as opt\nimport control.flatsys as fs\n\n\n\n","type":"content","url":"/lab8-model-predictive-control","position":1},{"hierarchy":{"lvl1":"Lab8: Model Predictive Control","lvl2":"System definition for Double Integrator"},"type":"lvl2","url":"/lab8-model-predictive-control#system-definition-for-double-integrator","position":2},{"hierarchy":{"lvl1":"Lab8: Model Predictive Control","lvl2":"System definition for Double Integrator"},"content":"To illustrate the implementation of a model predictive controller, we consider a linear system corresponding to a double integrator with bounded input:\\dot x = \\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix} x + \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\text{clip}(u)\n  \\qquad\\text{where}\\qquad\n  \\text{clip}(u) = \\begin{cases}\n    -1 & u < -1, \\\\\n    u & -1 \\leq u \\leq 1, \\\\\n    1 & u > 1.\n  \\end{cases}\n\nWe implement a model predictive controller by choosingQ_x = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix}, \\qquad\n  Q_u = \\begin{bmatrix} 1 \\end{bmatrix}, \\qquad\n  P_1 = \\begin{bmatrix} 0.1 & 0 \\\\ 0 & 0.1 \\end{bmatrix}.\n\nThe system can be defined as follows.\n\ndef doubleint_update(t, x, u, params):\n    # Get the parameters\n    lb = params.get('lb', -1)\n    ub = params.get('ub', 1)\n    assert lb < ub\n\n    # bound the input\n    u_clip = np.clip(u, lb, ub)\n\n    return np.array([x[1], u_clip[0]])\n\nproc = ctl.nlsys(\n    doubleint_update, None, name=\"double integrator\",\n    inputs = ['u'], outputs=['x[0]', 'x[1]'], states=2)\n\n\n\n","type":"content","url":"/lab8-model-predictive-control#system-definition-for-double-integrator","position":3},{"hierarchy":{"lvl1":"Lab8: Model Predictive Control","lvl2":"Task 1: Continous time MPC"},"type":"lvl2","url":"/lab8-model-predictive-control#task-1-continous-time-mpc","position":4},{"hierarchy":{"lvl1":"Lab8: Model Predictive Control","lvl2":"Task 1: Continous time MPC"},"content":"We will first implement a continous time MPC for the system.","type":"content","url":"/lab8-model-predictive-control#task-1-continous-time-mpc","position":5},{"hierarchy":{"lvl1":"Lab8: Model Predictive Control","lvl3":"Problem Setup","lvl2":"Task 1: Continous time MPC"},"type":"lvl3","url":"/lab8-model-predictive-control#problem-setup","position":6},{"hierarchy":{"lvl1":"Lab8: Model Predictive Control","lvl3":"Problem Setup","lvl2":"Task 1: Continous time MPC"},"content":"To define a MPC controller, we first need to create an optimal control problem (using the OptimalControlProblem class) and then use the compute_trajectory method to solve for the trajectory from the current state. This compute_trajectory method is the same as the ocp_solve we played with in Lab 7.\n\nWe start by defining the cost functions, which consists of a trajectory cost (or integral cost) and a terminal cost:\n\nQx = np.diag([1, 0])            # weight matrix for state cost\nQu = np.diag([1])               # weight matrix for input cost\ntraj_cost=opt.quadratic_cost(proc, Qx, Qu)\n\nP1 = np.diag([0.1, 0.1])        # weight matrix for terminal cost\nterm_cost = opt.quadratic_cost(proc, P1, None)\n\n\n\nWe also set up a set of constraints that correspond to the fact that the input should be less than 1.  This can be done using either the \n\ninput_range_constraint function or the \n\ninput_poly_constraint function.\n\ntraj_constraints = opt.input_range_constraint(proc, -1, 1)\n# traj_constraints = opt.input_poly_constraint(\n#     proc, np.array([[1], [-1]]),  np.array([1, 1]))\n\n\n\nWe define the horizon for evaluating finite-time, optimal control by setting up a set of time points across the designed horizon.  The input will be computed at each time point.\n\nTh = 5  # horizon length: this is how far ahead we plan for each repetition of the MPC\ntimepts = np.linspace(0, Th, 11, endpoint=True)\nprint(timepts)\n\n\n\nFinally, we define the optimal control problem that we want to solve (without actually solving it).\n\n# Set up the optimal control problem\nocp = opt.OptimalControlProblem(\n    proc, timepts, traj_cost,\n    terminal_cost=term_cost,\n    trajectory_constraints=traj_constraints,\n    # terminal_constraints=term_constraints,\n)\n\n\n\nTo make sure that the problem is properly defined, we first solve the problem for a specific initial condition.  We also compare the amount of time required to solve the problem from a “cold start” (no initial guess) versus a “warm start” (use the previous solution, shifted forward on point in time).\n\nX0 = np.array([1, 1])\n\nstart_time = time.process_time()\nres = ocp.compute_trajectory(X0, initial_guess=0, return_states=True)\nstop_time = time.process_time()\nprint(f'* Cold start: {stop_time-start_time:.3} sec')\n\n# Resolve using previous solution (shifted forward) as initial guess to compare timing\nstart_time = time.process_time()\nu = res.inputs\nu_shift = np.hstack([u[:, 1:], u[:, -1:]])\nocp.compute_trajectory(X0, initial_guess=u_shift, print_summary=False)\nstop_time = time.process_time()\nprint(f'* Warm start: {stop_time-start_time:.3} sec')\n\n\n\nIn this case the timing is not that different since the system is very simple.\n\nPlotting the result, we see that the solution is properly computed.\n\nplt.plot(res.time, res.states[0], 'k-', label='$x_1$')\nplt.plot(res.time, res.inputs[0], 'b-', label='u')\nplt.xlabel('Time [s]')\nplt.ylabel('$x_1$, $u$')\nplt.legend();\n\n\n\n","type":"content","url":"/lab8-model-predictive-control#problem-setup","position":7},{"hierarchy":{"lvl1":"Lab8: Model Predictive Control","lvl3":"MPC controller","lvl2":"Task 1: Continous time MPC"},"type":"lvl3","url":"/lab8-model-predictive-control#mpc-controller","position":8},{"hierarchy":{"lvl1":"Lab8: Model Predictive Control","lvl3":"MPC controller","lvl2":"Task 1: Continous time MPC"},"content":"After setting up the problem, now we can implement the MPC controller using a function that we can use with different versions of the problem.\n\ndef run_mpc(proc, ocp, x0, tf, print_summary=False, verbose=False):\n    \"\"\"Run a MPC simulation.\"\"\"\n    x = x0\n    t_cur = 0.0 # current time\n\n    # Lists to store the full trajectory\n    time_hist, states_hist, inputs_hist = [np.array([t_cur])], [x0[:, np.newaxis]], []\n    first_iteration = True\n\n    # MPC loop\n    while t_cur < tf:\n        # Compute the optimal trajectory over the horizon\n        start_time = time.process_time()\n        res = ocp.compute_trajectory(x, print_summary=print_summary)\n        if verbose:\n            print(f\"t={t_cur:.2f}, compute time: {time.process_time() - start_time:.3f}s\")\n\n        # Determine the time step to apply (first interval of the OCP)\n        dt = res.time[1] - res.time[0]\n\n        # Handle the final, possibly partial, time step\n        t_apply = min(dt, tf - t_cur)\n        if t_apply < 1e-6: break # Avoid tiny final steps\n\n        # Simulate the system for one step using a constant u computed from the trajectory optimization\n        u_const = res.inputs[:, 0] # obtain the first control input\n        t_seg = np.linspace(0, t_apply, 20)\n        u_const_seg = np.tile(u_const[:, np.newaxis], (1, len(t_seg))) # constant input over the segment\n        soln = ctl.input_output_response(proc, t_seg, u_const_seg, x)\n\n        \"\"\"\n        # Simulate the system for one step using a first-order hold on the input\n        t_seg = np.linspace(0, t_apply, 20)\n        u_foh = res.inputs[:, 0] + np.outer(\n            (res.inputs[:, 1] - res.inputs[:, 0]) / dt, t_seg\n        )\n        soln = ctl.input_output_response(proc, t_seg, u_foh, x)\n        \"\"\"\n\n        # On the first iteration, store the initial input value\n        if first_iteration:\n            inputs_hist.append(soln.inputs[:, 0:1])\n            first_iteration = False\n\n        # Store the trajectory segment (omitting the first point to avoid overlap)\n        time_hist.append(t_cur + soln.time[1:])\n        states_hist.append(soln.states[:, 1:])\n        inputs_hist.append(soln.inputs[:, 1:])\n\n        # Update state and time for the next iteration\n        t_cur += t_apply # Advance current time\n        x = soln.states[:, -1] # Get the final state of the segment\n\n    return ctl.TimeResponseData(\n        np.hstack(time_hist),\n        np.hstack(states_hist), # Note: output is state for this system\n        np.hstack(states_hist),\n        np.hstack(inputs_hist)\n    )\n\n# Plotting function for MPC response\ndef plot_mpc(mpc_resp, ax=None):\n    \"\"\"Plot the results of a model predictive control simulation.\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(1, 1)\n\n    ax.plot(mpc_resp.time, mpc_resp.states[0], 'k-', label='$x_1$')\n    ax.plot(mpc_resp.time, mpc_resp.inputs[0], 'b-', label='$u$')\n\n    # Add reference line for input lower bound\n    ax.plot([0, mpc_resp.time[-1]], [-1, -1], 'k--', linewidth=0.7)\n\n    ax.set_ylim([-4, 3.5])\n    ax.set_xlabel(\"Time $t$ [sec]\")\n    ax.set_ylabel(\"State $x_1$, input $u$\")\n    ax.legend(loc='lower right')\n    ax.set_title(\"Model Predictive Control Response\")\n    plt.tight_layout()\n\n\n\nFinally, we call the controller and plot the response. To get rid of the statistics of each optimization call, use print_summary=False.\n\nTf = 10 # Total time for control\n# Run the simulation\nmpc_resp = run_mpc(proc, ocp, X0, Tf, verbose=True, print_summary=False)\nprint(f\"Final state xf = {mpc_resp.states[:, -1]}\")\n\n# Plot the results\nplot_mpc(mpc_resp)\n\n\n\n\n\nQuestions to think about:\n\nWe used four different time in the above simulation: timepts, Th, Tf, and t_seg. Explain what are they. Try to use different values for them and see how will it affect the simulation results.\n\nIn each step of the MPC loop, an optimal control problem is solved over the horizon Th. This provides an optimal input trajectory u(t) for t from 0 to Th. How much of this trajectory is actually applied to the plant before the optimization is performed again? What is the time step of the MPC controller?\n\nThe terminal cost is defined by the matrix P1. What is the purpose of a terminal cost? What might happen to the system’s final state if you set P1 to be a zero matrix?\n\nTry to use the first-order hold on the input that is commented out in the above code. Explain what is different from the constant control input? Do you notice any difference on the state trajectory?\n\n","type":"content","url":"/lab8-model-predictive-control#mpc-controller","position":9},{"hierarchy":{"lvl1":"Lab8: Model Predictive Control","lvl3":"MPC vs LQR vs LQR terminal cost","lvl2":"Task 1: Continous time MPC"},"type":"lvl3","url":"/lab8-model-predictive-control#mpc-vs-lqr-vs-lqr-terminal-cost","position":10},{"hierarchy":{"lvl1":"Lab8: Model Predictive Control","lvl3":"MPC vs LQR vs LQR terminal cost","lvl2":"Task 1: Continous time MPC"},"content":"In the formulation above, we used a MPC controller with the terminal cost as P_1 = \\text{diag}(0.1, 0.1). An alternative is to set the terminal cost to be the LQR terminal cost that goes along with the trajectory cost, which then provides a “cost to go” that matches the LQR “cost to go” (but keeping in mind that the LQR controller does not necessarily respect the constraints for the control inputs).\n\nThe following code compares three different controllers:\n\nMPC with P1 terminal cost (MPC + P1): This is the original MPC controller we designed. It uses a simple quadratic terminal cost defined by P1. This controller solves an optimal control problem at each step to plan a trajectory over a finite horizon, explicitly considering input constraints.\n\nStandard LQR controller (LQR): This is a linear state-feedback controller with gain K derived for the linearized, unconstrained system. The control law is u = -Kx. For the simulation, this control law is applied to the original nonlinear system, so the input is effectively saturated (clip(-Kx)). This controller is not predictive and does not explicitly account for constraints in its design, though it is subject to them in simulation.\n\nMPC with LQR terminal cost (MPC + P_lqr): This MPC controller is identical to the first, but it uses the solution P_lqr of the algebraic Riccati equation from the corresponding LQR problem as its terminal cost. This provides a more accurate estimate of the “cost-to-go” beyond the prediction horizon, often leading to improved performance and stability.\n\n# Get the LQR solution\nK, P_lqr, E = ctl.lqr(proc.linearize(0, 0), Qx, Qu)\nprint(f\"P_lqr = \\n{P_lqr}\")\n\n# Create an LQR controller (and run it)\nlqr_ctrl, lqr_clsys = ctl.create_statefbk_iosystem(proc, K)\nlqr_resp = ctl.input_output_response(lqr_clsys, mpc_resp.time, 0, X0)\n\n# Create a new optimal control problem using the LQR terminal cost\n# (need use more refined time grid as well, to approximate LQR rate)\nlqr_timepts = np.linspace(0, Th, 25, endpoint=True)\nlqr_term_cost=opt.quadratic_cost(proc, P_lqr, None) # here we used P_lqr instead of P1\nocp_lqr = opt.OptimalControlProblem(\n    proc, lqr_timepts, traj_cost, terminal_cost=lqr_term_cost,\n    trajectory_constraints=traj_constraints,\n)\n\n# Create the response for the new controller\nmpc_lqr_resp = run_mpc(\n    proc, ocp_lqr, X0, 10, print_summary=False)\n\n# Plot the different responses to compare them\nfig, ax = plt.subplots(2, 1)\nax[0].plot(mpc_resp.time, mpc_resp.states[0], label='MPC + P_1')\nax[0].plot(lqr_resp.time, lqr_resp.outputs[0], ':', label='LQR')\nax[0].plot(mpc_lqr_resp.time, mpc_lqr_resp.states[0], '--', label='MPC + P_lqr')\nax[0].set_ylabel('Position $x_1$')\nax[0].legend()\n\nax[1].plot(mpc_resp.time, mpc_resp.inputs[0], label='MPC + P_1')\nax[1].plot(lqr_resp.time, lqr_resp.outputs[2], ':', label='LQR')\nax[1].plot(mpc_lqr_resp.time, mpc_lqr_resp.inputs[0], '--', label='MPC + P_lqr')\nax[1].set_ylabel('Input $u$')\nax[1].set_xlabel('Time [s]')\nax[1].legend()\n\n\n\n\n\n","type":"content","url":"/lab8-model-predictive-control#mpc-vs-lqr-vs-lqr-terminal-cost","position":11},{"hierarchy":{"lvl1":"Lab8: Model Predictive Control","lvl2":"Task 2: Discrete time MPC"},"type":"lvl2","url":"/lab8-model-predictive-control#task-2-discrete-time-mpc","position":12},{"hierarchy":{"lvl1":"Lab8: Model Predictive Control","lvl2":"Task 2: Discrete time MPC"},"content":"Continous time MPC can generate smooth, higher-fidelity behavior at higher compute cost and with interpolation subtleties.  Many MPC control problems are solved based on a discrete time model, which is simpler, faster, and closer to how real controllers run, provided your time step is chosen well.  We show here how to implement this for the “double integrator” system, which in discrete time has the formx[k+1] = \\begin{bmatrix} 1 & dt \\\\ 0 & 1 \\end{bmatrix} x[k] + \\begin{bmatrix} 0 \\\\ dt \\end{bmatrix} \\text{clip}(u[k])\n\nwhere dt is the time step.\n\n# System definition\n\ndef doubleint_update(t, x, u, params):\n    # Get the parameters\n    lb = params.get('lb', -1)\n    ub = params.get('ub', 1)\n    assert lb < ub\n\n    # Get the sampling time\n    dt = params.get('dt', 1)\n\n    # bound the input\n    u_clip = np.clip(u, lb, ub)\n\n    return np.array([x[0] + dt * x[1], x[1] + dt * u_clip[0]])\n\nproc = ctl.nlsys(\n    doubleint_update, None, name=\"double integrator\",\n    inputs = ['u'], outputs=['x[0]', 'x[1]'], states=2,\n    params={'dt': 1}, dt=1)\n\n#\n# Linear quadratic regulator\n#\n\n# Define the cost functions to use\nQx = np.diag([1, 0])            # matrix for state cost\nQu = np.diag([1])               # matrix for input cost\nP1 = np.diag([0.1, 0.1])        # matrix for terminal cost\n\n# Get the LQR solution\nK, P, E = ctl.dlqr(proc.linearize(0, 0), Qx, Qu)\n\n# Test out the LQR controller, with no constraints\nlinsys = proc.linearize(0, 0)\nclsys_lin = ctl.ss(linsys.A - linsys.B @ K, linsys.B, linsys.C, 0, dt=proc.dt)\n\nX0 = np.array([2, 1])           # initial conditions\nTf = 10                         # simulation time\nres = ctl.initial_response(clsys_lin, Tf, X0=X0)\n\n# Plot the results\nplt.figure(1); plt.clf(); ax = plt.axes()\nax.plot(res.time, res.states[0], 'k-', label='$x_1$')\nax.plot(res.time, (-K @ res.states)[0], 'b-', label='$u$')\n\n# Test out the LQR controller with constraints\nclsys_lqr = ctl.feedback(proc, -K, 1)\ntvec = np.arange(0, Tf, proc.dt)\nres_lqr_const = ctl.input_output_response(clsys_lqr, tvec, 0, X0)\n\n# Plot the results\nax.plot(res_lqr_const.time, res_lqr_const.states[0], 'k--', label='constrained')\n#ax.plot(res_lqr_const.time, (-K @ res_lqr_const.states)[0], 'b--')\nax.plot([0, 7], [-1, -1], 'k--', linewidth=0.75)\n\n# Compute and plot the clipped (actual) control input\nu_raw_const = (-K @ res_lqr_const.states)       # controller output before clipping\nu_clip_const = np.clip(u_raw_const, -1, 1)      # actual applied control\nax.plot(res_lqr_const.time, u_clip_const[0], 'b--')\n\n\n# Adjust the limits for consistency\nax.set_ylim([-4, 3.5])\n\n# Label the results\nax.set_xlabel(\"Time $t$ [sec]\")\nax.set_ylabel(\"State $x_1$, input $u$\")\nax.legend(loc='lower right', labelspacing=0)\nplt.title(\"Linearized LQR response from x0\")\n\n\n\n#\n# MPC controller\n#\n\n# Create the constraints\ntraj_constraints = opt.input_range_constraint(proc, -1, 1)\nterm_constraints = opt.state_range_constraint(proc, [0, 0], [0, 0])\n\n# Define the optimal control problem we want to solve\nT = 5\ntimepts = np.arange(0, T * proc.dt, proc.dt)\n\n# Set up four optimal control problems\n# ocp1: the original with P1\nocp_orig = opt.OptimalControlProblem(\n    proc, timepts,\n    opt.quadratic_cost(proc, Qx, Qu),\n    trajectory_constraints=traj_constraints,\n    terminal_cost=opt.quadratic_cost(proc, P1, None),\n)\n\n# ocp2: the LQR with P\nocp_lqr = opt.OptimalControlProblem(\n    proc, timepts,\n    opt.quadratic_cost(proc, Qx, Qu),\n    trajectory_constraints=traj_constraints,\n    terminal_cost=opt.quadratic_cost(proc, P, None),\n)\n\n#ocp3: scaled down P\nocp_low = opt.OptimalControlProblem(\n    proc, timepts,\n    opt.quadratic_cost(proc, Qx, Qu),\n    trajectory_constraints=traj_constraints,\n    terminal_cost=opt.quadratic_cost(proc, P/10, None),\n)\n\n#ocp4: scaled up P\nocp_high = opt.OptimalControlProblem(\n    proc, timepts,\n    opt.quadratic_cost(proc, Qx, Qu),\n    trajectory_constraints=traj_constraints,\n    terminal_cost=opt.quadratic_cost(proc, P*10, None),\n)\nweight_list = [P1, P, P/10, P*10]\nocp_list = [ocp_orig, ocp_lqr, ocp_low, ocp_high]\n\n# Do a test run to figure out how long computation takes\nstart_time = time.process_time()\nocp_lqr.compute_trajectory(X0)\nstop_time = time.process_time()\nprint(\"* Process time: %0.2g s\\n\" % (stop_time - start_time))\n\n# Create a figure to use for plotting\nfig, [[ax_orig, ax_lqr], [ax_low, ax_high]] = plt.subplots(2, 2)\nax_list = [ax_orig, ax_lqr, ax_low, ax_high]\nax_name = ['orig', 'lqr', 'low', 'high']\n\n# Generate the individual traces for the MPC control\nfor ocp, ax, name, Pf in zip(ocp_list, ax_list, ax_name, weight_list):\n    x, t = X0, 0\n    for i in np.arange(0, Tf, proc.dt):\n        # Calculate the optimal trajectory\n        res = ocp.compute_trajectory(x, print_summary=False)\n        soln = ctl.input_output_response(proc, res.time, res.inputs, x)\n\n        # Plot the results for this time instant\n        ax.plot(res.time[:2] + t, res.inputs[0, :2], 'b-', linewidth=1)\n        ax.plot(res.time[:2] + t, soln.outputs[0, :2], 'k-', linewidth=1)\n\n        # Plot the results projected forward\n        ax.plot(res.time[1:] + t, res.inputs[0, 1:], 'b--', linewidth=0.75)\n        ax.plot(res.time[1:] + t, soln.outputs[0, 1:], 'k--', linewidth=0.75)\n\n        # Update the state to use for the next time point\n        x = soln.states[:, 1]\n        t += proc.dt\n\n    # Adjust the limits for consistency\n    ax.set_ylim([-1.5, 3.5])\n\n    # Label the results\n    ax.set_xlabel(\"Time $t$ [sec]\")\n    ax.set_ylabel(\"State $x_1$, input $u$\")\n    ax.set_title(f\"MPC response for {name}\")\n    plt.tight_layout()\n\n\n\n\n\nWe can also implement a MPC controller for a discrete time system using opt.create_mpc_iosystem.  This creates a controller that accepts the current state as the input and generates the control to apply from that state.\n\n# Construct using create_mpc_iosystem\nclsys = opt.create_mpc_iosystem(\n    proc, timepts, opt.quadratic_cost(proc, Qx, Qu), traj_constraints,\n    terminal_cost=opt.quadratic_cost(proc, P1, None),\n)\nprint(clsys)\n\n\n\n(This function needs some work to be more user-friendly, e.g. renaming of the inputs and outputs.)\n\n","type":"content","url":"/lab8-model-predictive-control#task-2-discrete-time-mpc","position":13},{"hierarchy":{"lvl1":"Lab8: Model Predictive Control","lvl2":"HW Problem"},"type":"lvl2","url":"/lab8-model-predictive-control#hw-problem","position":14},{"hierarchy":{"lvl1":"Lab8: Model Predictive Control","lvl2":"HW Problem"},"content":"","type":"content","url":"/lab8-model-predictive-control#hw-problem","position":15},{"hierarchy":{"lvl1":"Lab8: Model Predictive Control","lvl3":"Problem: MPC for kinematic car","lvl2":"HW Problem"},"type":"lvl3","url":"/lab8-model-predictive-control#problem-mpc-for-kinematic-car","position":16},{"hierarchy":{"lvl1":"Lab8: Model Predictive Control","lvl3":"Problem: MPC for kinematic car","lvl2":"HW Problem"},"content":"We take the state of the system as (x, y, \\theta) where (x, y) is the position of the vehicle in the plane and \\theta is the angle of the vehicle with respect to horizontal.  The vehicle input is given by (v, \\delta) where v is the forward velocity of the vehicle and \\delta is the angle of the steering wheel.  The model includes saturation of the vehicle steering angle.\\begin{aligned}\n  \\dot x &= \\cos\\theta\\, v \\\\\n  \\dot y &= \\sin\\theta\\, v \\\\\n  \\dot\\theta &= \\frac{v}{l} \\tan \\delta\n\\end{aligned}\n\nScenario: A vehicle needs to perform a lane change maneuver while traveling forward. The vehicle starts in the left lane (at y = -2 m) and needs to move to the right lane (at y = +2 m) over a distance of 100 m, completing the maneuver in 10 seconds. The vehicle should maintain a constant forward velocity of 10 m/s throughout the maneuver.\n\nInitial and Final Conditions:\n\nInitial state: x_0 = [0, -2, 0] (position: origin, left lane, heading: 0°)\n\nInitial input: u_0 = [10, 0] (velocity: 10 m/s, steering: 0 rad)\n\nFinal state: x_f = [100, 2, 0] (position: 100 m forward, right lane, heading: 0°)\n\nFinal input: u_f = [10, 0] (velocity: 10 m/s, steering: 0 rad)\n\nTotal maneuver time: T_f = 10 s\n\nBefore you perform the tasks below, first generate an open‑loop optimal trajectory x_d(t),\\, u_d(t) for the scenario above using Lab 7 (Approach 3: input cost + terminal constraints). We will use this planned trajectory as the reference to be tracked by the MPC controller in Task 1.\n\nTasks:\n\nFollowing the implementation structure used for the double integrator system in Tasks 1 and 2, implement MPC controllers for the kinematic car:\n\nContinuous-time MPC (Task 1 approach) — TRACK the given reference x_d(t), u_d(t) from Lab 7:\n\nBuild time-aligned interpolants for the reference over the prediction horizon:\n\nCreate x_d(t) and u_d(t) from your sampled Lab 7 trajectory using linear interpolation (first‑order hold). In Python, you can use scipy.interpolate.interp1d(..., kind='linear', axis=1) for states and inputs.\n\nDefine a tracking cost that penalizes deviation from the reference and control effort:\n\nTrajectory (integral) cost: \\int_0^{T_h} (x(t)-x_d(t))^T Q_x (x(t)-x_d(t)) + (u(t)-u_d(t))^T Q_u (u(t)-u_d(t)) \\, dt.\n\nTerminal cost: (x(T_h)-x_d(T_h))^T P (x(T_h)-x_d(T_h)).\n\nYou may need to use something like traj_cost = opt.quadratic_cost(proc, Qx, Qu, x0=x_d(t), u0=u_d(t))\n\nSet input constraints consistent with the model and scenario:\n\nSteering limit |\\delta| \\le \\delta_{\\max} (e.g., 0.5 rad). Optionally bound velocity around 10 m/s if desired.\n\nChoose a prediction horizon T_h=5s and a time grid timepts for the OCP (e.g., with 11 grid points).\n\nCreate an OptimalControlProblem using the tracking cost and constraints.\n\nImplement an MPC simulation loop (similar to run_mpc() from Task 1 for the double integrator).\n\nPlot and report:\n\nXY path: plot (x(t), y(t)) for the closed loop and overlay the desired path from Lab7.\n\nTime histories: plot x(t), y(t), \\theta(t) and inputs v(t), \\delta(t) versus their references.\n\nDiscrete-time MPC (Task 2 approach):\n\nDiscretize the kinematic car model with an appropriate sampling time (e.g., dt = 0.1 s)\n\nDefine discrete-time cost functions and constraints\n\nSet up the discrete-time optimal control problem\n\nImplement and simulate the discrete-time MPC controller\n\nCompare the continuous-time and discrete-time MPC results\n\n# Code to model vehicle steering dynamics\n\n# Function to compute the RHS of the system dynamics\ndef kincar_update(t, x, u, params):\n    # Get the parameters for the model\n    l = params['wheelbase']             # vehicle wheelbase\n    deltamax = params['maxsteer']         # max steering angle (rad)\n\n    # Saturate the steering input\n    delta = np.clip(u[1], -deltamax, deltamax)\n\n    # Return the derivative of the state\n    return np.array([\n        np.cos(x[2]) * u[0],            # xdot = cos(theta) v\n        np.sin(x[2]) * u[0],            # ydot = sin(theta) v\n        (u[0] / l) * np.tan(delta)      # thdot = v/l tan(delta)\n    ])\n\nkincar_params={'wheelbase': 3, 'maxsteer': 0.5}\n\n# Create nonlinear input/output system\nkincar = ctl.nlsys(\n    kincar_update, None, name=\"kincar\", params=kincar_params,\n    inputs=('v', 'delta'), outputs=('x', 'y', 'theta'),\n    states=('x', 'y', 'theta'))\n\n","type":"content","url":"/lab8-model-predictive-control#problem-mpc-for-kinematic-car","position":17},{"hierarchy":{"lvl1":"Lab9: RL: Value Iteration & Policy Iteration"},"type":"lvl1","url":"/lab9-value-iteration-policy-iteration","position":0},{"hierarchy":{"lvl1":"Lab9: RL: Value Iteration & Policy Iteration"},"content":"This lab will guide you through Value Iteration and Policy Iteration on a small Grid World. You will learn how to solve Markov Decision Processes (MDPs) using dynamic programming methods. Starting with a custom Grid World environment, you will implement and compare two fundamental RL algorithms: Value Iteration and Policy Iteration. Finally, you will adapt your solution to work with the standard Gymnasium API.\n\nWhat You Will Learn:\n\nTask 1: Grid World Setup\n\nDefine an MDP with states, actions, transition dynamics, rewards, and discount factor.\n\nUnderstand the GridWorld class structure and environment mechanics.\n\nVisualize the environment with forbidden cells, goal states, and policies.\n\nTask 2: Value Iteration\n\nImplement the Value Iteration algorithm using the Bellman optimality equation.\n\nPerform synchronous updates to compute optimal state values and policy.\n\nUnderstand convergence criteria and interpret the resulting value function and policy.\n\nTask 3: Policy Iteration\n\nImplement Policy Evaluation to compute the value function for a given policy.\n\nImplement Policy Improvement to update the policy greedily.\n\nCompare Policy Iteration with Value Iteration in terms of iterations and computational cost.\n\nTask 4: Using Gymnasium\n\nRewrite the Grid World environment following the Gymnasium API (reset(), step(), render()).\n\nUnderstand action spaces (Discrete) and observation spaces (MultiDiscrete).\n\nImplement Value Iteration for the Gymnasium-based environment and verify compatibility with standard RL frameworks.\n\n\n# Minimal prerequisites\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Matplotlib: one chart per cell; no specific styles or colors set per instructions.\nprint(\"NumPy:\", np.__version__)\nprint(\"Matplotlib:\", plt.matplotlib.__version__)\n\n\n\n","type":"content","url":"/lab9-value-iteration-policy-iteration","position":1},{"hierarchy":{"lvl1":"Lab9: RL: Value Iteration & Policy Iteration","lvl2":"Task 1. Grid World Setup"},"type":"lvl2","url":"/lab9-value-iteration-policy-iteration#task-1-grid-world-setup","position":2},{"hierarchy":{"lvl1":"Lab9: RL: Value Iteration & Policy Iteration","lvl2":"Task 1. Grid World Setup"},"content":"We use a small rectangular grid to represent the environment. Each cell in the grid corresponds to a state that the agent can occupy. At each state, the agent can attempt one of five possible actions: up, right, down, left, or stay.\n\nStates: A state is represented by a tuple (row, column), indicating the agent’s position. The grid is zero-indexed, so the top-left corner is (0, 0).\n\nActions: Actions are represented by integers from 0 to 4: 0 (up), 1 (right), 2 (down), 3 (left), and 4 (stay).\n\nTransitions: If an action moves the agent off the grid, the agent remains in its current state and receives a boundary_reward.\n\nRewards: The environment provides different rewards based on the outcome of an action:\n\ngoal_reward: For reaching the goal state.\n\nforbidden_reward: For entering a forbidden cell.\n\nboundary_reward: For hitting a wall or boundary.\n\nstep_reward: For any other valid move.\n\nDiscount Factor (gamma): A value between 0 and 1 that balances the importance of immediate rewards versus future rewards.\n\nThe GridWorld class encapsulates this entire setup, managing the grid, states, actions, transitions, and rewards. A class in Python is a blueprint for creating objects. It bundles data (attributes) and functions that operate on that data (methods) into a single, organized unit. An object created from a class is called an instance.\n\nIn contrast, a function is a standalone block of code that performs a specific task. While functions can take inputs and produce outputs, they don’t inherently store data or maintain a state between calls. A class, through its instances, can maintain its state (the values of its attributes) over time, making it ideal for representing complex entities like our GridWorld environment, which needs to keep track of its configuration (grid size, rewards, etc.) across multiple interactions.\n\n# Define actions and their visual representations\nACTIONS = [0, 1, 2, 3, 4]  # up, right, down, left, stay\nACTION_ARROWS = {0:\"↑\", 1:\"→\", 2:\"↓\", 3:\"←\", 4:\"*\"} # For rendering the policy\n\nclass GridWorld:\n    \"\"\"\n    A class to represent the Grid World environment.\n    Manages the grid, states, actions, transitions, and rewards.\n    \"\"\"\n    def __init__(self,\n                 grid_shape=(5, 5),  # number of rows and columns\n                 forbidden=None,\n                 goal=None,\n                 goal_reward=10.0,\n                 step_reward=0.0,\n                 forbidden_reward=-1.0,\n                 boundary_reward=-1.0,\n                 gamma=0.9):\n        \"\"\"\n        Initializes the GridWorld environment.\n\n        Args:\n            grid_shape (tuple): The (rows, columns) of the grid.\n            forbidden (list): A list of (row, col) tuples for forbidden states.\n            goal (tuple): The (row, col) of the goal state.\n            goal_reward (float): Reward for reaching the goal.\n            step_reward (float): Reward for a standard step.\n            forbidden_reward (float): Penalty for entering a forbidden state.\n            boundary_reward (float): Penalty for hitting a wall or boundary.\n            gamma (float): The discount factor for future rewards.\n        \"\"\"\n        self.grid_shape = grid_shape\n        # Default locations for forbidden cells and the goal if not provided\n        self.forbidden = forbidden if forbidden is not None else [(1,1), (1,2),(2,2), (3,1), (3,3) ,(4,1)] # Example forbidden positions\n        self.goal = goal if goal is not None else (3,2) # Example goal position\n\n        # Reward structure\n        self.goal_reward = goal_reward\n        self.step_reward = step_reward\n        self.forbidden_reward = forbidden_reward\n        self.boundary_reward = boundary_reward\n\n        # Discount factor\n        self.gamma = gamma\n\n    def in_bounds(self, s):\n        \"\"\"Check if a state 's' is within the grid boundaries.\"\"\"\n        r, c = s # r is the row, c is the column\n        R, C = self.grid_shape # Total rows and columns\n        return 0 <= r < R and 0 <= c < C # if within bounds then return True\n\n    def is_forbidden(self, s):\n        \"\"\"Check if a state 's' is a forbidden cell.\"\"\"\n        return s in self.forbidden # if s is in forbidden list then return True\n\n    def states(self):\n        \"\"\"Returns a list of all states in the grid.\"\"\"\n        R, C = self.grid_shape\n        return [(r, c) for r in range(R) for c in range(C)]\n\n    def step(self, s, a):\n        \"\"\"\n        Performs a state transition based on the current state and action.\n        This function defines the environment's dynamics.\n\n        Args:\n            s (tuple): The current state (row, col).\n            a (int): The action to take.\n\n        Returns:\n            tuple: A tuple containing the next state (next_s) and the reward.\n        \"\"\"\n        assert a in ACTIONS, f\"Invalid action: {a}. Valid actions are: {ACTIONS}\"\n\n        r, c = s\n        # Determine the next potential position based on the action\n        if a == 0:    nr, nc = r-1, c    # up\n        elif a == 1:  nr, nc = r, c+1    # right\n        elif a == 2:  nr, nc = r+1, c    # down\n        elif a == 3:  nr, nc = r, c-1    # left\n        else:         nr, nc = r, c      # stay\n\n        # Check for boundary collisions or walls\n        if (not self.in_bounds((nr, nc))):\n            next_s = s  # Agent stays in the same state\n            reward = self.boundary_reward\n        else:\n            # The move is valid, so the agent moves to the new position\n            next_s = (nr, nc)\n            # Determine the reward based on the new state\n            if next_s == self.goal:\n                reward = self.goal_reward\n            elif self.is_forbidden(next_s):\n                reward = self.forbidden_reward\n            else:\n                reward = self.step_reward\n\n        return next_s, reward\n\n\n\n","type":"content","url":"/lab9-value-iteration-policy-iteration#task-1-grid-world-setup","position":3},{"hierarchy":{"lvl1":"Lab9: RL: Value Iteration & Policy Iteration","lvl3":"Visualization helpers","lvl2":"Task 1. Grid World Setup"},"type":"lvl3","url":"/lab9-value-iteration-policy-iteration#visualization-helpers","position":4},{"hierarchy":{"lvl1":"Lab9: RL: Value Iteration & Policy Iteration","lvl3":"Visualization helpers","lvl2":"Task 1. Grid World Setup"},"content":"plot_values(V): Creates a heatmap to visualize the value of each state. Higher values are typically shown in brighter colors, providing an intuitive overview of which states are more desirable for the agent.\n\nrender_policy(P): table of arrows showing the greedy action per state.\n\nACTION_ARROWS = {\n    0: \"↑\",   # down\n    1: \"→\",   # right\n    2: \"↓\",   # up\n    3: \"←\",   # left\n    4: \"•\",   # stay\n}\n\ndef render_policy_with_values(env, P, V=None, title=\"Policy and Values\", ax=None, show=True):\n    \"\"\"\n    Renders the greedy policy as arrows and overlays the value for each state in a matplotlib plot.\n\n    Args:\n        env: GridWorld environment object. Should provide .grid_shape and .is_forbidden().\n        P (dict): The policy mapping states (r, c) to actions (int).\n        V (dict): Optional. A dictionary mapping states to their values.\n        title (str): Title of the plot.\n        ax: Optional matplotlib axes object. If provided, plots on this axes instead of creating a new figure.\n        show (bool): Whether to call plt.show() at the end. Default True. Set False when creating subplots.\n\n    Returns:\n        tuple: (fig, ax) - The figure and axes objects.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    R, C = env.grid_shape\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(7, 6))\n    else:\n        fig = ax.figure\n    ax.set_xlim(-0.5, C - 0.5)\n    ax.set_ylim(-0.5, R - 0.5)\n    ax.set_xticks(np.arange(-0.5, C, 1))\n    ax.set_yticks(np.arange(-0.5, R, 1))\n    ax.grid(True, color='gray', linewidth=1, linestyle='-')\n    ax.set_aspect('equal')\n    ax.invert_yaxis()  # Match array indexing\n\n    # Draw cell highlights for forbidden/goal/start\n    for r in range(R):\n        for c in range(C):\n            s = (r, c)\n            if env.is_forbidden(s):\n                rect = plt.Rectangle((c-0.5, r-0.5), 1, 1, color='gray', alpha=0.5)\n                ax.add_patch(rect)\n                ax.text(c, r, \"X\", ha='center', va='center', color='black', fontsize=14)\n            elif s == env.goal:\n                rect = plt.Rectangle((c-0.5, r-0.5), 1, 1, color='yellow', alpha=0.45)\n                ax.add_patch(rect)\n                ax.text(c, r, \"G\", ha='center', va='center', color='black', fontsize=14)\n            elif s == getattr(env, \"start_state\", None):\n                rect = plt.Rectangle((c-0.5, r-0.5), 1, 1, color='deepskyblue', alpha=0.35)\n                ax.add_patch(rect)\n                ax.text(c, r, \"S\", ha='center', va='center', color='black', fontsize=14)\n\n    # Print arrows and values in every cell (except forbidden)\n    for r in range(R):\n        for c in range(C):\n            s = (r, c)\n            # Print policy arrow\n            if (r, c) in P:\n                arrow = ACTION_ARROWS[P[(r, c)]]\n            else:\n                arrow = \"\"\n            if (V is not None) and ((r, c) in V):\n                valstr = f\"{V[(r, c)]:.2f}\"\n            else:\n                valstr = \"\"\n            # Place arrow in cell\n            ax.text(\n                c, r-0.18, arrow,\n                ha='center', va='center', color='darkblue', fontsize=22, fontweight='bold', zorder=10\n            )\n            # Always show value in each cell (if provided), not just if valstr != \"\"\n            ax.text(\n                c, r+0.27, valstr if valstr != \"\" else \"\",\n                ha='center', va='center', color='crimson', fontsize=11, alpha=0.99,\n                bbox=dict(facecolor=\"white\", alpha=0.63, edgecolor='none', boxstyle='round,pad=0.16'),\n                zorder=10\n            )\n\n    # Label grid\n    for x in range(C):\n        ax.text(x, -0.75, str(x+1), ha='center', va='center', color='black')\n    for y in range(R):\n        ax.text(-0.75, y, str(y+1), ha='center', va='center', color='black')\n\n    ax.set_title(title)\n    ax.tick_params(bottom=False, left=False, right=False, top=False,\n                   labelbottom=False, labelleft=False, labeltop=False, labelright=False)\n    if show:\n        plt.tight_layout()\n        plt.show()\n\n    return fig, ax\n\n\n\n","type":"content","url":"/lab9-value-iteration-policy-iteration#visualization-helpers","position":5},{"hierarchy":{"lvl1":"Lab9: RL: Value Iteration & Policy Iteration","lvl2":"Task 2. Value Iteration"},"type":"lvl2","url":"/lab9-value-iteration-policy-iteration#task-2-value-iteration","position":6},{"hierarchy":{"lvl1":"Lab9: RL: Value Iteration & Policy Iteration","lvl2":"Task 2. Value Iteration"},"content":"We first implement value iteration to find the optimal value function (v^*) and the optimal policy (\\pi^*). The value function v^*(s) tells us the maximum expected future return an agent can get starting from state s. The policy \\pi^*(s) tells us the best action to take in state s.\n\nThese are found by solving the Bellman Optimality Equation. For our deterministic Grid World, this equation simplifies to:v^*(s) = \\max_{a \\in A} \\Big[ r(s,a) + \\gamma v^*(s') \\Big]\n\nwhere:\n\ns is the current state.\n\na is a possible action.\n\ns' is the state you land in after taking action a from state s.\n\nr(s,a) is the immediate reward.\n\n\\gamma (gamma) is the discount factor, which balances immediate vs. future rewards.","type":"content","url":"/lab9-value-iteration-policy-iteration#task-2-value-iteration","position":7},{"hierarchy":{"lvl1":"Lab9: RL: Value Iteration & Policy Iteration","lvl3":"Algorithm","lvl2":"Task 2. Value Iteration"},"type":"lvl3","url":"/lab9-value-iteration-policy-iteration#algorithm","position":8},{"hierarchy":{"lvl1":"Lab9: RL: Value Iteration & Policy Iteration","lvl3":"Algorithm","lvl2":"Task 2. Value Iteration"},"content":"Value Iteration finds the optimal value function and policy by repeatedly applying the Bellman optimality equation. The algorithm iterates, updating both the value estimates and the policy at each step until convergence.\n\n1. Initialization\n\nInitialize an arbitrary value function v_0(s) for all states s (e.g., all zeros).\n\nChoose a small positive number theta (\\theta) to check for convergence.\n\n2. Iteration and Synchronous Updates\n\nRepeat for each iteration k=0, 1, 2, \\dots until convergence:\n\nFor every state s \\in S:\n\na. Compute Action-Values (q-values): For each possible action a, calculate its value using the value function from the previous iteration, v_k:\nq_k(s,a) = r(s,a) + \\gamma v_k(s')\n\nb. Find Best Action: Identify the best action a_k^{*}(s) that maximizes the q-value:\na_k^{*}(s) = \\arg\\max_{a} q_k(s,a)\n\nc. Update Policy: Update the policy for the next iteration, \\pi_{k+1}, to be greedy with respect to the q-values:\n\\pi_{k+1}(s) = a_k^*(s)\n\nd. Update State-Value: Update the state-value for the next iteration, v_{k+1}, with the value of the best action:\nv_{k+1}(s) = \\max_{a} q_k(s,a)\n\n3. Convergence Check\n\nThe algorithm has converged when the largest change in any state’s value is less than theta:\\max_{s \\in S} |v_{k+1}(s) - v_k(s)| < \\theta\n\nOnce converged, v_{k+1} and \\pi_{k+1} are the optimal value function v^* and optimal policy \\pi^*.\n\ndef value_iteration(env, theta, max_iters):\n    \"\"\"\n    Performs Value Iteration to find the optimal value function and policy,\n    following the structure of the provided pseudocode.\n\n    Args:\n        env: The GridWorld environment.\n        theta (float): A small threshold for determining convergence.\n        max_iters (int): The maximum number of iterations to prevent infinite loops.\n\n    Returns:\n        tuple: A tuple containing:\n            - V (dict): The optimal value function, mapping states to values.\n            - Pi (dict): The optimal policy, mapping states to actions.\n            - iters (int): The number of iterations until convergence.\n    \"\"\"\n    # Initialization: Initial guess v0\n    V = {s: 0.0 for s in env.states()}\n    Pi = {s: 0 for s in env.states()} # Policy can be initialized arbitrarily\n    iters = 0\n\n    while True:\n        delta = 0.0\n        V_old = V.copy() # Store vk to calculate delta = ||vk - vk-1||\n\n        # For every state s ∈ S, do\n        for s in env.states():\n            # For every action a ∈ A(s), do\n            # q-value: qk(s, a) = r(s,a) + γ * V(s')\n            q_values = {}\n            for a in ACTIONS:\n                s_next, r = env.step(s, a) # Get next state and reward\n                q_values[a] = r + env.gamma * V_old[s_next] # get the action value\n\n            # Maximum action value: a*k(s) = arg maxa qk(a, s)\n            best_action = max(q_values, key=q_values.get)\n\n            # Value update: vk+1(s) = maxa qk(a, s)\n            best_value = q_values[best_action]\n            V[s] = best_value\n\n            # Policy update: πk+1(s) = a*k(s)\n            Pi[s] = best_action\n\n            # Check for convergence\n            delta = max(delta, abs(V[s] - V_old[s]))\n\n        iters += 1\n        if delta < theta or iters >= max_iters:\n            break\n\n        # Show intermediate policy and values during value iteration\n        # if iters % 20 == 0 or delta < theta:  # Visualize every 10 steps and at convergence\n        #     render_policy_with_values(env, Pi, V, title=f\"Value Iteration Step {iters}\")\n        #     import time\n        #     time.sleep(1)\n        #     plt.close('all')  # Close all existing figures before drawing a new one\n    return V, Pi, iters\n\n# Demo run\nenv = GridWorld()\nV_vi, Pi_vi, n_vi = value_iteration(env, 1e-3, 1000)\nprint(f\"Value Iteration converged in {n_vi} iterations.\")\nrender_policy_with_values(env, Pi_vi, V_vi, title=\"Policy and Values\")\n\n\n\n\n\n\n\n","type":"content","url":"/lab9-value-iteration-policy-iteration#algorithm","position":9},{"hierarchy":{"lvl1":"Lab9: RL: Value Iteration & Policy Iteration","lvl2":"Task 3. Policy Iteration"},"type":"lvl2","url":"/lab9-value-iteration-policy-iteration#task-3-policy-iteration","position":10},{"hierarchy":{"lvl1":"Lab9: RL: Value Iteration & Policy Iteration","lvl2":"Task 3. Policy Iteration"},"content":"We then implement policy interation to find the optimal policy \\pi^* and its corresponding value function v^* through an iterative process that alternates between two main steps:\n\nPolicy Evaluation: For a given policy \\pi, calculate the state-value function v_\\pi. This step determines the expected long-term return for each state, assuming the agent follows the current policy \\pi.\n\nPolicy Improvement: Using the value function v_\\pi, find a new, better policy \\pi'. This is done by acting greedily with respect to v_\\pi—that is, for each state, selecting the action that leads to the highest expected return.\n\nThis cycle of evaluation and improvement continues until the policy no longer changes, at which point it has converged to the optimal policy.","type":"content","url":"/lab9-value-iteration-policy-iteration#task-3-policy-iteration","position":11},{"hierarchy":{"lvl1":"Lab9: RL: Value Iteration & Policy Iteration","lvl3":"Algorithm","lvl2":"Task 3. Policy Iteration"},"type":"lvl3","url":"/lab9-value-iteration-policy-iteration#algorithm-1","position":12},{"hierarchy":{"lvl1":"Lab9: RL: Value Iteration & Policy Iteration","lvl3":"Algorithm","lvl2":"Task 3. Policy Iteration"},"content":"1. Initialization\n\nInitialize a policy \\pi_0(s) arbitrarily for all states s \\in S.\n\nInitialize the state-value function v(s) arbitrarily (e.g., zeros).\n\nSet iteration counter k = 0.\n\n2. Policy Evaluation\n\nGiven the current policy \\pi_k, compute its value function v^{\\pi_k} by solving the Bellman equation:v_{\\pi_k}(s)\n= \\sum_a \\pi_k(a|s)\n  \\Big[\n    r(s,a)\n    + \\gamma \\sum_{s'} p(s'|s,a)\\, v_{\\pi_k}(s')\n  \\Big].\n\nWe usually solve it iteratively until convergence:v_{i+1}(s)\n\\leftarrow\n\\sum_a \\pi_k(a|s)\n\\Big[\n  r(s,a)\n  + \\gamma \\sum_{s'} p(s'|s,a)\\, v_i(s')\n\\Big].\n\nStop when \\|v_{i+1} - v_i\\|_\\infty < \\theta or when i is large than a given value.\n\n3. Policy Improvement\n\nUpdate the policy greedily with respect to v^{\\pi_k}:\\pi_{k+1}(s)\n= \\arg\\max_a\n\\Big[\n  r(s,a)\n  + \\gamma \\sum_{s'} p(s'|s,a)\\, v^{\\pi_k}(s')\n\\Big].\n\n4. Termination Check\n\nIf \\pi_{k+1} = \\pi_k (i.e., policy no longer changes),then the algorithm has converged:\\pi^* = \\pi_k,\\quad v^* = v^{\\pi_k}.\n\nOtherwise, set k \\leftarrow k+1 and repeat.\n\ndef policy_evaluation(env, Pi, theta, max_iters):\n    \"\"\"\n    Calculates the state-value function V for a given policy Pi.\n    This is an iterative process that applies the Bellman equation until V converges.\n\n    Args:\n        env (GridWorld): The environment.\n        Pi (dict): The policy to evaluate, mapping states to actions.\n        theta (float): Convergence threshold. The loop stops when the max change in V is less than this.\n        max_iters (int): Maximum number of iterations to prevent infinite loops.\n\n    Returns:\n        tuple: A tuple containing:\n            - V (dict): The converged state-value function for the policy Pi.\n            - iters (int): The number of iterations it took to converge.\n    \"\"\"\n    # Initialize the value function V(s) = 0 for all states s.\n    V = {s: 0.0 for s in env.states()}\n\n    iters = 0\n    while True:\n        delta = 0.0  # Tracks the maximum change in V in a single sweep.\n        V_old = V.copy() # Store the old value function for comparison.\n\n        # Loop through each state to update its value.\n        for s in env.states():\n            # Get the action prescribed by the current policy for state s.\n            a = Pi[s]\n\n            # Get the next state and reward for the (s, a) pair.\n            s_next, r = env.step(s, a)\n\n            # Apply the Bellman equation for the given policy:\n            # V(s) = R(s, a) + gamma * V(s_next)\n            # This calculates the expected return from state s by following policy Pi.\n            val = r + env.gamma * V_old[s_next]\n\n            # Update the value for the current state.\n            V[s] = val\n\n            # Check for convergence by tracking the largest change in V.\n            delta = max(delta, abs(val - V_old[s]))\n\n        iters += 1\n        # If the value function has stabilized, or we've hit the max iterations, stop.\n        if delta < theta or iters >= max_iters:\n            break\n\n    return V, iters\n\ndef policy_improvement(env, V):\n    \"\"\"\n    Improves a policy by making it greedy with respect to a given value function V.\n\n    Args:\n        env (GridWorld): The environment.\n        V (dict): The value function to be greedy upon.\n\n    Returns:\n        dict: The new, improved policy.\n    \"\"\"\n    Pi_new = {} # Initialize the new policy.\n\n    # Iterate over all states to find the best action for each.\n    for s in env.states():\n        q_values = {}\n        # For each state, calculate the Q-value for all possible actions.\n        for a in ACTIONS:\n            s_next, r = env.step(s, a)\n            # Q(s, a) = R(s, a) + gamma * V(s_next)\n            q_values[a] = r + env.gamma * V[s_next]\n\n        # The new policy for state s will be the action that maximizes the Q-value.\n        best_a = max(q_values, key=q_values.get)\n        Pi_new[s] = best_a\n\n    return Pi_new\n\ndef policy_iteration(env, theta, max_iters):\n    \"\"\"\n    Performs Policy Iteration to find the optimal policy and value function.\n    It alternates between Policy Evaluation and Policy Improvement until the policy converges.\n\n    Args:\n        env (GridWorld): The environment.\n        theta (float): Convergence threshold for policy evaluation.\n        max_iters (int): Maximum number of policy improvement steps.\n\n    Returns:\n        tuple: A tuple containing:\n            - V (dict): The optimal value function.\n            - Pi (dict): The optimal policy.\n            - iteration (int): The number of policy improvement steps.\n    \"\"\"\n\n    # Start with a random initial policy.\n    # For each state, choose a random action from the set of possible actions.\n    Pi = {s: np.random.choice(ACTIONS) for s in env.states()}\n    iteration = 0\n\n    while True:\n        # Step 1: Policy Evaluation\n        V, _ = policy_evaluation(env, Pi, theta, max_iters)\n\n        # Step 2: Policy Improvement\n        Pi_new = policy_improvement(env, V)\n        iteration += 1\n\n        # Step 3: Termination Check\n        if Pi_new == Pi or iteration >= max_iters:\n            break\n\n        # Update the policy for the next iteration of the main loop.\n        Pi = Pi_new\n\n    # Return the final converged value function, policy, and number of iterations.\n    return V, Pi_new, iteration\n\n# --- Demo Run ---\n# Create a new GridWorld instance for this demonstration.\nenv2 = GridWorld()\n\n# Run the Policy Iteration algorithm.\nV_pi, Pi_pi, iteration = policy_iteration(env2, theta=1e-5, max_iters=1000)\n\n# Print the results and visualize the value function and policy.\nprint(f\"Policy Iteration converged in {iteration} improvement steps.\")\nrender_policy_with_values(env, Pi_pi, V_pi, title=\"Policy and Values\")\n\n\n\n\n\n\n\n","type":"content","url":"/lab9-value-iteration-policy-iteration#algorithm-1","position":13},{"hierarchy":{"lvl1":"Lab9: RL: Value Iteration & Policy Iteration","lvl4":"Comparing the Algorithms","lvl3":"Algorithm","lvl2":"Task 3. Policy Iteration"},"type":"lvl4","url":"/lab9-value-iteration-policy-iteration#comparing-the-algorithms","position":14},{"hierarchy":{"lvl1":"Lab9: RL: Value Iteration & Policy Iteration","lvl4":"Comparing the Algorithms","lvl3":"Algorithm","lvl2":"Task 3. Policy Iteration"},"content":"Value Iteration:\n\nPerforms a single Bellman optimality backup for every state in each iteration. This step simultaneously improves the value estimate and the implicit policy.\n\nEach iteration is computationally fast, involving a single sweep through the state space.\n\nIt often requires a larger number of iterations to converge compared to the number of policy improvement steps in Policy Iteration.\n\nPolicy Iteration:\n\nAlternates between two distinct steps:\n\nPolicy Evaluation: Fully computes the value function for the current policy until it converges. This step can be computationally expensive.\n\nPolicy Improvement: Updates the policy to be greedy with respect to the newly computed value function.\n\nIt typically converges in fewer overall iterations (policy improvement steps), but each iteration is more computationally intensive due to the full policy evaluation.\n\n# Quick side-by-side run for the same environment:\nenv_cmp = GridWorld(gamma=0.9)\n\nV_vi, Pi_vi, n_vi = value_iteration(env_cmp, theta=1e-5, max_iters=1000)\nV_pi, Pi_pi, n_pi = policy_iteration(env_cmp, theta=1e-5, max_iters=1000)\n\nprint(f\"Value Iteration: {n_vi} iterations\")\nprint(f\"Policy Iteration: {n_pi} iterations\")\n\nimport matplotlib.pyplot as plt\n\n# Create a figure with two subplots side by side\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Plot Policy from Value Iteration (left subplot)\nrender_policy_with_values(env_cmp, Pi_vi, V_vi, title=\"Value Iteration\", ax=axes[0], show=False)\n\n# Plot Policy from Policy Iteration (right subplot)\nrender_policy_with_values(env_cmp, Pi_pi, V_pi, title=\"Policy Iteration\", ax=axes[1], show=False)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n","type":"content","url":"/lab9-value-iteration-policy-iteration#comparing-the-algorithms","position":15},{"hierarchy":{"lvl1":"Lab9: RL: Value Iteration & Policy Iteration","lvl2":"Task 4: Using Gymansium"},"type":"lvl2","url":"/lab9-value-iteration-policy-iteration#task-4-using-gymansium","position":16},{"hierarchy":{"lvl1":"Lab9: RL: Value Iteration & Policy Iteration","lvl2":"Task 4: Using Gymansium"},"content":"We now re-write the grid world environment following the Gymnasium API. Gymnasium provides a standardized API (reset(), step(), render()) that allows seamless integration with popular RL libraries such as Stable-Baselines3. As one of the most widely used frameworks in reinforcement learning, Gymnasium offers a common interface shared by countless research projects and benchmarks. Using it ensures that our environment is compatible, reproducible, and easy to compare with other RL studies.\n\nThe Gymnasium API is built around a few core components that define the interaction between an agent and its environment.\n\ngym.make(id): This function creates and initializes an environment specified by its id. It’s the standard entry point for loading any of the built-in or custom-registered environments.\n\nenv.reset(): This method resets the environment to a valid starting state and returns the initial observation. This is called at the beginning of every new episode.\n\nenv.step(action): This is the most important method. It executes the agent’s chosen action for one time-step. In return, it provides a tuple of five crucial pieces of information: the next observation, the reward received, a boolean flag indicating if the episode has terminated (e.g., the goal was reached), a boolean flag indicating if the episode was truncated (e.g., a time limit was hit), and a dictionary containing diagnostic info.\n\nenv.render(): This method generates a visualization of the environment’s current state. It’s invaluable for watching your agent perform during evaluation or for debugging your code.\n\nDetails can be found at \n\nBasic Usage of Gym.\n\nIn Gymnasium, an action space defines what moves or commands an agent can take — for example, in GridWorld, actions are discrete choices such as up, right, down, left, or stay.\nThe observation space describes what the agent perceives about the environment at each step — in this case, it is the agent’s position on the grid, represented by row and column indices.\n\nA Discrete action space means the agent selects one action from a finite set of integers. Discrete(4) means the action can choose from five consecutive integers starting from 0, that is 0~4.  In our grid world environment, 0, 1, 2 , 3, 4 means up, right, down, left, stay, respectively.\nA MultiDiscrete observation space means the observation or state has multiple integer components, each with its own range — for instance, Discrete([4,4]) means both values can choose from five integers [0~4, 0~4]. In our grid world environment, the two numbers represent the row and column indices of the cell position. For instance, [2,3] means the cell position at row 2 and column 3.\n\nThe function tuple(s_next) converts the NumPy array returned by the environment (like [2, 3]) into a tuple (2, 3) so it can be used as a dictionary key when storing or retrieving state values.\n\ntry:\n    import gymnasium as gym\nexcept ModuleNotFoundError:\n    import sys\n    !{sys.executable} -m pip install gymnasium\n    import gymnasium as gym\nfrom gymnasium import spaces # spaces are used to define action and observation spaces\nimport numpy as np\n\n# Define the set of possible actions and their visual representations\nACTIONS = [0, 1, 2, 3, 4]  # up, right, down, left, stay\nACTION_ARROWS = {0:\"↑\", 1:\"→\", 2:\"↓\", 3:\"←\", 4:\"*\"}\n\n\nclass GridWorldEnv(gym.Env):\n    \"\"\"Custom GridWorld environment compatible with Gymnasium API.\"\"\"\n    \"\"\"Note that Gymnasium environments are typically implemented as classes that inherit from gym.Env.\n    This allows them to integrate seamlessly with the Gymnasium framework,\n    which provides tools for reinforcement learning research and development.\"\"\"\n    \"\"\"You may compare this class with the one implemented at the beginning as a simple GridWorld class.\n    The only difference is that this class follows the Gymnasium API requirements: it defines action_space and observation_space,\n    and implements reset() and step() methods.\"\"\"\n\n    def __init__(self,\n                 grid_shape=(5, 5),\n                 forbidden=None,\n                 goal=None,\n                 goal_reward=1.0,\n                 step_reward=0.0,\n                 forbidden_reward=-10.0,\n                 boundary_reward=-1.0,\n                 gamma=0.9,\n                 render_mode=None):\n        \"\"\"\n        Initializes the GridWorld environment.\n\n        Args:\n            grid_shape (tuple): The shape of the grid (rows, columns).\n            forbidden (list of tuples): A list of coordinates for forbidden states.\n            goal (tuple): The coordinates of the goal state.\n            goal_reward (float): The reward for reaching the goal.\n            step_reward (float): The reward for a regular step.\n            forbidden_reward (float): The reward for entering a forbidden state.\n            boundary_reward (float): The reward for hitting a boundary.\n            gamma (float): The discount factor for future rewards.\n            render_mode (str, optional): The mode for rendering the environment.\n        \"\"\"\n        super().__init__()\n\n        # Environment parameters\n        self.grid_shape = grid_shape\n        self.forbidden = forbidden if forbidden is not None else [(2,2)]\n        self.goal = goal if goal is not None else (4,4)\n        self.goal_reward = goal_reward\n        self.step_reward = step_reward\n        self.forbidden_reward = forbidden_reward\n        self.boundary_reward = boundary_reward\n        self.gamma = gamma\n        self.render_mode = render_mode\n\n        # Agent's current position, initialized in reset()\n        self.agent_pos = None\n\n        # Define Gymnasium spaces\n        # The action space is discrete with a size equal to the number of actions.\n        self.action_space = spaces.Discrete(len(ACTIONS))\n        # The observation space represents the agent's position on the grid.\n        self.observation_space = spaces.MultiDiscrete(list(grid_shape))\n\n    # -----------------------------------\n    # Helper functions\n    # -----------------------------------\n    def in_bounds(self, s):\n        \"\"\"Check if a state 's' is within the grid boundaries.\"\"\"\n        r, c = s\n        R, C = self.grid_shape\n        return 0 <= r < R and 0 <= c < C\n\n    def is_forbidden(self, s):\n        \"\"\"Check if a state 's' is a forbidden state.\"\"\"\n        return s in self.forbidden\n\n    def states(self):\n        \"\"\"Return a list of all possible states (coordinates) in the grid.\"\"\"\n        valid_states = []\n        R, C = self.grid_shape\n        for r in range(R):\n            for c in range(C):\n                valid_states.append((r,c))\n        return valid_states\n\n    # -----------------------------------\n    # Gymnasium required methods\n    # -----------------------------------\n\n    def reset(self, seed=None, options=None):\n        \"\"\"\n        Reset the environment to its initial state.\n        The agent is placed at the starting position (0,0).\n        Returns the initial observation and an info dictionary.\n        \"\"\"\n        super().reset(seed=seed)\n        self.agent_pos = (0, 0)   # Always start in the top-left corner\n        # The observation is the agent's position as a numpy array.\n        obs = np.array(self.agent_pos, dtype=np.int32)\n        return obs, {}\n\n    def step(self, action):\n        \"\"\"\n        Execute one time step in the environment.\n        The agent takes an action, and the environment transitions to a new state.\n\n        Args:\n            action (int): The action to be taken by the agent.\n\n        Returns:\n            tuple: A tuple containing the new observation, the reward,\n                   a boolean indicating if the episode is terminated,\n                   a boolean indicating if the episode is truncated,\n                   and an info dictionary.\n        \"\"\"\n        assert self.action_space.contains(action), f\"Invalid action {action}\"\n\n        r, c = self.agent_pos\n\n        # Map the action to a new position (nr, nc for new row, new col)\n        if action == 0:    nr, nc = r - 1, c     # up\n        elif action == 1:  nr, nc = r, c + 1     # right\n        elif action == 2:  nr, nc = r + 1, c     # down\n        elif action == 3:  nr, nc = r, c - 1     # left\n        else:              nr, nc = r, c         # stay\n\n        # Check for environment boundaries\n        if not self.in_bounds((nr, nc)):\n            next_pos = self.agent_pos  # Agent stays in the same position\n            reward = self.boundary_reward\n        else:\n            # The new position is valid\n            next_pos = (nr, nc)\n            # Assign reward based on the new state\n            if next_pos == self.goal:\n                reward = self.goal_reward\n            elif self.is_forbidden(next_pos):\n                reward = self.forbidden_reward\n            else:\n                reward = self.step_reward\n\n        # Update the agent's position\n        self.agent_pos = next_pos\n\n        # Prepare the return values\n        obs = np.array(self.agent_pos, dtype=np.int32)\n\n        # In this simple environment, an episode never terminates or gets truncated on its own.\n        # This would be different in environments with terminal states or time limits.\n        terminated, truncated, info = False, False, {}\n\n        return obs, reward, terminated, truncated, info\n\n\n\nNow, let’s implement the value iteration algorithm based on this Gymnaiusm-style grid world environment. The policy iteration algorithm is left as an exercise.\n\ndef value_iteration_gym(env, theta, max_iters):\n    \"\"\"\n    Performs Value Iteration for a Gymnasium-based GridWorld environment.\n    This implementation is similar to the one in Task 2.\n\n    Args:\n        env (GridWorldEnv): The Gymnasium-style GridWorld environment.\n        theta (float): A small threshold for determining convergence.\n        max_iters (int): The maximum number of iterations.\n\n    Returns:\n        tuple: A tuple containing:\n            - V (dict): The optimal value function.\n            - Pi (dict): The optimal policy.\n            - iters (int): The number of iterations until convergence.\n    \"\"\"\n    # Initialization\n    states = env.states()\n    V = {s: 0.0 for s in states}\n    Pi = {s: 0 for s in states}  # Arbitrary initial policy\n    iters = 0\n\n    while True:\n        delta = 0.0\n        V_old = V.copy()  # Store the value function from the previous iteration\n\n        # Loop over all states\n        for s in states:\n            q_values = {}\n            # Calculate the q-value for each action in the current state\n            for a in ACTIONS:\n                # The Gym env is stateful, so we need to set its position to the state 's' we are evaluating\n                env.agent_pos = s\n                # The step function returns the next state and reward\n                s_next_arr, r, _, _, _ = env.step(a)\n                s_next = tuple(s_next_arr) # Convert numpy array to tuple for dict key\n\n                # Bellman update using the value function from the *previous* iteration\n                q_values[a] = r + env.gamma * V_old[s_next]\n\n            # Find the best action and its value\n            best_action = max(q_values, key=q_values.get)\n            best_value = q_values[best_action]\n\n            # Update the value function and policy for the current state\n            V[s] = best_value\n            Pi[s] = best_action\n\n            # Track the maximum change in the value function for convergence check\n            delta = max(delta, abs(V[s] - V_old[s]))\n\n        iters += 1\n        # Check for convergence\n        if delta < theta or iters >= max_iters:\n            break\n\n    return V, Pi, iters\n\n# ---- Demo Run ----\nenv = GridWorldEnv()\nV_vi, Pi_vi, n_vi = value_iteration_gym(env, theta=1e-3, max_iters=1000)\nprint(f\"Value Iteration converged in {n_vi} iterations.\")\nrender_policy_with_values(env, Pi_vi, V_vi, title=\"Policy and Values\")\n\n\n\n\n\n\n\n","type":"content","url":"/lab9-value-iteration-policy-iteration#task-4-using-gymansium","position":17},{"hierarchy":{"lvl1":"Lab9: RL: Value Iteration & Policy Iteration","lvl2":"HW Problems"},"type":"lvl2","url":"/lab9-value-iteration-policy-iteration#hw-problems","position":18},{"hierarchy":{"lvl1":"Lab9: RL: Value Iteration & Policy Iteration","lvl2":"HW Problems"},"content":"Value Iteration Manual derivation: Consider a 2×2 grid world with the following configuration:\n\n\n\n0\n\n1\n\n0\n\n[   ]\n\n[ G ]\n\n1\n\n[   ]\n\n[ X ]\n\nGoal state: (0,1) - marked with “G”\n\nForbidden state: (1,1) - marked with “X”\n\nRewards:\n\ngoal_reward = 10.0 (for reaching the goal)\n\nstep_reward = -0.1 (small penalty for each step)\n\nforbidden_reward = -5.0 (penalty for entering forbidden state)\n\nboundary_reward = -1.0 (penalty for hitting boundary)\n\nDiscount factor: gamma = 0.9\n\nAssume we have five actions for each state similar to the example we discussed in lectures. Manually perform Value Iteration to find the optimal value function v(s) and optimal policy \\pi(s) for all states. Show the details steps.\n\nPolicy Iteration Manual derivation: consider a 1x2 grid world with the following configuration:\n\n\n\n0\n\n1\n\n0\n\n[ G ]\n\n[  ]\n\nGoal state: (0,0) - marked with “G”\n\nRewards:\n\ngoal_reward = 5.0 (for reaching the goal)\n\nstep_reward = -0.1 (small penalty for each step)\n\nboundary_reward = -1.0 (penalty for hitting boundary)\n\nDiscount factor: gamma = 0.9\n\nAssume we have three actions a_l, a_0, and a_r for moving left, stay, and moving right. Manually perform Policy Iteration to find the optimal value function v(s) and optimal policy \\pi(s) for all states. Assuming an initial policy that will both move to the right.  Show the details steps (note that in the policy evaluation step, you can directly calculate the state values instead of using iterative method).\n\nTruncated Policy Evaluation: Modify policy_evaluation in Task 3 so it runs for a fixed number of inner sweeps, denoted by j_trunc, instead of until full convergence. Implement Policy Iteration using this truncated evaluation (often called Approximate Policy Iteration) for j_trunc = 2, 5, 10. For each setting:\n\nRun until the outer policy no longer changes (or a reasonable cap on outer iterations is reached).\n\nRecord: (a) number of outer policy improvement steps, (b) final value function for all states, (c) max absolute difference between the final truncated value function and the fully converged value function obtained with the original (non-truncated) method.\n\nPlot the resulting greedy policy and state values for each j_trunc side by side.\n\nExplain:\n\nHow increasing j_trunc affects accuracy vs. computation time.\n\nWhy smaller j_trunc may lead to more outer iterations.\n\nThe trade-off between faster (less accurate) evaluation and slower (more accurate) evaluation in achieving the optimal policy.\n\nPolicy iteration based on Gymnasium environment: Implement policy iteration in Task 4, using the Gymnasium-based grid world environment. Plot the final results.","type":"content","url":"/lab9-value-iteration-policy-iteration#hw-problems","position":19},{"hierarchy":{"lvl1":"Final Project"},"type":"lvl1","url":"/mech529-finalproject-acrobot","position":0},{"hierarchy":{"lvl1":"Final Project"},"content":"Model-Based and Reinforcement Learning Control of the Acrobot","type":"content","url":"/mech529-finalproject-acrobot","position":1},{"hierarchy":{"lvl1":"Final Project","lvl3":"Project Overview"},"type":"lvl3","url":"/mech529-finalproject-acrobot#project-overview","position":2},{"hierarchy":{"lvl1":"Final Project","lvl3":"Project Overview"},"content":"In this final project, you will apply various concepts learned in this course to a single underactuated dynamic system: the Acrobot (a two-link pendulum with an actuator only at the second joint). This project integrates all the tools explored in the labs, providing a comprehensive comparison between model-based control and learning-based control approaches.\n\nSpecifically, you will:\n\nDerive and linearize the continuous-time dynamics of the Acrobot.\n\nDesign state-feedback and LQR controllers for stabilization.\n\nPerform trajectory generation (e.g., swing-up trajectory)\n\nPerform trajectory tracking (via MPC).\n\nImplement a Reinforcement Learning (RL) controller for the Gymnasium Acrobot-v1 environment.\n\nCompare the model-based and RL-based controllers using common performance metrics.\n\nThe final project is individual. You should submit:\n\nThis completed notebook (with code and plots).\n\nA short written report summarizing methods and results.","type":"content","url":"/mech529-finalproject-acrobot#project-overview","position":3},{"hierarchy":{"lvl1":"Final Project","lvl3":"Introduction to Acrobot system"},"type":"lvl3","url":"/mech529-finalproject-acrobot#introduction-to-acrobot-system","position":4},{"hierarchy":{"lvl1":"Final Project","lvl3":"Introduction to Acrobot system"},"content":"The Acrobot is a classic underactuated system. It is a planar two-link robotic arm in the vertical plane (working against gravity), with an actuator at the elbow, but no actuator at the shoulder. The Acrobot is so named because of its resemblance to a gymnast (or acrobat) on a parallel bar, who controls his/her motion predominantly by effort at the waist (and not effort at the wrist). The control goal is typically to swing the Acrobot up from a hanging position to an inverted, balanced position. You may watch a video for the acrobot here.\n\n\nAcrobot swing up and stabilization. A recent physical AI competition using acrobot can be found here:\n\n\nAI Olympics","type":"content","url":"/mech529-finalproject-acrobot#introduction-to-acrobot-system","position":5},{"hierarchy":{"lvl1":"Final Project","lvl3":"System and Tools"},"type":"lvl3","url":"/mech529-finalproject-acrobot#system-and-tools","position":6},{"hierarchy":{"lvl1":"Final Project","lvl3":"System and Tools"},"content":"System: Acrobot (two-link underactuated pendulum)\n\nModel-based control: numpy, scipy, matplotlib, python-control (LQR)\n\nTrajectory optimization / MPC: python-control\n\nReinforcement Learning: gymnasium (Acrobot-v1), stable-baselines3 (PPO)\n\n","type":"content","url":"/mech529-finalproject-acrobot#system-and-tools","position":7},{"hierarchy":{"lvl1":"Final Project","lvl2":"Task 0. Setup"},"type":"lvl2","url":"/mech529-finalproject-acrobot#task-0-setup","position":8},{"hierarchy":{"lvl1":"Final Project","lvl2":"Task 0. Setup"},"content":"Run this cell to import packages and (optionally) install extra dependencies.\n\n# Install dependencies for Colab\n%pip install -q control gymnasium stable-baselines3[extra] scipy matplotlib\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# For numerical integration\ntry:\n    import scipy as sp\n    from scipy.integrate import solve_ivp\nexcept ImportError:\n    %pip install scipy\n    import scipy as sp\n    from scipy.integrate import solve_ivp\n\n# For control systems\ntry:\n    import control as ctl\n    print(f\"control already installed: {ctl.__version__}\")\nexcept ImportError:\n    %pip install control\n    import control as ctl\n\n# For RL APIs\ntry:\n    import gymnasium as gym\n    print(f\"gymnasium already installed: {gym.__version__}\")\nexcept ImportError:\n    %pip install gymnasium\n    import gymnasium as gym\n\n# For RL algorithms\ntry:\n    import stable_baselines3 as sb3\n    print(f\"stable_baselines3 already installed: {sb3.__version__}\")\nexcept ImportError:\n    %pip install stable-baselines3\n    import stable_baselines3 as sb3\n\n# For visualization\ntry:\n    import pygame\n    print(f\"pygame already installed: {pygame.__version__}\")\nexcept ImportError:\n    %pip install pygame\n\n# Set default plotting style\nplt.style.use(\"ggplot\")\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/mech529-finalproject-acrobot#task-0-setup","position":9},{"hierarchy":{"lvl1":"Final Project","lvl2":"Task 1. Acrobot Dynamics"},"type":"lvl2","url":"/mech529-finalproject-acrobot#task-1-acrobot-dynamics","position":10},{"hierarchy":{"lvl1":"Final Project","lvl2":"Task 1. Acrobot Dynamics"},"content":"For this task, we will establish the dynamics model for all the following tasks. Note the solution for this task is provided so that everyone is the on the same page. The system is shown in the following figure\n\nFor the system, we define the state as:\n$$\nx =\\begin{bmatrix}\n\\theta_1, \\theta_2, \\dot{\\theta}_1, \\dot{\\theta}_2\n\\end{bmatrix}\n\n$$\nwhere:\n\n\\theta_1 is the angle of the first link from the vertical\n\n\\theta_2 is the relative angle of the second link with respect to the first\n\nThe control input is the torque applied at joint 2:u =\\tau \\in \\mathbb{R}\n\nWe have the following constant parameters\n\nm_1, m_2: link masses\n\nl_1: length of link 1\n\nl_{c1}, l_{c2}: Center of Mass (COM) distances\n\nI_1, I_2: moments of inertia\n\ng: gravity\n\nThe dynamics equation in state-space form can be written as\n$$\n\\dot{x} =\\begin{bmatrix}\n\\dot{\\theta}_1 \\\\\n\\dot{\\theta}_2 \\\\\n\\ddot{\\theta}_1 \\\\\n\\ddot{\\theta}_2\n\\end{bmatrix}\n\n=\\begin{bmatrix}\n\\omega_1 \\\\\n\\omega_2 \\\\\n-\\dfrac{d_2\\,\\ddot{\\theta}_2 + \\phi_1}{d_1} \\\\\n\\dfrac{\nu\n+ \\frac{d_2}{d_1}\\phi_1\n- m_2 l_1 l_{c2}\\,\\omega_1^2\\sin\\theta_2\n- \\phi_2\n}{\nm_2 l_{c2}^2 + I_2 - \\frac{d_2^2}{d_1}\n}\n\\end{bmatrix}where\n\nd_1 =\nm_1 l_{c1}^2 +\nm_2\\left(l_1^2 + l_{c2}^2 + 2,l_1 l_{c2}\\cos \\theta_2\\right)\n\nI_1 + I_2\n$$d_2 =\nm_2\\left(l_{c2}^2 + l_1 l_{c2}\\cos\\theta_2\\right) + I_2\\phi_2 =\nm_2 l_{c2} g \\cos(\\theta_1 + \\theta_2 - \\tfrac{\\pi}{2})\\phi_1 =\n- m_2 l_1 l_{c2}\\,\\omega_2^2 \\sin\\theta_2\n- 2 m_2 l_1 l_{c2}\\,\\omega_1\\omega_2 \\sin\\theta_2\n+ (m_1 l_{c1} + m_2 l_1) g \\cos(\\theta_1 - \\tfrac{\\pi}{2})\n+ \\phi_2\n\nThe system can be implement as follows\n\n# Example default parameters (you can adjust)\ndefault_params = dict(\n    m1=1.0, m2=1.0,\n    l1=1.0, l2=1.0,\n    lc1=0.5, lc2=0.5,\n    I1=1.0, I2=1.0,\n    g=9.81\n)\n\ndef acrobot_dynamics(t, x, u, params):\n    \"\"\"\n    Continuous-time Acrobot dynamics:\n        x = [theta1, theta2, theta1_dot, theta2_dot]\n        u = torque at joint 2 (scalar)\n    Returns xdot.\n    \"\"\"\n    # Unpack state\n    th1, th2, omega1, omega2 = x\n\n    # Unpack parameters\n    m1, m2, l1, lc1, lc2, I1, I2, g = (params.get(key) for key in ['m1', 'm2', 'l1', 'lc1', 'lc2', 'I1', 'I2', 'g'])\n\n    # Compute intermediate terms\n    d1 = (m1 * lc1**2 +\n          m2 * (l1**2 + lc2**2 + 2 * l1 * lc2 * np.cos(th2)) +\n          I1 + I2)\n\n    d2 = m2 * (lc2**2 + l1 * lc2 * np.cos(th2)) + I2\n\n    phi2 = m2 * lc2 * g * np.cos(th1 + th2 - np.pi/2)\n\n    phi1 = (-m2 * l1 * lc2 * omega2**2 * np.sin(th2)\n            - 2 * m2 * l1 * lc2 * omega1 * omega2 * np.sin(th2)\n            + (m1 * lc1 + m2 * l1) * g * np.cos(th1 - np.pi/2)\n            + phi2)\n\n    # Compute angular accelerations\n    # From the coupled equations, solve for th2_dd first, then th1_dd\n    denominator = m2 * lc2**2 + I2 - d2**2 / d1\n    th2_dd = (u + (d2 / d1) * phi1\n              - m2 * l1 * lc2 * omega1**2 * np.sin(th2)\n              - phi2) / denominator\n\n    th1_dd = -(d2 * th2_dd + phi1) / d1\n\n    # Return state derivative\n    return np.array([omega1, omega2, th1_dd, th2_dd])\n\n\n\nWe can test the dynamics model by simulating it\n\nChoose an initial condition, e.g., both links hanging downward with small perturbation.\n\nSimulate the passive dynamics (u = 0) for a few seconds.\n\nPlot \\theta_1(t), \\theta_2(t) to verify qualitatively that the dynamics look reasonable.\n\ndef simulate_acrobot(x0, u_fun, T=5.0, params=default_params):\n    \"\"\"\n    Simulate Acrobot with given input function u_fun(t, x).\n    \"\"\"\n    def fwrap(t, x):\n        u = u_fun(t, x)\n        return acrobot_dynamics(t, x, u, params)\n\n    sol = solve_ivp(fwrap, [0, T], x0, max_step=0.01, rtol=1e-6, atol=1e-8)\n    return sol.t, sol.y\n\n# Example usage (uncomment after implementing dynamics):\nx0 = np.array([0.3, 0.4, 0.0, 0.0])\nt, X = simulate_acrobot(x0, lambda t, x: 0.0) # lambda t, x: 0.0 means u=0\nplt.figure()\nplt.plot(t, X[0], label=\"theta1\")\nplt.plot(t, X[2], label=\"theta2\")\nplt.legend()\nplt.xlabel(\"Time [s]\")\nplt.ylabel(\"Angle [rad]\")\nplt.title(\"Passive Acrobot Dynamics (u=0)\")\nplt.show()\n\n\n\nWe can also create animation to show the motion for the system.\n\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML, display\n\ndef animate_acrobot_solution(sol, x0, params=None, duration=5.0):\n    \"\"\"\n    Create animation from a solved acrobot ODE solution.\n\n    Parameters:\n    sol: Solution object containing time array t and state array y\n    x0: Initial state [theta1, theta2, omega1, omega2]\n    params: Dictionary of acrobot parameters (uses default_params if None)\n    duration: Animation duration (s)\n\n    Returns:\n    HTML animation object\n    \"\"\"\n    if params is None:\n        params = default_params\n\n    l1 = params['l1']\n    l2 = params['l2']\n\n    # Set up the figure and axis for animation\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n    # Acrobot animation subplot\n    total_length = l1 + l2\n    ax1.set_xlim(-total_length * 1.2, total_length * 1.2)\n    ax1.set_ylim(-total_length * 1.2, total_length * 1.2)\n    ax1.set_aspect('equal')\n    ax1.set_title(f'Acrobot Animation (θ₁₀={x0[0]:.2f}, θ₂₀={x0[1]:.2f})')\n    ax1.grid(True)\n    ax1.plot(0, 0, 'ko', markersize=8)  # Fixed pivot point\n\n    # Create acrobot elements\n    link1, = ax1.plot([], [], 'b-', linewidth=4, label='Link 1')\n    link2, = ax1.plot([], [], 'r-', linewidth=4, label='Link 2')\n    joint1, = ax1.plot([], [], 'ko', markersize=8)\n    joint2, = ax1.plot([], [], 'go', markersize=10)\n    trace, = ax1.plot([], [], 'g--', alpha=0.3, linewidth=1)\n    ax1.legend()\n\n    # Time series subplot\n    ax2.set_xlim(0, duration)\n    ax2.set_ylim(-np.pi, np.pi)\n    ax2.set_xlabel('Time (s)')\n    ax2.set_ylabel('Angle (rad)')\n    ax2.set_title('Joint Angles vs Time')\n    ax2.grid(True)\n\n    theta1_line, = ax2.plot([], [], 'b-', label='θ₁(t)', linewidth=2)\n    theta2_line, = ax2.plot([], [], 'r-', label='θ₂(t)', linewidth=2)\n    time_marker1, = ax2.plot([], [], 'bo', markersize=6)\n    time_marker2, = ax2.plot([], [], 'ro', markersize=6)\n    ax2.legend()\n\n    # Store trace points\n    x_trace, y_trace = [], []\n\n    # Animation function\n    def animate(frame):\n        if frame < len(sol.t):\n            # Current state\n            theta1 = sol.y[0][frame]\n            theta2 = sol.y[1][frame]\n            t_current = sol.t[frame]\n\n            # Joint positions\n            x1 = l1 * np.sin(theta1)\n            y1 = -l1 * np.cos(theta1)\n            x2 = x1 + l2 * np.sin(theta1 + theta2)\n            y2 = y1 - l2 * np.cos(theta1 + theta2)\n\n            # Update acrobot links\n            link1.set_data([0, x1], [0, y1])\n            link2.set_data([x1, x2], [y1, y2])\n            joint1.set_data([x1], [y1])\n            joint2.set_data([x2], [y2])\n\n            # Update trace (last 100 points)\n            x_trace.append(x2)\n            y_trace.append(y2)\n            if len(x_trace) > 100:\n                x_trace.pop(0)\n                y_trace.pop(0)\n            trace.set_data(x_trace, y_trace)\n\n            # Update time series\n            theta1_line.set_data(sol.t[:frame+1], sol.y[0][:frame+1])\n            theta2_line.set_data(sol.t[:frame+1], sol.y[1][:frame+1])\n            time_marker1.set_data([t_current], [theta1])\n            time_marker2.set_data([t_current], [theta2])\n\n        return link1, link2, joint1, joint2, trace, theta1_line, theta2_line, time_marker1, time_marker2\n\n    # Create and display animation\n    anim = FuncAnimation(fig, animate, frames=len(sol.t), interval=50, blit=True, repeat=True)\n    plt.tight_layout()\n    plt.show()\n\n    # Return HTML version for display\n    return HTML(anim.to_jshtml())\n\n# Create solution object from existing variables\nfrom types import SimpleNamespace\nsolution = SimpleNamespace(t=t, y=X)\n\nprint(f\"Animating acrobot with initial state: θ₁={x0[0]:.2f}, θ₂={x0[1]:.2f}, ω₁={x0[2]:.2f}, ω₂={x0[3]:.2f}\")\nprint(f\"Solution contains {len(solution.t)} time points over {solution.t[-1]:.2f} seconds\")\n\n# Create and display animation\nprint(\"Creating animation...\")\nanimation_html = animate_acrobot_solution(solution, x0, default_params, duration=t[-1])\ndisplay(animation_html)\n\n\n\n","type":"content","url":"/mech529-finalproject-acrobot#task-1-acrobot-dynamics","position":11},{"hierarchy":{"lvl1":"Final Project","lvl2":"Task 2. Linearization and LQR Design (20 pts)"},"type":"lvl2","url":"/mech529-finalproject-acrobot#task-2-linearization-and-lqr-design-20-pts","position":12},{"hierarchy":{"lvl1":"Final Project","lvl2":"Task 2. Linearization and LQR Design (20 pts)"},"content":"With the dynamics model, we can design controllers for the Acrobot. In this task, you need to:\n\nChoose an equilibrium point for the system to stabilize (e.g., the upright configuration).\n\nLinearize the dynamics about that equilibrium to obtain:  \\dot{x} = A x + B u \n\nDesign an LQR controller:    u = -Kx \n\nSimulate the closed-loop system and analyze performance (settling time, energy, etc.).","type":"content","url":"/mech529-finalproject-acrobot#task-2-linearization-and-lqr-design-20-pts","position":13},{"hierarchy":{"lvl1":"Final Project","lvl3":"Task 2.1 – Equilibrium Point (5 pts)","lvl2":"Task 2. Linearization and LQR Design (20 pts)"},"type":"lvl3","url":"/mech529-finalproject-acrobot#task-2-1-equilibrium-point-5-pts","position":14},{"hierarchy":{"lvl1":"Final Project","lvl3":"Task 2.1 – Equilibrium Point (5 pts)","lvl2":"Task 2. Linearization and LQR Design (20 pts)"},"content":"For the Acrobot system, one important equilibrium point is the upright configuration:x_{eq} = [\\pi, 0, 0, 0]^T\n\nThis corresponds to:\n\n\\theta_1 = \\pi (first link pointing upward)\n\n\\theta_2 = 0 (second link aligned with first link)\n\n\\dot{\\theta}_1 = 0 (no angular velocity)\n\n\\dot{\\theta}_2 = 0 (no angular velocity)\n\nAnswer the following questions in your project report:\n\nVerification: Show that x_{eq} = [\\pi, 0, 0, 0]^T is indeed an equilibrium point by verifying that \\dot{x} = 0 when x = x_{eq}.\n\nEquilibrium Input: Calculate the corresponding equilibrium control input u_{eq} (torque at joint 2) required to maintain this upright configuration.\n\n","type":"content","url":"/mech529-finalproject-acrobot#task-2-1-equilibrium-point-5-pts","position":15},{"hierarchy":{"lvl1":"Final Project","lvl3":"Task 2.2 – Linearization around the equilibrium point (5 pts)","lvl2":"Task 2. Linearization and LQR Design (20 pts)"},"type":"lvl3","url":"/mech529-finalproject-acrobot#task-2-2-linearization-around-the-equilibrium-point-5-pts","position":16},{"hierarchy":{"lvl1":"Final Project","lvl3":"Task 2.2 – Linearization around the equilibrium point (5 pts)","lvl2":"Task 2. Linearization and LQR Design (20 pts)"},"content":"We will use the Python Control library to linearize the system around x_{eq} and u_{eq}. First create the nonlinear system object using ctl.nlsys in the python control library, then use linearize to linearize the system.\n\nIn the code below, you need to:\n\nComplete the acrobot_update function by calling acrobot_dynamics with the correct inputs (note that u will be a 1-element array)\n\nComplete the acrobot_output function to return the full state vector as output\n\nThe linearization will then be performed automatically around the specified equilibrium point\n\nAfter linearization, you will obtain the linear system matrices A and B such that:\\dot{x} = A(x - x_{eq}) + B(u - u_{eq})\n\nPrint out the A and B matrices for the linearized system and put this into your project report.\n\n# Use the default parameters defined earlier for the Acrobot model\nacrobot_params = default_params\n\n# State derivative function for python-control\ndef acrobot_update(t, x, u, params):\n    \"\"\"\n    Wrapper for the acrobot_dynamics function to be used with control.nlsys.\n    The input 'u' from nlsys is an array, so we extract the scalar torque.\n    \"\"\"\n    # TODO: Call the acrobot_dynamics function with the correct inputs.\n\n# Output function for python-control\ndef acrobot_output(t, x, u, params):\n    \"\"\"\n    Defines the system output. For state-feedback control, we assume we have the full state as the output.\n    \"\"\"\n    # TODO: Return the state vector as the output.\n\n\n# Create the nonlinear system object using nlsys\nacrobot_nlsys = ctl.nlsys(\n    acrobot_update,\n    acrobot_output,\n    name='acrobot',\n    params=acrobot_params,\n    states=['theta1', 'theta2', 'omega1', 'omega2'],\n    outputs=['theta1', 'theta2', 'omega1', 'omega2'],\n    inputs=['tau']\n)\n\n# After filling in the TODOs, linearize the system using acrobot_nlsys.linearize\n# TODO: linearize the system at the upright equilibrium point\n\n\n\n","type":"content","url":"/mech529-finalproject-acrobot#task-2-2-linearization-around-the-equilibrium-point-5-pts","position":17},{"hierarchy":{"lvl1":"Final Project","lvl3":"Task 2.3 – LQR Controller Design for the Linearized System (10 pts)","lvl2":"Task 2. Linearization and LQR Design (20 pts)"},"type":"lvl3","url":"/mech529-finalproject-acrobot#task-2-3-lqr-controller-design-for-the-linearized-system-10-pts","position":18},{"hierarchy":{"lvl1":"Final Project","lvl3":"Task 2.3 – LQR Controller Design for the Linearized System (10 pts)","lvl2":"Task 2. Linearization and LQR Design (20 pts)"},"content":"For this task, you will design an LQR controller to stabilize the Acrobot around the upright configuration by completing the following steps:\n\nStep 1: Check System Controllability\n\nCompute the controllability matrix using ctl.ctrb(A, B)\n\nCheck if the system is controllable by verifying that the rank equals the number of states (4)\n\nStep 2: Design LQR Controller\n\nChoose appropriate LQR weight matrices Q_x (4×4) and Q_u (1×1)\n\nQ_x penalizes state deviations from equilibrium\n\nQ_u penalizes control effort\n\nUse ctl.lqr(A, B, Qx, Qu) to compute the feedback gain matrix K\n\nThe resulting controller will be: u = -K (x - x_{\\text{eq}}) + u_{\\text{eq}}\n\nStep 3: Simulate Closed-Loop System\nTest your LQR controller with two initial conditions near the upright equilibrium:\n\nx_0 = [19\\pi/20, \\pi/10, 0, 0]^T\n\nx_0 = [9\\pi/10, \\pi/5, 0, 0]^T\n\nFor each initial condition, create plots showing:\n\nAll four state trajectories vs time: \\theta_1(t), \\theta_2(t), \\dot{\\theta}_1(t), \\dot{\\theta}_2(t)\n\nControl input vs time: u(t)\n\nStep 4: Performance Evaluation\nAnalyze your results by addressing:\n\nDoes the controller successfully stabilize both initial conditions?\n\nWhat is the approximate settling time for each state?\n\nHow does changing the weight matrices Q_x and Q_u affect performance? Try at least two different weight combinations and compare the results\n\n# TODO: choose equilibrium and design LQR\n\n\n\n\n","type":"content","url":"/mech529-finalproject-acrobot#task-2-3-lqr-controller-design-for-the-linearized-system-10-pts","position":19},{"hierarchy":{"lvl1":"Final Project","lvl2":"Task 3. Trajectory Generation and Tracking (30 pts)"},"type":"lvl2","url":"/mech529-finalproject-acrobot#task-3-trajectory-generation-and-tracking-30-pts","position":20},{"hierarchy":{"lvl1":"Final Project","lvl2":"Task 3. Trajectory Generation and Tracking (30 pts)"},"content":"Task 2 only stabilizes the Acrobot around the upright configuration. But what if the Acrobot is initially pointing downward? In this task, you will design controllers to swing the Acrobot up from the downward position to the upright position. Specifically, you will:\n\nDefine a swing-up task for the Acrobot (e.g., from downward configuration to upright).\n\nGenerate an optimal trajectory from downward configuration to upright (refer to Lab 7).\n\nDesign an MPC controller to track the generated optimal trajectory (refer to Lab 8)\n\nSimulate the resulting trajectory tracking control on the nonlinear system.\n\n","type":"content","url":"/mech529-finalproject-acrobot#task-3-trajectory-generation-and-tracking-30-pts","position":21},{"hierarchy":{"lvl1":"Final Project","lvl3":"Task 3.1 – Optimal Trajectory Generation (10 pts)","lvl2":"Task 3. Trajectory Generation and Tracking (30 pts)"},"type":"lvl3","url":"/mech529-finalproject-acrobot#task-3-1-optimal-trajectory-generation-10-pts","position":22},{"hierarchy":{"lvl1":"Final Project","lvl3":"Task 3.1 – Optimal Trajectory Generation (10 pts)","lvl2":"Task 3. Trajectory Generation and Tracking (30 pts)"},"content":"Generate and plot the optimal trajectory using solve_ocp for the followint case\n\nInput cost with terminal constraint:\n\\min_{u(\\cdot)} \\int_0^T (u(\\tau) - u_f)^T Q_u (u(\\tau) - u_f)\\, d\\tau\n\n\nsubject to x(T) = x_f\nAlso consider the constraint on the control input |u| \\leq 5 Nm.\n\nAfter generating the optimal trajectory, you need to simluate the system using the optimal control inputs and make sure that the system will behave accordingly, i.e., it will actually swing up.\n\n# TODO: write your code here for the optimal trajecotry generation\n\n\n\n","type":"content","url":"/mech529-finalproject-acrobot#task-3-1-optimal-trajectory-generation-10-pts","position":23},{"hierarchy":{"lvl1":"Final Project","lvl3":"Task 3.2 – Trajectory Tracking (20 pts)","lvl2":"Task 3. Trajectory Generation and Tracking (30 pts)"},"type":"lvl3","url":"/mech529-finalproject-acrobot#task-3-2-trajectory-tracking-20-pts","position":24},{"hierarchy":{"lvl1":"Final Project","lvl3":"Task 3.2 – Trajectory Tracking (20 pts)","lvl2":"Task 3. Trajectory Generation and Tracking (30 pts)"},"content":"For this task, you need to track the optimal trajectory (x_d(t), u_d(t)) you have generated in Task 3.1 using a continuous-time Model Predictive Controller (MPC). The controller will solve a finite-horizon optimal control problem at each time step to find a corrective input \\delta u(t) that minimizes deviations from the reference trajectory. The total control applied to the system is:u(t) = u_d(t) + \\delta u(t)\n\nThis requires defining an error system where the state is the tracking error e(t) = x(t) - x_d(t) and the input is the control error \\delta u(t). The dynamics of this error system can be expressed as:\\dot{e}(t) = f(x_d(t) + e(t), u_d(t) + \\delta u(t)) - f(x_d(t), u_d(t))\n\nwhere f represents the nonlinear Acrobot dynamics.\n\nTo implement the MPC tracker, you will:\n\nCreate interpolants for your reference trajectory, x_d(t) and u_d(t), using the results from Task 3.1.\n\nDefine the error dynamics as a new nonlinear system (e.g., acrobot_err_update). The Python code in the next cell provides a template for this.\n\nDefine a quadratic cost function for the MPC that penalizes the error state e(t) and the error input \\delta u(t) over a prediction horizon T_h:\n\nTrajectory cost: \\int_0^{T_h} (e(\\tau)^T Q_x e(\\tau) + \\delta u(\\tau)^T Q_u \\delta u(\\tau)) \\, d\\tau.\n\nTerminal cost: e(T_h)^T P e(T_h).\n\nDefine input constraints on the total control input, |u(t)| \\le u_{\\max}= 5 Nm. Since u(t) = u_d(t) + \\delta u(t), the constraints on the MPC’s decision variable \\delta u(t) become time-varying:\n-u_{\\max} - u_d(t) \\le \\delta u(t) \\le u_{\\max} - u_d(t)\n\nCreate an OptimalControlProblem for the error system using the tracking cost and time-varying input constraints.\n\nImplement an MPC simulation loop that, at each time step:\n\nCalculates the current error e(t) = x(t) - x_d(t).\n\nUpdates the input constraints for the OCP based on the current reference input u_d(t).\n\nSolves the OCP to get an optimal error input trajectory \\delta u(\\tau) for the next T_h seconds.\n\nApplies the first segment of the total control input u(t) = u_d(t) + \\delta u(t) to the true nonlinear Acrobot system.\n\nSimulates the system forward for a short duration and repeats the process.\n\ndef acrobot_err_update(t, e, du, params):\n    # e: error state (4,)\n    # du: error input (1,)  (delta torque)\n    xref = params['x_ref'](t).ravel()    # x_ref(t)\n    uref = params['u_ref'](t).ravel()[0] # scalar\n    x_tot = xref + e\n    u_tot = uref + du[0]\n\n    # total and reference dynamics\n    f_tot = acrobot_dynamics(t, x_tot, u_tot, params['plant_params'])\n    f_ref = acrobot_dynamics(t, xref, uref, params['plant_params'])\n\n    return f_tot - f_ref    # error dynamics\n\n\nacrobot_err = ctl.nlsys(\n    acrobot_err_update, None, name=\"acrobot_err\",\n    inputs=('du',),\n    outputs=('e1','e2','e3','e4'),\n    states=('e1','e2','e3','e4'),\n    params={\n        'x_ref': x_d,\n        'u_ref': u_d,\n        'plant_params': plant_params\n    }\n)\n\n# TODO: design MPC controller for trajectory tracking using acrobot_err system\n\n\n\n","type":"content","url":"/mech529-finalproject-acrobot#task-3-2-trajectory-tracking-20-pts","position":25},{"hierarchy":{"lvl1":"Final Project","lvl2":"Task 4: Swing Up and then Stay (20 pts)"},"type":"lvl2","url":"/mech529-finalproject-acrobot#task-4-swing-up-and-then-stay-20-pts","position":26},{"hierarchy":{"lvl1":"Final Project","lvl2":"Task 4: Swing Up and then Stay (20 pts)"},"content":"In this task, you will combine your optimal swing-up trajectory and tracking controller (from Task 3) with your LQR stabilization controller around the upright (from Task 2) to build a hybrid controller that:\n\nUses the trajectory-tracking controller to swing the Acrobot up from the downward position\n\nAutomatically switches to the LQR controller when the Acrobot gets close to the upright position\n\nMaintains balance around the upright equilibrium using LQR control","type":"content","url":"/mech529-finalproject-acrobot#task-4-swing-up-and-then-stay-20-pts","position":27},{"hierarchy":{"lvl1":"Final Project","lvl3":"Hybrid Control Strategy","lvl2":"Task 4: Swing Up and then Stay (20 pts)"},"type":"lvl3","url":"/mech529-finalproject-acrobot#hybrid-control-strategy","position":28},{"hierarchy":{"lvl1":"Final Project","lvl3":"Hybrid Control Strategy","lvl2":"Task 4: Swing Up and then Stay (20 pts)"},"content":"The hybrid controller operates as a state machine with two modes:\n\nSwing-up Mode: When the Acrobot is far from upright, use your MPC trajectory tracking controller from Task 3\n\nStabilization Mode: When the Acrobot is near upright, switch to your LQR controller from Task 2","type":"content","url":"/mech529-finalproject-acrobot#hybrid-control-strategy","position":29},{"hierarchy":{"lvl1":"Final Project","lvl3":"Implementation Requirements","lvl2":"Task 4: Swing Up and then Stay (20 pts)"},"type":"lvl3","url":"/mech529-finalproject-acrobot#implementation-requirements","position":30},{"hierarchy":{"lvl1":"Final Project","lvl3":"Implementation Requirements","lvl2":"Task 4: Swing Up and then Stay (20 pts)"},"content":"You need to implement the following components:\n\nController Functions (adapt from your previous tasks):\n\nu_track(x, t): Your trajectory tracking controller from Task 3\n\nu_lqr(x): Your LQR controller from Task 2: u = -K(x - x_{eq}) + u_{eq}\n\nSwitching Logic Functions:\n\nshould_switch_to_lqr(x): Returns True when state is close enough to upright to switch to LQR\n\nshould_switch_to_swingup(x): Returns True when state has drifted too far from upright (optional)\n\nRegion Detection: Use the provided in_upright_region(x) function (see next cell) which defines a “capture region” around the upright equilibrium based on:\n\nAngular position tolerances: |\\theta_1 - \\pi| < 0.25 rad, |\\theta_2| < 0.5 rad\n\nAngular velocity limits: |\\dot{\\theta}_1| < 1.0 rad/s, |\\dot{\\theta}_2| < 1.0 rad/s","type":"content","url":"/mech529-finalproject-acrobot#implementation-requirements","position":31},{"hierarchy":{"lvl1":"Final Project","lvl3":"Tasks to Complete:","lvl2":"Task 4: Swing Up and then Stay (20 pts)"},"type":"lvl3","url":"/mech529-finalproject-acrobot#tasks-to-complete","position":32},{"hierarchy":{"lvl1":"Final Project","lvl3":"Tasks to Complete:","lvl2":"Task 4: Swing Up and then Stay (20 pts)"},"content":"Implement the hybrid controller using the template in the code cells below\n\nTest the controller starting from the downward hanging position: x_0 = [0, 0, 0, 0]\n\nSimulate and plot:\n\nState trajectories showing the swing-up and stabilization phases\n\nControl input over time\n\nMode switching events\n\nEvaluate performance:\n\nDoes it successfully swing up and stabilize?\n\nHow smooth is the transition between controllers?\n\nWhat is the total time to reach and stabilize at upright?\n\nThe code framework is provided in the following cells to help you get started.\n\ndef wrap_angle(angle):\n    \"\"\"Wrap angle to [-pi, pi).\"\"\"\n    return (angle + np.pi) % (2*np.pi) - np.pi\n\n# Thresholds for upright region, you can change them as needed\nth1_thresh = 0.25   # rad (~14 deg) around pi\nth2_thresh = 0.5    # rad (~29 deg) around 0\nw1_thresh  = 1.0    # rad/s\nw2_thresh  = 1.0    # rad/s\n\n# function to check if state is in upright region\ndef in_upright_region(x):\n    th1, th2, th1d, th2d = x\n    e_th1 = wrap_angle(th1 - np.pi)\n    e_th2 = wrap_angle(th2 - 0.0)\n\n    cond_angle = (abs(e_th1) < th1_thresh) and (abs(e_th2) < th2_thresh) # check angles\n    cond_vel   = (abs(th1d) < w1_thresh) and (abs(th2d) < w2_thresh) # check angular velocities\n\n    return cond_angle and cond_vel\n\n\n\nu_max = 5.0   # input limit for safety\nmode  = \"swingup\"   # initial mode\n\ndef hybrid_controller(x, t):\n    global mode\n\n    if mode == \"swingup\":\n        # 1) Use your trajectory-tracking controller (OCP/MPC/PD+FF)\n        u = u_track(x, t)\n\n        # 2) Check if we can switch to LQR\n        if should_switch_to_lqr(x):\n            mode = \"stabilize\"\n\n    elif mode == \"stabilize\":\n        # 1) Use LQR controller around upright\n        u = u_lqr(x)\n\n        # 2) Optionally: if we drift too far → go back to swing-up\n        if should_switch_to_swingup(x):\n            mode = \"swingup\"\n\n    else:\n        raise ValueError(\"Unknown mode: \" + mode)\n\n    # Saturate the torque for safety\n    u = float(np.clip(u, -u_max, u_max))\n    return u\n\n\n\n","type":"content","url":"/mech529-finalproject-acrobot#tasks-to-complete","position":33},{"hierarchy":{"lvl1":"Final Project","lvl2":"Task 5. Reinforcement Learning Control (25 pts)"},"type":"lvl2","url":"/mech529-finalproject-acrobot#task-5-reinforcement-learning-control-25-pts","position":34},{"hierarchy":{"lvl1":"Final Project","lvl2":"Task 5. Reinforcement Learning Control (25 pts)"},"content":"Tasks 2 to 4 are based on model-based optimization and control to swing-up and balance the Acrobot. Another way to do it is to use RL. In this task, you will:\n\nUse the Gymnasium Acrobot-v1 environment.\n\nImplement and train an RL agent (e.g., PPO algorithm from stable-baselines3).\n\nEvaluate the trained policy: success rate, episode reward, qualitative behavior.\n\nThe Acrobot-v1 \n\nenvironment has different characteristics compared to the continuous control problem in previous tasks:\n\nEnvironment Settings:\n\nActions: 3 discrete actions (torque -1, 0, +1) vs. continuous torque in Tasks 1-4\n\nReward structure: -1 per time step (encourages faster completion) vs. quadratic costs in optimal control\n\nSuccess criterion: End-effector height > 1.0 (approximately upright) vs. exact state targets\n\nEpisode termination: Success OR maximum 500 steps vs. fixed time horizons\n\nNo input penalties: Unlike LQR/MPC which penalize control effort\n\nStochastic initial conditions: Random start states vs. deterministic initial conditions\n\nKey Differences from Model-Based Control:\n\nObjective: Minimize episode length (maximize reward) vs. minimize quadratic cost functionals\n\nAction space: Discrete bang-bang control vs. smooth continuous control\n\nLearning approach: Trial-and-error policy optimization vs. analytical/numerical optimal control\n\nRobustness: Trained on distribution of initial states vs. designed for specific operating points\n\nThe following code cells demonstrate how to get a RL policy for the default Acrobot-v1 environment.\n\nCreate Environment: Set up the Gymnasium Acrobot-v1 environment with appropriate observation and action spaces\n\nTrain RL Agent: Use PPO from stable-baselines3 to learn a policy through interaction with the environment\n\nEvaluate Performance: Test the trained model on multiple episodes to assess success rate and behavior\n\nVisualize Results: Render the learned policy in action to qualitatively evaluate the swing-up strategy\n\n# Create and inspect the Gymnasium Acrobot environment\nenv = gym.make(\"Acrobot-v1\", render_mode=None)\nobs, info = env.reset()\nprint(\"Observation space:\", env.observation_space)\nprint(\"Action space:\", env.action_space)\nprint(\"Initial observation:\", obs)\n\nenv.close()\n\n\n\nAfter create the environement, we can train the model using PPO algorithm from stable baseline3 (you may modify hyperparameters, total timesteps, etc.):\n\nfrom stable_baselines3 import PPO\nimport numpy as np\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\n# Use the wrapper\nenv = gym.make(\"Acrobot-v1\")\nmodel = PPO(\"MlpPolicy\", env, verbose=1) # Create PPO model instance\nmodel.learn(total_timesteps=200_000) # Train for 200,000 timesteps\n\n# Save model\nmodel.save(\"ppo_acrobot\") # this will save the trained model to a zip file\n\n# Evaluate\nmean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20)\nprint(f\"Mean reward over 20 eval episodes: {mean_reward:.2f} ± {std_reward:.2f}\")\n\nenv.close()\n\n\n\nAfter that, we can evaluate and visualize the trained policy performance. The code below demonstrates the trained PPO agent in action by running several episodes with visual rendering enabled. This allows us to see how the agent swings up the acrobot. The visualization helps validate that the training was successful and provides intuition about the RL controller’s behavior.\n\nimport gymnasium as gym\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\nfrom IPython.display import Video\n\n# Create a vectorized environment for video recording\ndef make_env():\n    return gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\n\nenv = DummyVecEnv([make_env])\n\n# Wrap with video recorder\nenv = VecVideoRecorder(\n    env,\n    \"videos/\",\n    record_video_trigger=lambda x: x == 0,  # Record first episode\n    video_length=500,  # Maximum video length\n    name_prefix=\"acrobot_ppo\"\n)\n\n# Load the trained model\nmodel = PPO.load(\"ppo_acrobot\")\n\n# Run one episode with video recording\nobs = env.reset()\nfor _ in range(500):\n    action, _ = model.predict(obs, deterministic=True)\n    obs, reward, done, info = env.step(action)\n    if done[0]:\n        break\n\nenv.close()\n\n# Display the recorded video\nVideo(\"videos/acrobot_ppo-step-0-to-step-500.mp4\", width=600)\n\n\n\n","type":"content","url":"/mech529-finalproject-acrobot#task-5-reinforcement-learning-control-25-pts","position":35},{"hierarchy":{"lvl1":"Final Project","lvl3":"Task 5.1 – Evaluation and Analysis for default Gymnasium Acrobot-v1 environment  (5 pts)","lvl2":"Task 5. Reinforcement Learning Control (25 pts)"},"type":"lvl3","url":"/mech529-finalproject-acrobot#task-5-1-evaluation-and-analysis-for-default-gymnasium-acrobot-v1-environment-5-pts","position":36},{"hierarchy":{"lvl1":"Final Project","lvl3":"Task 5.1 – Evaluation and Analysis for default Gymnasium Acrobot-v1 environment  (5 pts)","lvl2":"Task 5. Reinforcement Learning Control (25 pts)"},"content":"After training your PPO agent, you need to analyze its behavior and performance. Run the evaluation code above and then address the following questions:\n\nQuestions to Answer:\n\nState Trajectory Analysis: Plot the joint angles \\theta_1(t) and \\theta_2(t) with respect to time for several episodes. What patterns do you observe in the learned policy’s swing-up strategy?\n\nTip Height Analysis: Plot the tip’s height with respect to time. The tip height can be calculated as: height = -cos(θ₁) - cos(θ₁ + θ₂).\n\nYou should notice that the acrobot swings up but doesn’t get to the upright position (θ₁ = π, θ₂ = 0). Instead, it typically terminates the episode once the tip height exceeds 1.0.  This is because the default Gymnasium Acrobot-v1 environment has a sparse reward structure that differs significantly from optimal control formulations:\n\nTermination condition: Episode ends when tip height > 1.0\n\nReward signal: -1 per time step (encourages faster completion)\n\nNo balancing phase: Once the success condition is met, the episode terminates immediately\n\nThis means the RL agent learns to Swing up quickly to minimize the negative reward accumulation, reach the target height as fast as possible, and it does not learn balancing behavior since the episode ends upon reaching the goal. This is fundamentally different from the model-based controllers you designed in Tasks 2-4, which were formulated to both reach AND maintain the upright equilibrium. The RL agent essentially learns only the “swing-up phase” without the “stabilization phase.” This difference highlights an important consideration in RL problem formulation: the reward structure directly shapes the learned behavior. To make the acrobot fully upright, you would need to modify the environment with different rewards.\n\n","type":"content","url":"/mech529-finalproject-acrobot#task-5-1-evaluation-and-analysis-for-default-gymnasium-acrobot-v1-environment-5-pts","position":37},{"hierarchy":{"lvl1":"Final Project","lvl3":"Task 5.2 – Train a Stablization Policy (5 pts)","lvl2":"Task 5. Reinforcement Learning Control (25 pts)"},"type":"lvl3","url":"/mech529-finalproject-acrobot#task-5-2-train-a-stablization-policy-5-pts","position":38},{"hierarchy":{"lvl1":"Final Project","lvl3":"Task 5.2 – Train a Stablization Policy (5 pts)","lvl2":"Task 5. Reinforcement Learning Control (25 pts)"},"content":"Now we can try to revise the default setting to make it similar to model-based approach. To do this, we will do two things through the wrapper of gym:\n\nConvert the problem to be continuous\n\nUse a new reward function similar to the cost function we have used in trajectory optimization\n\nWhat is a Gym \n\nWrapper?\n\nA wrapper is a design pattern that allows us to modify the behavior of an existing environment without creating an entirely new one from scratch. Wrappers “wrap around” the original environment and can modify:\n\nAction spaces (discrete → continuous)\n\nObservation spaces\n\nReward functions\n\nEpisode termination conditions\n\nAny other environment behavior\n\nWhy Use Wrappers Instead of Creating New Environments?\n\nReusability: We can apply the same modifications to different base environments\n\nModularity: Each wrapper handles one specific modification, making code more organized\n\nMaintainability: Changes to the base environment automatically propagate to wrapped versions\n\nEfficiency: We leverage the existing, well-tested Gymnasium implementation rather than reimplementing dynamics\n\nIn our case, we’ll use two wrappers:\n\nContinuousAcrobotActionWrapper: Converts discrete actions {-1, 0, +1} to continuous torque values\n\nAcrobotOCPRewardWrapper: Replaces the sparse -1 reward with a customized reward similar to optimal control\n\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\n\n# Action wrapper to convert discrete actions to continuous torque\nclass ContinuousAcrobotActionWrapper(gym.Wrapper):\n    \"\"\"\n    Convert discrete Acrobot-v1 actions into continuous torque in [-torque_limit, torque_limit].\n    The underlying env is still Acrobot-v1, but we override the torque used in the dynamics.\n    \"\"\"\n    def __init__(self, env, torque_limit=5.0):\n        super().__init__(env)\n        self.torque_limit = float(torque_limit)\n        # Continuous 1-D action: torque in [-torque_limit, torque_limit]\n        self.action_space = spaces.Box(\n            low=np.array([-self.torque_limit], dtype=np.float32),\n            high=np.array([self.torque_limit], dtype=np.float32),\n            dtype=np.float32,\n        )\n        # for reward wrapper\n        self.unwrapped.last_u = 0.0\n\n    def reset(self, **kwargs):\n        obs, info = self.env.reset(**kwargs)\n        self.unwrapped.last_u = 0.0\n        return obs, info\n\n    def step(self, action):\n        # action is expected to be a 1D array or scalar; clip to [-torque_limit, torque_limit]\n        if isinstance(action, np.ndarray):\n            u = float(action.squeeze())\n        else:\n            u = float(action)\n        u = np.clip(u, -self.torque_limit, self.torque_limit)\n        self.unwrapped.last_u = u\n\n        # Hack: temporarily set AVAIL_TORQUE so that super().step(1) uses our u\n        orig_avail = self.env.unwrapped.AVAIL_TORQUE.copy()\n        self.env.unwrapped.AVAIL_TORQUE[:] = [u, u, u]\n\n        # Use middle index (1) – all entries are the same anyway\n        obs, reward, terminated, truncated, info = self.env.step(1)\n\n        # Restore original discrete torque set\n        self.env.unwrapped.AVAIL_TORQUE[:] = orig_avail\n\n        return obs, reward, terminated, truncated, info\n\n# Reward wrapper to implement OCP-like reward\nclass AcrobotOCPRewardWrapper(gym.Wrapper):\n    \"\"\"\n    Replace Acrobot reward by r = -u^T Q_u u + height reward + terminal bonus (on success).\n    Assumes the wrapped env sets env.unwrapped.last_u to the actual torque used.\n    \"\"\"\n    def __init__(self, env, Q_u=1.0, height_weight=5.0, terminal_bonus=200.0,\n                 failure_penalty=0.0, success_height_threshold=1.8):\n        super().__init__(env)\n        self.Q_u = float(Q_u)\n        self.height_weight = float(height_weight) # weight for height reward\n        self.terminal_bonus = float(terminal_bonus) # bonus for success\n        self.failure_penalty = float(failure_penalty) # penalty on failure (timeout)\n        self.success_height_threshold = float(success_height_threshold)  # height threshold for success\n\n    def step(self, action):\n        obs, _, terminated, truncated, info = self.env.step(action)\n\n        u = float(getattr(self.env.unwrapped, \"last_u\", 0.0))\n        cos_th1, sin_th1, cos_th2, sin_th2, th1_dot, th2_dot = obs\n\n        height = -cos_th1 - (cos_th1 * cos_th2 - sin_th1 * sin_th2)\n\n        # Reward components\n        height_reward = self.height_weight * height\n        control_cost = self.Q_u * (u ** 2)\n        velocity_penalty = 0.01 * (th1_dot**2 + th2_dot**2)\n        reward = height_reward - control_cost - velocity_penalty\n\n        # OVERRIDE termination logic: only terminate if reaching near-upright (height > 1.8)\n        # or if episode times out (truncated)\n        if height >= self.success_height_threshold:\n            terminated = True  # True success\n            reward += self.terminal_bonus\n        else:\n            terminated = False  # Keep going even if original env terminated at height > 1\n\n        if truncated and self.failure_penalty != 0.0:\n            reward -= self.failure_penalty\n\n        return obs, reward, terminated, truncated, info\n\n\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\n\n# Function to create the wrapped Acrobot environment\ndef make_acrobot_ocp_env():\n    base_env = gym.make(\"Acrobot-v1\")\n    env = ContinuousAcrobotActionWrapper(base_env, torque_limit=5.0)\n    env = AcrobotOCPRewardWrapper(\n        env,\n        Q_u=0.1,\n        height_weight=5.0,\n        terminal_bonus=200.0,\n        success_height_threshold=1.7  # Require height for success, you may start with a smaller value so that it is easier to learn\n    )\n    return env\n\nvec_env = make_vec_env(make_acrobot_ocp_env, n_envs=8) # Vectorized environment for parallel training\n\nmodel = PPO(\"MlpPolicy\", vec_env, verbose=1)\nmodel.learn(total_timesteps=500_000)  # May need more timesteps\nmodel.save(\"ppo_acrobot_ocp_continuous\")\n\n\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\nfrom IPython.display import Video\n\n# Create the wrapped environment with rgb_array render mode for video recording\ndef make_acrobot_ocp_env_visual():\n    base_env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\n    env = ContinuousAcrobotActionWrapper(base_env, torque_limit=5.0)\n    env = AcrobotOCPRewardWrapper(\n        env,\n        Q_u=0.1,\n        height_weight=5.0,\n        terminal_bonus=200.0,\n        success_height_threshold=1.7\n    )\n    return env\n\n# Create vectorized environment for video recording\nenv = DummyVecEnv([make_acrobot_ocp_env_visual])\n\n# Wrap with video recorder\nenv = VecVideoRecorder(\n    env,\n    \"videos/\",\n    record_video_trigger=lambda x: x == 0,  # Record first episode\n    video_length=500,\n    name_prefix=\"acrobot_ocp_continuous\"\n)\n\n# Load the trained model\nmodel = PPO.load(\"ppo_acrobot_ocp_continuous\")\n\n# Run one episode with video recording\nobs = env.reset()\nfor _ in range(500):\n    action, _ = model.predict(obs, deterministic=True)\n    obs, reward, done, info = env.step(action)\n    if done[0]:\n        break\n\nenv.close()\n\n# Display the recorded video\nVideo(\"videos/acrobot_ocp_continuous-step-0-to-step-500.mp4\",embed=True, width=600)\n\n\n\n\n\n\n\n\n\n\n\nTasks:\n\nTrain models with different success_height_threshold values (keep Q_u = 0.1, height_weight = 5.0):\n\nEasy: success_height_threshold = 1.5\n\nMedium: success_height_threshold = 1.7 (baseline)\n\nHard: success_height_threshold = 1.85\n\nFor each threshold, plot for 5 sample episodes:\n\nTip height vs time\n\nControl input (torque) vs time\n\nExplain the results you have obtained.\n\n","type":"content","url":"/mech529-finalproject-acrobot#task-5-2-train-a-stablization-policy-5-pts","position":39},{"hierarchy":{"lvl1":"Final Project","lvl3":"Task 5.3 – Train a Swing-up Policy (10 pts)","lvl2":"Task 5. Reinforcement Learning Control (25 pts)"},"type":"lvl3","url":"/mech529-finalproject-acrobot#task-5-3-train-a-swing-up-policy-10-pts","position":40},{"hierarchy":{"lvl1":"Final Project","lvl3":"Task 5.3 – Train a Swing-up Policy (10 pts)","lvl2":"Task 5. Reinforcement Learning Control (25 pts)"},"content":"In this task, you will train a policy that can swing the Acrobot up from the downward position. The goal here is only to achieve swing-up to a target height, not to maintain balance afterward. The episode will terminate upon reaching the target height.\n\nThe code below implements this using:\n\nContinuous torque via ContinuousAcrobotActionWrapper\n\nA swing-up-specific reward and termination with AcrobotSwingUpWrapper\n\nSuccess defined by reaching a tip height threshold (e.g., 1.8)","type":"content","url":"/mech529-finalproject-acrobot#task-5-3-train-a-swing-up-policy-10-pts","position":41},{"hierarchy":{"lvl1":"Final Project","lvl4":"Design Considerations","lvl3":"Task 5.3 – Train a Swing-up Policy (10 pts)","lvl2":"Task 5. Reinforcement Learning Control (25 pts)"},"type":"lvl4","url":"/mech529-finalproject-acrobot#design-considerations","position":42},{"hierarchy":{"lvl1":"Final Project","lvl4":"Design Considerations","lvl3":"Task 5.3 – Train a Swing-up Policy (10 pts)","lvl2":"Task 5. Reinforcement Learning Control (25 pts)"},"content":"Termination: Terminate the episode when the tip height exceeds the success threshold (swing-up achieved); Otherwise, terminate on timeout (max steps)\n\nReward Function (Swing-up only): the reward function should encourage upward movement via height reward; discourage staying near the downward configuration; penalize large control effort, among others.\n\nYou will need to play with the reward function to obtain the correct behaviror\n\nSuggested reward structure:\n\nHeight reward: encourages swing-up\n\nDownward penalty: discourages staying down\n\nControl penalty: promotes energy efficiency\n\nExample (as implemented below):\n\nreward = height_weight * height - downward_penalty - control_cost * u^2","type":"content","url":"/mech529-finalproject-acrobot#design-considerations","position":43},{"hierarchy":{"lvl1":"Final Project","lvl4":"Training Setup","lvl3":"Task 5.3 – Train a Swing-up Policy (10 pts)","lvl2":"Task 5. Reinforcement Learning Control (25 pts)"},"type":"lvl4","url":"/mech529-finalproject-acrobot#training-setup","position":44},{"hierarchy":{"lvl1":"Final Project","lvl4":"Training Setup","lvl3":"Task 5.3 – Train a Swing-up Policy (10 pts)","lvl2":"Task 5. Reinforcement Learning Control (25 pts)"},"content":"Train for 300k–700k timesteps\n\nTune hyperparameters:\n\nheight_weight: 5–15\n\ncontrol_cost: 0.05–0.2\n\nsuccess_height: 1.7–1.9\n\ndownward_penalty: 3–10","type":"content","url":"/mech529-finalproject-acrobot#training-setup","position":45},{"hierarchy":{"lvl1":"Final Project","lvl4":"Evaluation","lvl3":"Task 5.3 – Train a Swing-up Policy (10 pts)","lvl2":"Task 5. Reinforcement Learning Control (25 pts)"},"type":"lvl4","url":"/mech529-finalproject-acrobot#evaluation","position":46},{"hierarchy":{"lvl1":"Final Project","lvl4":"Evaluation","lvl3":"Task 5.3 – Train a Swing-up Policy (10 pts)","lvl2":"Task 5. Reinforcement Learning Control (25 pts)"},"content":"Start from the downward position and check if the agent reaches the target height\n\nPlot tip height over time to verify swing-up completion\n\nNote: The agent is not trained to balance; episodes end upon success","type":"content","url":"/mech529-finalproject-acrobot#evaluation","position":47},{"hierarchy":{"lvl1":"Final Project","lvl4":"Using GPU Acceleration in Google Colab","lvl3":"Task 5.3 – Train a Swing-up Policy (10 pts)","lvl2":"Task 5. Reinforcement Learning Control (25 pts)"},"type":"lvl4","url":"/mech529-finalproject-acrobot#using-gpu-acceleration-in-google-colab","position":48},{"hierarchy":{"lvl1":"Final Project","lvl4":"Using GPU Acceleration in Google Colab","lvl3":"Task 5.3 – Train a Swing-up Policy (10 pts)","lvl2":"Task 5. Reinforcement Learning Control (25 pts)"},"content":"Training RL policies can be time-consuming on CPU. You can get free compute credits through:\n\n\nhttps://​colab​.research​.google​.com​/signup\n\nThis will allow you to use GPUs in the cloud to speed up training in Google Colab:\n\nEnable GPU runtime: Runtime → Change runtime type → Hardware accelerator → GPU (T4 or better)\n\nVerify GPU availability: Run !nvidia-smi\n\nPyTorch will automatically use GPU (Stable-Baselines3 runs on PyTorch)\n\nExpected speedup: A 30–60 min CPU run can complete in 5–15 min on GPU\n\nYou can save checkpoints with model.save() and resume with model = PPO.load() if your session is interrupted.\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\nclass AcrobotSwingUpWrapper(gym.Wrapper):\n    \"\"\"\n    Modified Acrobot wrapper for swing-up only (no balance).\n    Key changes:\n    1. Terminates when target height is reached\n    2. Reward function focuses on height achievement\n    3. No balance phase - episode ends at success\n    \"\"\"\n    def __init__(self,\n                 env,\n                 height_weight=10.0,\n                 control_cost=0.1,\n                 success_height=1.8,\n                 success_bonus=100.0,\n                 downward_penalty=5.0):\n        super().__init__(env)\n        self.height_weight = height_weight\n        self.control_cost = control_cost\n        self.success_height = success_height\n        self.success_bonus = success_bonus\n        self.downward_penalty = downward_penalty\n\n    def reset(self, **kwargs):\n        obs, info = self.env.reset(**kwargs)\n        return obs, info\n\n    def _is_downward(self, th1):\n        \"\"\"Check if the acrobot is in a downward configuration\"\"\"\n        e_th1_down = min(np.abs(th1), np.abs(th1 - 2*np.pi))\n        return e_th1_down < 0.5\n\n    def step(self, action):\n        obs, _, terminated, truncated, info = self.env.step(action)\n\n        # Extract state from observation\n        cos_th1, sin_th1, cos_th2, sin_th2, th1_dot, th2_dot = obs\n\n        # Recover angles\n        th1 = np.arctan2(sin_th1, cos_th1)\n\n        # Calculate tip height\n        height = -cos_th1 - (cos_th1 * cos_th2 - sin_th1 * sin_th2)\n\n        # Get control input\n        u = float(getattr(self.env.unwrapped, \"last_u\", 0.0))\n\n        # Swing-up reward function (no balance components)\n        # 1. Height reward (main objective)\n        height_reward = self.height_weight * height\n\n        # 2. Downward penalty (discourage staying down)\n        downward_cost = 0.0\n        if self._is_downward(th1):\n            downward_cost = self.downward_penalty\n\n        # 3. Control cost (energy efficiency)\n        control_penalty = self.control_cost * (u**2)\n\n        # Total reward (simplified for swing-up only)\n        reward = height_reward - downward_cost - control_penalty\n\n        # TERMINATE upon reaching target height (swing-up success)\n        if height >= self.success_height:\n            terminated = True\n            reward += self.success_bonus  # Large bonus for success\n\n        return obs, reward, terminated, truncated, info\n\n\n# Updated environment creation for swing-up only\ndef make_acrobot_swingup_env():\n    base_env = gym.make(\"Acrobot-v1\", max_episode_steps=500)  # Shorter episodes for swing-up\n    env = ContinuousAcrobotActionWrapper(base_env, torque_limit=5.0)\n    env = AcrobotSwingUpWrapper(\n        env,\n        height_weight=10.0,\n        control_cost=0.1,\n        success_height=1.8,  # Height threshold for success\n        success_bonus=100.0,\n        downward_penalty=5.0\n    )\n    return env\n\n# Train the model for swing-up only\nprint(\"Training the swing-up only policy...\")\nvec_env_swingup = make_vec_env(make_acrobot_swingup_env, n_envs=32)\nmodel_swingup = PPO(\"MlpPolicy\", vec_env_swingup, verbose=1,\n                    learning_rate=3e-4, n_steps=2048)\nmodel_swingup.learn(total_timesteps=500_000)  # Less timesteps needed for simpler task\nmodel_swingup.save(\"ppo_acrobot_swingup\")\nprint(\"Training complete!\")\n\n# Evaluate the trained policy\nenv_eval = make_acrobot_swingup_env()\nmean_reward, std_reward = evaluate_policy(model_swingup, env_eval, n_eval_episodes=10)\nprint(f\"Mean reward over 10 episodes: {mean_reward:.2f} ± {std_reward:.2f}\")\nenv_eval.close()\n\n\n\n\n\n\n\nVisualize and Analyze the Trained Policy\n\nThe code below demonstrates the performance of the trained RL agent that learns the swing-up behavior. It includes:\n\nVideo Recording: Creates a video of the trained policy in action, showing the complete swing-up and balance behavior over a full episode\n\nQuantitative Analysis: Runs 5 evaluation episodes and plots for each:\n\nJoint angles (θ₁ and θ₂) over time with target values indicated\n\nTip height trajectory showing progression from downward to upright\n\nControl input (torque) showing the agent’s learned control strategy\n\nReward per step demonstrating the multi-phase reward structure\n\nThese visualizations help verify that the policy successfully learns the swing-up process.\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\nimport gymnasium as gym\nfrom IPython.display import Video\n\n# Create environment wrapper for video recording\ndef make_acrobot_balance_env_video():\n    base_env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\", max_episode_steps=1000)\n    env = ContinuousAcrobotActionWrapper(base_env, torque_limit=5.0)\n    env = AcrobotSwingUpWrapper(\n        env,\n        height_weight=10.0,\n        control_cost=0.1,\n        success_height=1.8,\n        success_bonus=100.0,\n        downward_penalty=5.0\n    )\n    return env\n\n# Load the trained model\nmodel = PPO.load(\"ppo_acrobot_swingup\")\n\n# Record video\nenv = DummyVecEnv([make_acrobot_balance_env_video])\nenv = VecVideoRecorder(env, \"videos/\",\n                       record_video_trigger=lambda x: x == 0,\n                       video_length=1000,\n                       name_prefix=\"acrobot_balance\")\n\nobs = env.reset()\nfor _ in range(5000):\n    action, _ = model.predict(obs, deterministic=True)\n    obs, reward, done, info = env.step(action)\n    if done[0]:\n        break\n\nenv.close()\n\n# Display the video\nVideo(\"videos/acrobot_swingup-step-0-to-step-1000.mp4\",embed=True, width=600)\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom stable_baselines3 import PPO\nimport numpy as np\n\n# Create environment without rendering\ndef make_acrobot_balance_env_eval():\n    base_env = gym.make(\"Acrobot-v1\", max_episode_steps=1000)\n    env = ContinuousAcrobotActionWrapper(base_env, torque_limit=5.0)\n    env = AcrobotSwingUpWrapper(\n        env,\n        height_weight=10.0,\n        control_cost=0.1,\n        success_height=1.8,\n        success_bonus=100.0,\n        downward_penalty=5.0\n    )\n    return env\n\nmodel = PPO.load(\"ppo_acrobot_swingup\")\nenv = make_acrobot_balance_env_eval()\n\n# Run multiple episodes and collect data\nnum_episodes = 5\nfor ep in range(num_episodes):\n    obs, info = env.reset()\n    done = False\n    step_count = 0\n\n    # Storage for this episode\n    observations = []\n    actions = []\n    rewards = []\n\n    while not done and step_count < 1000:\n        action, _ = model.predict(obs, deterministic=True)\n        observations.append(obs)\n        actions.append(action)\n\n        obs, reward, terminated, truncated, info = env.step(action)\n        rewards.append(reward)\n        done = terminated or truncated\n        step_count += 1\n\n    # Convert to numpy arrays\n    observations = np.array(observations)\n    actions = np.array(actions)\n\n    # Extract angles from observations\n    cos_th1, sin_th1 = observations[:, 0], observations[:, 1]\n    cos_th2, sin_th2 = observations[:, 2], observations[:, 3]\n    th1 = np.arctan2(sin_th1, cos_th1)\n    th2 = np.arctan2(sin_th2, cos_th2)\n\n    # Calculate tip height\n    height = -cos_th1 - (cos_th1 * cos_th2 - sin_th1 * sin_th2)\n\n    # Plot results for this episode\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n    # Plot angles\n    axes[0, 0].plot(th1, label='θ₁')\n    axes[0, 0].plot(th2, label='θ₂')\n    axes[0, 0].axhline(y=np.pi, color='r', linestyle='--', alpha=0.5, label='θ₁ target (π)')\n    axes[0, 0].axhline(y=0, color='g', linestyle='--', alpha=0.5, label='θ₂ target (0)')\n    axes[0, 0].set_xlabel('Step')\n    axes[0, 0].set_ylabel('Angle (rad)')\n    axes[0, 0].set_title('Joint Angles')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True)\n\n    # Plot tip height\n    axes[0, 1].plot(height)\n    axes[0, 1].axhline(y=2.0, color='r', linestyle='--', alpha=0.5, label='Max height (upright)')\n    axes[0, 1].set_xlabel('Step')\n    axes[0, 1].set_ylabel('Height')\n    axes[0, 1].set_title('Tip Height Over Time')\n    axes[0, 1].legend()\n    axes[0, 1].grid(True)\n\n    # Plot control actions\n    axes[1, 0].plot(actions)\n    axes[1, 0].axhline(y=5.0, color='r', linestyle='--', alpha=0.5, label='Torque limit')\n    axes[1, 0].axhline(y=-5.0, color='r', linestyle='--', alpha=0.5)\n    axes[1, 0].set_xlabel('Step')\n    axes[1, 0].set_ylabel('Torque (Nm)')\n    axes[1, 0].set_title('Control Input')\n    axes[1, 0].legend()\n    axes[1, 0].grid(True)\n\n    # Plot rewards\n    axes[1, 1].plot(rewards)\n    axes[1, 1].set_xlabel('Step')\n    axes[1, 1].set_ylabel('Reward')\n    axes[1, 1].set_title('Reward per Step')\n    axes[1, 1].grid(True)\n\n    plt.suptitle(f'Episode {ep+1}: {step_count} steps, Final reward: {rewards[-1]:.2f}')\n    plt.tight_layout()\n    plt.show()\n\n    print(f\"Episode {ep+1}: Steps = {step_count}, Total reward = {sum(rewards):.2f}\")\n\nenv.close()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/mech529-finalproject-acrobot#using-gpu-acceleration-in-google-colab","position":49},{"hierarchy":{"lvl1":"Final Project","lvl3":"Task 5.4 - Hybrid Policy to Swing Up and then Balance (5 pts)","lvl2":"Task 5. Reinforcement Learning Control (25 pts)"},"type":"lvl3","url":"/mech529-finalproject-acrobot#task-5-4-hybrid-policy-to-swing-up-and-then-balance-5-pts","position":50},{"hierarchy":{"lvl1":"Final Project","lvl3":"Task 5.4 - Hybrid Policy to Swing Up and then Balance (5 pts)","lvl2":"Task 5. Reinforcement Learning Control (25 pts)"},"content":"In this task, you need to combine two trained PPO policies: a swing-up policy and a stabilizer policy. You may refer to lab 10 for the cart pole on how to do this.\n\n#TODO: implement the missing parts\n\n\n\n","type":"content","url":"/mech529-finalproject-acrobot#task-5-4-hybrid-policy-to-swing-up-and-then-balance-5-pts","position":51},{"hierarchy":{"lvl1":"Final Project","lvl2":"Task 6. Discussions (5 pts)"},"type":"lvl2","url":"/mech529-finalproject-acrobot#task-6-discussions-5-pts","position":52},{"hierarchy":{"lvl1":"Final Project","lvl2":"Task 6. Discussions (5 pts)"},"content":"In this Task, you will compare the model-based control (LQR / trajectory tracking / MPC) with the RL-based control on the Acrobot system. Specifically, compare the hybrid model-based controller in Task 4 vs and the hybrid RL policy generated in task 5.3 by plotting the following:\n\nSingle comparative figure: control inputs vs time (overlay both controllers: model-based u_model(t) and RL-based u_RL(t), include legend)\n\nComparative state trajectories vs time: overlay for θ₁(t), θ₂(t) showing both controllers\n\nTip (end-effector) height vs time: single figure with both controllers overlaid and optionally target upright height reference line\n\nYou can choose the best policy you have obtained in RL by playing with the reward function. Based on the results you have obtained, explain the difference for the two approaches: model-based control vs  RL-based control.\n\nSubmission Requirements:\n\nComplete this notebook with all tasks implemented and documented.\n\nWritten report addressing the discussion points in each task.\n\nInclude quantitative comparisons with plots and performance metrics.\n\nReflect on lessons learned about control theory and reinforcement learning.","type":"content","url":"/mech529-finalproject-acrobot#task-6-discussions-5-pts","position":53},{"hierarchy":{"lvl1":"MECH 529 Advanced Mechanical Systems"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"MECH 529 Advanced Mechanical Systems"},"content":"Welcome to the MECH 529 Advanced Mechanical Systems laboratory course at Colorado State University.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"MECH 529 Advanced Mechanical Systems","lvl2":"Course Overview"},"type":"lvl2","url":"/#course-overview","position":2},{"hierarchy":{"lvl1":"MECH 529 Advanced Mechanical Systems","lvl2":"Course Overview"},"content":"This course provides hands-on experience with advanced topics in mechanical systems control, including:\n\nPython programming for control systems\n\nOrdinary differential equations (ODEs)\n\nControl system design and analysis\n\nLinear Quadratic Regulator (LQR)\n\nTrajectory tracking and generation\n\nModel Predictive Control (MPC)\n\nReinforcement Learning (RL)","type":"content","url":"/#course-overview","position":3},{"hierarchy":{"lvl1":"MECH 529 Advanced Mechanical Systems","lvl2":"Laboratory Exercises"},"type":"lvl2","url":"/#laboratory-exercises","position":4},{"hierarchy":{"lvl1":"MECH 529 Advanced Mechanical Systems","lvl2":"Laboratory Exercises"},"content":"The course consists of 10 laboratory exercises and a final project:\n\nLab 1: Python Basics\n\nLab 2: ODEs\n\nLab 3: Introduction to Python Control\n\nLab 4: Closed Loop Control\n\nLab 5: LQR (Linear Quadratic Regulator)\n\nLab 6: Trajectory Tracking\n\nLab 7: Trajectory Generation\n\nLab 8: Model Predictive Control\n\nLab 9: Value Iteration & Policy Iteration\n\nLab 10: Reinforcement Learning\n\nFinal Project: Acrobot","type":"content","url":"/#laboratory-exercises","position":5},{"hierarchy":{"lvl1":"MECH 529 Advanced Mechanical Systems","lvl2":"Getting Started"},"type":"lvl2","url":"/#getting-started","position":6},{"hierarchy":{"lvl1":"MECH 529 Advanced Mechanical Systems","lvl2":"Getting Started"},"content":"Each lab is provided as a Jupyter notebook that you can download and run in your own environment. The notebooks contain both explanatory text and executable code cells.","type":"content","url":"/#getting-started","position":7},{"hierarchy":{"lvl1":"MECH 529 Advanced Mechanical Systems","lvl2":"Prerequisites"},"type":"lvl2","url":"/#prerequisites","position":8},{"hierarchy":{"lvl1":"MECH 529 Advanced Mechanical Systems","lvl2":"Prerequisites"},"content":"Basic knowledge of Python programming\n\nUnderstanding of linear algebra and differential equations\n\nFamiliarity with control systems theory\n\nColorado State University | Fall 2025","type":"content","url":"/#prerequisites","position":9}]}